nohup: ignoring input
[01:04:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_multiply_add_nn_pad"
[01:04:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(1,), "float32"], placeholder_2: T.Buffer[(3, 1, 1), "float32"], T_pad: T.Buffer[(1, 3, 418, 418), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_multiply = T.alloc_buffer([1, 3, 416, 416], dtype="float32")
        T_add = T.alloc_buffer([1, 3, 416, 416], dtype="float32")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 416, 416):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], placeholder_1[0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] * placeholder_1[0]
        for i0, i1, i2, i3 in T.grid(1, 3, 416, 416):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_2[ax1, 0, 0], T_multiply[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = placeholder_2[ax1, 0, 0] + T_multiply[ax0, ax1, ax2, ax3]
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 3, 418, 418):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 417 and 1 <= ax3 and ax3 < 417, T_add[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 416, 416), "float32"], placeholder_1: T.Buffer[(1,), "float32"], placeholder_2: T.Buffer[(3, 1, 1), "float32"], T_pad: T.Buffer[(1, 3, 418, 418), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 3, 418, 418):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_2[ax1, 0, 0], placeholder[ax0, ax1, ax2 - 1, ax3 - 1], placeholder_1[0])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 417 and 1 <= ax3 and ax3 < 417, placeholder_2[ax1, 0, 0] + placeholder[ax0, ax1, ax2 - 1, ax3 - 1] * placeholder_1[0], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_multiply", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="compile_engine_const", func_name="main")
b3 = sch.get_block(name="T_cast", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[01:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_nn_leaky_relu"
[01:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 418, 418], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 16, 416, 416], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 416, 416], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 418, 418):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 416, 416, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 16, 416, 416):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 16, 416, 416):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:04:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(3328, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6720):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 32 + i5_0 + ax0_ax1_ax2_ax3_fused % 6720 // 210)
                                    v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + ax0_ax1_ax2_ax3_fused % 210)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(12):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 4 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 1664)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 1664 // 104)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 104)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 1664 + ax1)
                                v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 1664 // 104 + ax2)
                                v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 104 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 16, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 104, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:04:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_pad"
[01:04:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 416, 416), "float32"], T_pad: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 16, 416, 416):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

[01:04:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 416, 416), "float32"], T_pad: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 16, 416, 416):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_max_pool2d"
[01:04:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 416, 416), "float32"], tensor: T.Buffer[(1, 16, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 16, 208, 208, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[01:04:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 416, 416), "float32"], tensor: T.Buffer[(1, 16, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 16, 208, 208, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_pad_1"
[01:04:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208), "float32"], T_pad: T.Buffer[(1, 16, 210, 210), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 16, 210, 210):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 209 and 1 <= ax3 and ax3 < 209, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:04:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 208, 208), "float32"], T_pad: T.Buffer[(1, 16, 210, 210), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 16, 210, 210):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 209 and 1 <= ax3 and ax3 < 209, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"
[01:04:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 210, 210], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 208, 208], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 210, 210):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 208, 208, 16, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:04:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(208, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(9984):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 9984 // 624)
                                    v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + ax0_ax1_ax2_ax3_fused % 624 // 104)
                                    v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i6_0 + ax0_ax1_ax2_ax3_fused % 104)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 16 + ax0_ax1_ax2_ax3_fused // 48)
                                    v1 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 48 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 1, 1, 1, 1, 8, 2, 1, 1, 1, 2, 1, 13):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_4)
                                    yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i3_3 * 13 + i3_4)
                                    rc = T.axis.reduce(16, i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 104):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                                v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[52, 1, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 8, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:04:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_pad_2"
[01:04:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208), "float32"], T_pad: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208), "float32"], T_pad: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 32, 208, 208):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_max_pool2d_1"
[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208), "float32"], tensor: T.Buffer[(1, 32, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 32, 104, 104, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 208, 208), "float32"], tensor: T.Buffer[(1, 32, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 32, 104, 104, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_pad_3"
[01:04:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104), "float32"], T_pad: T.Buffer[(1, 32, 106, 106), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 32, 106, 106):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 105 and 1 <= ax3 and ax3 < 105, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:04:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 104, 104), "float32"], T_pad: T.Buffer[(1, 32, 106, 106), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 32, 106, 106):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 105 and 1 <= ax3 and ax3 < 105, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"
[01:04:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 106, 106], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        T_add = T.alloc_buffer([1, 64, 104, 104], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 106, 106):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 104, 104, 32, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 + 0)
                                    v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 26 + ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 16 * 32 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(32, i4_0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 3, 3, 1, 4, 13, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 16 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i2_4)
                                    xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 16 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 8 + ax1)
                                v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + ax2)
                                v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 1, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 13, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_pad_4"
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], T_pad: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], T_pad: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 64, 104, 104):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_max_pool2d_2"
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], tensor: T.Buffer[(1, 64, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 52, 52, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 104, 104), "float32"], tensor: T.Buffer[(1, 64, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 64, 52, 52, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_pad_5"
[01:04:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52), "float32"], T_pad: T.Buffer[(1, 64, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 64, 54, 54):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 53 and 1 <= ax3 and ax3 < 53, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 52, 52), "float32"], T_pad: T.Buffer[(1, 64, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 64, 54, 54):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 53 and 1 <= ax3 and ax3 < 53, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"
[01:04:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 54, 54], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        T_add = T.alloc_buffer([1, 128, 52, 52], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 54, 54):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 128, 52, 52, 64, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2704, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 128 // 2)
                                    v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 52 * 2 + i5_0 + ax0_ax1_ax2_ax3_fused % 2)
                                    v3 = T.axis.spatial(54, i6_0 + i0_0_i1_0_i2_0_i3_0_fused % 52 + 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(4096):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 64 + ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused % 64)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 16, 1, 1, 1, 4, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 52 * 2 + i2_4)
                                    xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52)
                                    rc = T.axis.reduce(64, i4_1 * 16 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                                v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 52 * 2 + ax2)
                                v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 52 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 1, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[52, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_pad_6"
[01:04:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], T_pad: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

[01:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], T_pad: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 128, 52, 52):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_max_pool2d_3"
[01:04:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], tensor: T.Buffer[(1, 128, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 26, 26, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[01:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 52, 52), "float32"], tensor: T.Buffer[(1, 128, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 128, 26, 26, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_pad_7"
[01:04:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26), "float32"], T_pad: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 27 and 1 <= ax3 and ax3 < 27, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 26, 26), "float32"], T_pad: T.Buffer[(1, 128, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 27 and 1 <= ax3 and ax3 < 27, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"
[01:04:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 128, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 26, 26], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 128, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 26, 26, 128, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:04:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(14336):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 14336 // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + ax0_ax1_ax2_ax3_fused % 112 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(147456):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + ax0_ax1_ax2_ax3_fused // 1152)
                                    v1 = T.axis.spatial(128, ax0_ax1_ax2_ax3_fused % 1152 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 3, 3, 1, 2, 2, 13, 4, 1, 1, 1, 4, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_3)
                                    xx = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + i3_3)
                                    rc = T.axis.reduce(128, i4_1 * 4 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 13):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                                v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + ax2)
                                v3 = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 2 * 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 32, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:04:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_pad_8"
[01:04:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26), "float32"], T_pad: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

[01:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26), "float32"], T_pad: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 256, 26, 26):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_max_pool2d_4"
[01:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[01:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 26, 26), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_pad_9"
[01:04:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_pad: T.Buffer[(1, 256, 15, 15), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 256, 15, 15):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 14 and 1 <= ax3 and ax3 < 14, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_pad: T.Buffer[(1, 256, 15, 15), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 256, 15, 15):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 14 and 1 <= ax3 and ax3 < 14, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"
[01:04:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 15, 15], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 512, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 512, 13, 13, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 512, 13, 13):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(240):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 240 // 15)
                                    v2 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 15)
                                    v3 = T.axis.spatial(15, i6_0 + i0_0_i1_0_i2_0_i3_0_fused % 13 + 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(12288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + ax0_ax1_ax2_ax3_fused // 48)
                                    v1 = T.axis.spatial(256, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 48 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 8, 1, 1, 8, 3, 1, 1, 4, 13, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(13, i2_4)
                                    xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    rc = T.axis.reduce(256, i4_0 * 16 + i4_1 * 8 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                                v2 = T.axis.spatial(13, ax2)
                                v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 8, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_pad_10"
[01:04:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], T_pad: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(-3.4028234663852886e+38)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 13 and ax3 < 13, placeholder[ax0, ax1, ax2, ax3], T_cast[()], dtype="float32")
    

[01:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], T_pad: T.Buffer[(1, 512, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3 in T.grid(1, 512, 14, 14):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 13 and ax3 < 13, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_max_pool2d_5"
[01:04:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], tensor: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 512, 13, 13, 2, 2):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
    

[01:05:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 14, 14), "float32"], tensor: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 512, 13, 13, 2, 2):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:05:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_pad_11"
[01:05:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], T_pad: T.Buffer[(1, 512, 15, 15), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 512, 15, 15):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 14 and 1 <= ax3 and ax3 < 14, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 13, 13), "float32"], T_pad: T.Buffer[(1, 512, 15, 15), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 512, 15, 15):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 14 and 1 <= ax3 and ax3 < 14, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"
[01:05:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 512, 15, 15], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 512, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 13, 13, 512, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(256, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(13, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(57600):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused // 225)
                                    v2 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 225 // 15)
                                    v3 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2359296):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused // 2304)
                                    v1 = T.axis.spatial(512, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 2304 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 13, 1, 64, 3, 3, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i2_3, i0_2_i1_2_i2_2_i3_2_fused])
                                    rc = T.axis.reduce(512, i4_0 * 256 + i4_1 * 64 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                                v2 = T.axis.spatial(13, ax2)
                                v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 256, 1, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_pad_12"
[01:05:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], T_pad: T.Buffer[(1, 1024, 15, 15), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_cast = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0)
        with T.block("T_cast"):
            T.reads(compile_engine_const[()])
            T.writes(T_cast[()])
            T_cast[()] = compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 1024, 15, 15):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 14 and 1 <= ax3 and ax3 < 14, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T_cast[()], dtype="float32")
    

[01:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], T_pad: T.Buffer[(1, 1024, 15, 15), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3 in T.grid(1, 1024, 15, 15):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 14 and 1 <= ax3 and ax3 < 14, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_cast", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[01:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"
[01:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 15, 15], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1024, 13, 13, 1024, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_leaky_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_leaky_relu[ax0, ax1, ax2, ax3])
                T_leaky_relu[ax0, ax1, ax2, ax3] = T.Select(T.float32(0) < T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3], T_add[ax0, ax1, ax2, ax3] * T.float32(0.10000000149011612))
    

[01:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(13, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(49920):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 49920 // 195)
                                    v2 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 195 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + ax0_ax1_ax2_ax3_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(786432):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused // 768)
                                    v1 = T.axis.spatial(1024, i4_0 * 256 + ax0_ax1_ax2_ax3_fused % 768 // 3)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 3, 1, 1, 512, 13, 1, 16, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 512 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i2_3, i0_2_i1_2_i2_2_i3_2_fused])
                                    rc = T.axis.reduce(1024, i4_0 * 256 + i4_1 * 16 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 512, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 512 + ax1)
                                v2 = T.axis.spatial(13, ax2)
                                v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_leaky_relu[v0, v1, v2, v3])
                                T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 512, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[01:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_pad_13"
[01:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], T_pad: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("T_pad"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_pad[ax0, ax1, ax2, ax3])
                T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

[01:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], T_pad: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
                with T.block("T_pad"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_pad[ax0, ax1, ax2, ax3])
                    T_pad[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[01:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_conv2d_add"
[01:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], placeholder_1: T.Buffer[(125, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 125, 1, 1), "float32"], T_add: T.Buffer[(1, 125, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1024, 13, 13], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 125, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1024, 13, 13):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 125, 13, 13, 1024, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 13, 13], "float32"], ["TENSOR", [125, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 125, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[01:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[01:05:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], placeholder_1: T.Buffer[(125, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 125, 1, 1), "float32"], T_add: T.Buffer[(1, 125, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 125, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([125, 1024, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1625, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6656):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 512 + ax0_ax1_ax2_ax3_fused // 13)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused % 13)
                                    v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(125, i0_0_i1_0_i2_0_i3_0_fused // 13)
                                    v1 = T.axis.spatial(1024, i4_0 * 512 + ax0_ax1_ax2_ax3_fused)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(128, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 13, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(125, i0_0_i1_0_i2_0_i3_0_fused // 13)
                                    yy = T.axis.spatial(13, i2_4)
                                    xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    rc = T.axis.reduce(1024, i4_0 * 512 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 13, 13], "float32"], ["TENSOR", [125, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(125, i0_0_i1_0_i2_0_i3_0_fused // 13 + ax1)
                                v2 = T.axis.spatial(13, ax2)
                                v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[125, 1, 1, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 128, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[01:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                        fused_nn_pad |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[01:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_nn_pad"
[01:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:06:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[01:06:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:07:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[01:08:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[01:10:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[01:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[01:11:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7083  0.6615  0.3661  0.2993  0.0188
[01:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:11:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:11:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:11:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_leaky_relu"
[01:11:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:11:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:11:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff854c18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc70ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff12988)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbde48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffef6738)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe13b98)]: 1885 failure(s)
[01:11:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 163 candidate(s)
[01:13:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff854c18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc70ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff12988)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbde48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffef6738)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe13b98)]: 318 failure(s)
[01:15:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff854c18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc70ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff12988)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbde48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffef6738)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe13b98)]: 271 failure(s)
[01:17:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff854c18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc70ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff12988)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbde48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffef6738)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe13b98)]: 249 failure(s)
[01:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff854c18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc70ff8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff12988)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbde48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffef6738)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe13b98)]: 219 failure(s)
[01:20:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9988  0.9988  0.9985  0.9979  0.9978  0.9977  0.9974  0.9974  0.9974  0.9974  0.9968  0.9966  0.9965  0.9962
[17 : 32]:	0.9959  0.9958  0.9957  0.9957  0.9954  0.9953  0.9948  0.9944  0.9943  0.9939  0.9935  0.9932  0.9931  0.9929  0.9929  0.9927
[01:20:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:20:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:20:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:21:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:21:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_pad"
[01:21:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:21:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:21:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[01:21:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:21:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[01:22:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[01:22:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[01:22:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[01:22:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8443  0.5392  0.3762  0.2413  0.0225
[01:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:22:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:22:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:22:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:23:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_max_pool2d"
[01:23:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:23:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:23:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[01:23:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:23:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[01:23:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[01:24:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[01:24:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[01:24:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.6191  0.5337  0.2724  0.2157  0.0896
[01:24:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:24:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:24:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:24:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:24:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_pad_1"
[01:24:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:24:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:26:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[01:26:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:27:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[01:27:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[01:28:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[01:29:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[01:29:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9456  0.7855  0.5451  0.3432  0.1436
[01:29:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:29:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:29:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:29:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:29:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"
[01:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:29:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:30:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3ffc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd33c18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe94668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffda4db8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdef638)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe17168)]: 1914 failure(s)
[01:30:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 134 candidate(s)
[01:31:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3ffc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd33c18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe94668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffda4db8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdef638)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe17168)]: 359 failure(s)
[01:34:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3ffc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd33c18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe94668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffda4db8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdef638)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe17168)]: 332 failure(s)
[01:36:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3ffc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd33c18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe94668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffda4db8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdef638)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe17168)]: 300 failure(s)
[01:39:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3ffc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd33c18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe94668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffda4db8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdef638)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe17168)]: 269 failure(s)
[01:40:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9995  0.9992  0.9992  0.9991  0.9986  0.9986  0.9984  0.9982  0.9982  0.9982  0.9978  0.9976  0.9975  0.9975
[17 : 32]:	0.9973  0.9970  0.9969  0.9968  0.9967  0.9967  0.9960  0.9959  0.9953  0.9953  0.9951  0.9949  0.9946  0.9946  0.9945  0.9943
[01:40:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:40:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:40:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:40:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:41:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_pad_2"
[01:41:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:41:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:41:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[01:41:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:41:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[01:41:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[01:42:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[01:42:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[01:42:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.3472  0.3053  0.2202  0.1093  0.0827
[01:42:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:42:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:42:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:42:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:42:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d_1"
[01:42:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:42:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:43:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[01:43:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:43:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[01:44:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[01:44:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[01:44:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[01:45:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8473  0.7607  0.6160  0.2824  0.2385
[01:45:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:45:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:45:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:45:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:45:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_pad_3"
[01:45:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:45:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:47:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[01:47:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[01:48:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[01:48:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[01:49:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[01:50:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[01:51:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7001  0.5964  0.2676  0.1125  0.0801
[01:51:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[01:51:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[01:51:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[01:51:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[01:51:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"
[01:51:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:51:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:52:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdc1788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffdaffd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb4578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2de48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff54158)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff329f8)]: 1846 failure(s)
[01:52:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 202 candidate(s)
[01:54:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdc1788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffdaffd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb4578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2de48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff54158)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff329f8)]: 328 failure(s)
[01:56:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdc1788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffdaffd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb4578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2de48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff54158)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff329f8)]: 297 failure(s)
[01:58:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdc1788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffdaffd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb4578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2de48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff54158)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff329f8)]: 303 failure(s)
[02:00:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdc1788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffdaffd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb4578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2de48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff54158)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff329f8)]: 295 failure(s)
[02:01:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9997  0.9996  0.9994  0.9994  0.9993  0.9992  0.9991  0.9991  0.9989  0.9988  0.9980  0.9976  0.9976
[17 : 32]:	0.9973  0.9969  0.9967  0.9964  0.9963  0.9962  0.9962  0.9960  0.9958  0.9957  0.9956  0.9954  0.9953  0.9950  0.9949  0.9948
[02:01:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:01:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:01:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:02:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:02:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_pad_4"
[02:02:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:02:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:02:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[02:02:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:03:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[02:03:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[02:03:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[02:03:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[02:04:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9919  0.6194  0.5083  0.3840  0.1446
[02:04:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:04:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:04:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:04:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_2"
[02:04:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:04:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[02:04:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:04:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[02:05:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[02:05:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[02:06:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[02:06:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8956  0.7879  0.5742  0.1850  0.0858
[02:06:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:06:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:06:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:06:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:06:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_pad_5"
[02:06:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:06:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:08:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[02:08:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:08:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[02:10:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[02:10:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[02:11:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[02:11:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.5392  0.5056  0.4369  0.1896  0.0680
[02:11:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:11:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:11:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:12:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:12:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"
[02:12:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:12:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:13:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffedf288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff84e58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0e398)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe96538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe756b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0e438)]: 1874 failure(s)
[02:13:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 174 candidate(s)
[02:14:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffedf288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff84e58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0e398)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe96538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe756b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0e438)]: 350 failure(s)
[02:16:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffedf288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff84e58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0e398)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe96538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe756b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0e438)]: 297 failure(s)
[02:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffedf288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff84e58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0e398)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe96538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe756b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0e438)]: 305 failure(s)
[02:21:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffedf288)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff84e58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0e398)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe96538)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe756b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0e438)]: 305 failure(s)
[02:22:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9995  0.9993  0.9993  0.9990  0.9990  0.9987  0.9985  0.9984  0.9983  0.9981  0.9981  0.9980  0.9980  0.9979
[17 : 32]:	0.9978  0.9977  0.9977  0.9977  0.9976  0.9973  0.9973  0.9971  0.9970  0.9968  0.9966  0.9966  0.9965  0.9963  0.9963  0.9962
[02:22:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:22:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:22:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:22:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:23:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_pad_6"
[02:23:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:23:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:23:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[02:23:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:23:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[02:24:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[02:24:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[02:24:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[02:24:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8565  0.8543  0.2390  0.0761  0.0143
[02:24:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:24:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:25:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:25:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:25:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_3"
[02:25:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:25:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:25:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[02:25:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:25:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[02:26:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[02:26:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[02:27:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[02:27:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7551  0.4894  0.4359  0.2052  0.0779
[02:27:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:27:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:27:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:27:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_pad_7"
[02:27:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:27:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:29:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[02:29:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:29:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[02:30:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[02:31:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[02:31:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[02:32:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8527  0.6747  0.3528  0.3527  0.1239
[02:32:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:32:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:32:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:32:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"
[02:32:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:32:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:33:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd126d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe8bc78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2acc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff34f68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe37ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2ad68)]: 1874 failure(s)
[02:33:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 174 candidate(s)
[02:35:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd126d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe8bc78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2acc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff34f68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe37ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2ad68)]: 399 failure(s)
[02:37:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd126d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe8bc78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2acc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff34f68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe37ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2ad68)]: 349 failure(s)
[02:40:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd126d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe8bc78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2acc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff34f68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe37ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2ad68)]: 332 failure(s)
[02:41:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd126d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe8bc78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2acc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff34f68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe37ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2ad68)]: 354 failure(s)
[02:42:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9992  0.9990  0.9984  0.9983  0.9983  0.9981  0.9978  0.9972  0.9972  0.9969  0.9969  0.9968  0.9968  0.9968  0.9967
[17 : 32]:	0.9963  0.9963  0.9960  0.9959  0.9957  0.9957  0.9950  0.9949  0.9948  0.9947  0.9945  0.9944  0.9943  0.9941  0.9938  0.9935
[02:42:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:42:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:42:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:43:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:43:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_pad_8"
[02:43:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:43:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:43:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[02:43:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:44:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[02:44:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[02:44:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[02:44:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[02:44:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9371  0.9092  0.8702  0.7391  0.1481
[02:44:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:45:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:45:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:45:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:45:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_max_pool2d_4"
[02:45:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:45:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:45:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[02:45:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:45:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[02:46:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[02:46:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[02:47:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[02:47:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8301  0.6676  0.6412  0.4309  0.4304
[02:47:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:47:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:47:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:47:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:47:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_pad_9"
[02:47:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:47:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:48:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[02:48:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[02:49:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[02:50:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[02:51:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[02:52:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[02:52:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8065  0.7717  0.3836  0.3587  0.3250
[02:52:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[02:52:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[02:52:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[02:52:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[02:52:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"
[02:52:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:52:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:53:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff8dce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe80648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff98fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700026b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe80c38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff99048)]: 1926 failure(s)
[02:53:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 122 candidate(s)
[02:55:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff8dce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe80648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff98fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700026b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe80c38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff99048)]: 534 failure(s)
[02:56:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff8dce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe80648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff98fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700026b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe80c38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff99048)]: 491 failure(s)
[02:58:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff8dce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe80648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff98fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700026b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe80c38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff99048)]: 397 failure(s)
[03:00:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff8dce8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe80648)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff98fa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700026b08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe80c38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff99048)]: 401 failure(s)
[03:02:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9995  0.9991  0.9991  0.9989  0.9988  0.9988  0.9982  0.9981  0.9981  0.9979  0.9978  0.9975  0.9972  0.9970  0.9969
[17 : 32]:	0.9968  0.9968  0.9966  0.9966  0.9963  0.9961  0.9960  0.9960  0.9956  0.9955  0.9952  0.9947  0.9943  0.9938  0.9938  0.9937
[03:02:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:02:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:02:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:03:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:03:23] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x00005643a20d6554
  69: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:03:41] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x0000557795a1a554
  69: 0xffffffffffffffff


[03:03:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_pad_10"
[03:03:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:03:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:04:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[03:04:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:04:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[03:05:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[03:05:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[03:05:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[03:05:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7041  0.4877  0.4252  0.3155  0.1276
[03:05:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[03:05:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[03:05:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[03:05:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[03:05:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_max_pool2d_5"
[03:05:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:05:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:06:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[03:06:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:06:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[03:07:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[03:07:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[03:08:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[03:08:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.6699  0.5546  0.4650  0.3174  0.2982
[03:08:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[03:08:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[03:08:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[03:08:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[03:08:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_pad_11"
[03:08:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:08:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:10:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[03:10:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:10:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[03:11:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[03:12:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[03:13:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[03:13:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.8949  0.8525  0.5414  0.2046  0.0219
[03:13:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[03:13:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[03:13:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[03:13:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[03:14:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"
[03:14:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:14:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe940f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff423e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3088)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffca3a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffeaf928)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3128)]: 1968 failure(s)
[03:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 80 candidate(s)
[03:16:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe940f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff423e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3088)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffca3a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffeaf928)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3128)]: 523 failure(s)
[03:18:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe940f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff423e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3088)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffca3a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffeaf928)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3128)]: 390 failure(s)
[03:20:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe940f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff423e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3088)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffca3a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffeaf928)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3128)]: 353 failure(s)
[03:22:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe940f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff423e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3088)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffca3a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffeaf928)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3128)]: 347 failure(s)
[03:23:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9996  0.9995  0.9994  0.9992  0.9991  0.9986  0.9985  0.9984  0.9982  0.9979  0.9978  0.9977  0.9977  0.9975  0.9972
[17 : 32]:	0.9972  0.9972  0.9971  0.9968  0.9966  0.9964  0.9963  0.9958  0.9956  0.9954  0.9947  0.9947  0.9945  0.9944  0.9938  0.9927
[03:23:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:23:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:23:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:24:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:24:55] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x00005632b0369554
  69: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:25:21] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x000055be13009554
  69: 0xffffffffffffffff


[03:25:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_pad_12"
[03:25:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:25:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:27:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[03:27:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:28:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[03:28:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[03:29:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[03:30:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[03:30:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.7764  0.6157  0.6140  0.6028  0.2216
[03:30:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[03:30:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[03:30:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[03:30:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[03:30:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"
[03:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:31:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe81028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffec9d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff74f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd365a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd85448)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff7598)]: 1955 failure(s)
[03:31:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 93 candidate(s)
[03:32:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe81028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffec9d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff74f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd365a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd85448)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff7598)]: 564 failure(s)
[03:34:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe81028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffec9d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff74f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd365a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd85448)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff7598)]: 525 failure(s)
[03:35:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe81028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffec9d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff74f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd365a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd85448)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff7598)]: 453 failure(s)
[03:37:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe81028)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffec9d78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff74f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd365a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd85448)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff7598)]: 406 failure(s)
[03:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9997  0.9996  0.9996  0.9996  0.9993  0.9992  0.9991  0.9990  0.9990  0.9990  0.9988  0.9986  0.9980  0.9979
[17 : 32]:	0.9978  0.9977  0.9976  0.9974  0.9972  0.9971  0.9969  0.9969  0.9967  0.9966  0.9963  0.9960  0.9960  0.9959  0.9959  0.9958
[03:38:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:38:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:38:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:39:15] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x00005593e7489554
  69: 0xffffffffffffffff


[03:40:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_pad_13"
[03:40:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:40:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:40:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[03:40:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[03:40:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[03:40:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[03:40:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[03:40:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[03:41:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9174  0.5248  0.4693  0.2988  0.2661
[03:41:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[03:41:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[03:41:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[03:41:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[03:41:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #29: "fused_nn_conv2d_add"
[03:41:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:41:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:41:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff86db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffda8118)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffeab4b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2d6e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd62938)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffeab558)]: 1872 failure(s)
[03:41:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 176 candidate(s)
[03:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff86db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffda8118)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffeab4b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2d6e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd62938)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffeab558)]: 757 failure(s)
[03:44:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff86db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffda8118)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffeab4b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2d6e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd62938)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffeab558)]: 638 failure(s)
[03:45:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff86db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffda8118)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffeab4b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2d6e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd62938)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffeab558)]: 589 failure(s)
[03:47:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff86db28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffda8118)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffeab4b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe2d6e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd62938)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffeab558)]: 639 failure(s)
[03:48:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9995  0.9994  0.9991  0.9990  0.9984  0.9982  0.9981  0.9976  0.9970  0.9964  0.9964  0.9963  0.9962  0.9959
[17 : 32]:	0.9956  0.9954  0.9953  0.9953  0.9952  0.9951  0.9949  0.9946  0.9946  0.9941  0.9939  0.9936  0.9936  0.9934  0.9931  0.9929
[03:48:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:48:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:48:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:48:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_nn_pad"] Trial #0: GFLOPs: 116.4897. Time: 0.0089 ms. Best GFLOPs: 116.4897
[03:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_nn_pad"] Trial #1: GFLOPs: 50.7820. Time: 0.0204 ms. Best GFLOPs: 116.4897
[03:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_nn_pad"] Trial #2: GFLOPs: 53.6440. Time: 0.0194 ms. Best GFLOPs: 116.4897
[03:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_nn_pad"] Trial #3: GFLOPs: 62.2571. Time: 0.0167 ms. Best GFLOPs: 116.4897
[03:49:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_multiply_add_nn_pad"] Trial #4: GFLOPs: 94.7561. Time: 0.0110 ms. Best GFLOPs: 116.4897
[03:49:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                        fused_nn_pad |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 5
Total latency (us): 8.91355

[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #0: GFLOPs: 186.6243. Time: 0.8309 ms. Best GFLOPs: 186.6243
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #1: GFLOPs: 874.8948. Time: 0.1772 ms. Best GFLOPs: 874.8948
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #2: GFLOPs: 4.6131. Time: 33.6123 ms. Best GFLOPs: 874.8948
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #3: GFLOPs: 165.6506. Time: 0.9361 ms. Best GFLOPs: 874.8948
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(106):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 5508 // 1836)
                                    v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused // 8 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 1836 // 54)
                                    v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 8 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 54)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 5508)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 16, 1, 1):
                            for i2_4_init in T.serial(2):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i1_3)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 8 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_4_init)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 8 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 3, 1, 1, 2, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i1_3)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 8 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_4)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 8 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 8 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 8 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 8, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 2, 26, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #5: GFLOPs: 2070.4809. Time: 0.0749 ms. Best GFLOPs: 2070.4809
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #6: GFLOPs: 11.8882. Time: 13.0430 ms. Best GFLOPs: 2070.4809
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #7: GFLOPs: 78.2465. Time: 1.9817 ms. Best GFLOPs: 2070.4809
[03:49:23] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(416, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 104 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 52 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 32)
                            yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 104 // 52 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8)
                            xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 52 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(27):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5040 // 1680)
                                            v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 104 // 52 * 208 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 8)
                                            v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 52 * 8 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5040)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 104 * 4 + ax0_ax1_ax2_ax3_fused_1 // 9)
                                        v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 9 // 3)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 104 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 52 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 32)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 104 // 52 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 52 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 104 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 52 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 32 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 104 // 52 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 52 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 52, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[52, 1, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l175)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #9: GFLOPs: 102.0514. Time: 1.5194 ms. Best GFLOPs: 2070.4809
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #10: GFLOPs: 437.6191. Time: 0.3543 ms. Best GFLOPs: 2070.4809
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #11: GFLOPs: 167.2767. Time: 0.9270 ms. Best GFLOPs: 2070.4809
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #12: GFLOPs: 131.6539. Time: 1.1778 ms. Best GFLOPs: 2070.4809
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #13: GFLOPs: 94.2560. Time: 1.6451 ms. Best GFLOPs: 2070.4809
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #14: GFLOPs: 830.9315. Time: 0.1866 ms. Best GFLOPs: 2070.4809
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 10812 // 3604)
                                        v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused // 13 * 104 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3604 // 34)
                                        v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 34)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 10812)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 27)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 27 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 432)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_4_init, i3_4_init in T.grid(2, 8):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4_init)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 13 * 104 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 8 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 8):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 13 * 104 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 8 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 13 * 104 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 4, 1, 1, 8])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 104])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l177)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #16: GFLOPs: 2504.7490. Time: 0.0619 ms. Best GFLOPs: 2504.7490
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #17: GFLOPs: 2925.8817. Time: 0.0530 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4_init)
                            yy = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused // 4 * 208 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 52 + i2_3_init * 26 + i2_4_init)
                            xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(418, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 7488 // 18)
                                        v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 7488)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 26, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4)
                                yy = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused // 4 * 208 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 52 + i2_3 * 26 + i2_4)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 52, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused // 4 * 208 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 52 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 26])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 4, 4, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(117):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7488 // 416)
                                        v3 = T.axis.spatial(418, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 416)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 8, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax2)
                            v3 = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 4, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 52, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i2_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 4, 4, 2, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4_init)
                                yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 13 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 4 + i2_3_init * 2 + i2_4_init)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 16 + i3_3_init * 4 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5184 // 1728)
                                            v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused // 13 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1728 // 32)
                                            v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5184)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i6_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 4, 3, 3, 1, 1, 4, 2, 4):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 13 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 4 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 16 + i3_3 * 4 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 16):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 13 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 16 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 13, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 2, 4, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l177)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #21: GFLOPs: 1027.8909. Time: 0.1509 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #22: GFLOPs: 1367.5629. Time: 0.1134 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #23: GFLOPs: 143.5566. Time: 1.0801 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #24: GFLOPs: 55.8016. Time: 2.7787 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #25: GFLOPs: 1686.7715. Time: 0.0919 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(208, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + i1_4_init)
                                yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 13)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2520 // 840)
                                            v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 840 // 210)
                                            v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 210)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2520)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused_1 // 9)
                                        v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 9 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 3, 1, 3, 1, 2, 1, 2):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + i1_4)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 13)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 13 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 2 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[104, 1, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 13, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 208, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 208])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(676, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i2_3_init, i1_4_init, i2_4_init in T.grid(2, 4, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 338 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i1_4_init)
                                yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i2_3_init * 2 + i2_4_init)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(54):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1728 // 576)
                                        v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 576 // 32)
                                        v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 338 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i6_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 1, 4, 2, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 338 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i1_4)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 338 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 338 // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 1, 4, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 4, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l175)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #28: GFLOPs: 2386.0205. Time: 0.0650 ms. Best GFLOPs: 2925.8817
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(416, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i2_4_init in T.grid(4, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 208 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 4 + i1_3_init)
                            yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 208 // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 13 * 8 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 16 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 208 // 16 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 952 // 28)
                                        v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 16 * 26 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 952)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 208 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 4, 4, 1, 1, 1, 3, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 208 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 4 + i1_3)
                                yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 208 // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 13 * 8 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 16 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 208 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 4 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 208 // 16 * 32 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 13 * 8 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 16 * 26 + i0_1_i1_1_i2_1_i3_1_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 4, 4, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[16, 2, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:49:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(13, 2, 2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 52 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4_init)
                                yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 13 + i2_3_init)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(79):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10080 // 3360)
                                            v2 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 208 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3360 // 16)
                                            v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 10080)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 52 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i6_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 13, 2, 1, 3, 1, 1, 2, 1, 2):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 52 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4)
                                    yy = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 13 + i2_3)
                                    xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 52 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 208 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 13 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 8, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 2, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l177)
[03:49:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_leaky_relu"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 418, 418), "float32"], placeholder_1: T.Buffer[(16, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 16, 416, 416), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 416, 416], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 418, 418], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i2_4_init in T.grid(26, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                            yy = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 208 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 104 + i2_3_init * 4 + i2_4_init)
                            xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(59):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(418, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7524 // 18)
                                        v3 = T.axis.spatial(418, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 7524)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 26, 1, 1, 1, 3, 1, 1, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                yy = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 208 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 104 + i2_3 * 4 + i2_4)
                                xx = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 418, 418], "float32"], ["TENSOR", [16, 3, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 104, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 26 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(416, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 208 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 104 + ax2)
                            v3 = T.axis.spatial(416, i0_0_i1_0_i2_0_i3_0_fused % 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 26, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 2, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:49:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_nn_leaky_relu"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 37
Total latency (us): 61.9089

[03:49:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_pad"] Trial #0: GFLOPs: 0.0000. Time: 0.0655 ms. Best GFLOPs: 0.0000
[03:49:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_pad"] Trial #1: GFLOPs: 0.0000. Time: 0.0574 ms. Best GFLOPs: 0.0000
[03:49:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_pad"] Trial #2: GFLOPs: 0.0000. Time: 0.0620 ms. Best GFLOPs: 0.0000
[03:49:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_pad"] Trial #3: GFLOPs: 0.0000. Time: 0.0658 ms. Best GFLOPs: 0.0000
[03:49:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_pad"] Trial #4: GFLOPs: 0.0000. Time: 0.1262 ms. Best GFLOPs: 0.0000
[03:49:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 42
Total latency (us): 119.298

[03:49:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 30.6622. Time: 0.0903 ms. Best GFLOPs: 30.6622
[03:49:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 37.8551. Time: 0.0731 ms. Best GFLOPs: 37.8551
[03:49:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 34.5202. Time: 0.0802 ms. Best GFLOPs: 37.8551
[03:49:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 57.6583. Time: 0.0480 ms. Best GFLOPs: 57.6583
[03:49:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 40.3005. Time: 0.0687 ms. Best GFLOPs: 57.6583
[03:50:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_max_pool2d"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 47
Total latency (us): 167.321

[03:50:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_pad_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0211 ms. Best GFLOPs: 0.0000
[03:50:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_pad_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0499 ms. Best GFLOPs: 0.0000
[03:50:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_pad_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0278 ms. Best GFLOPs: 0.0000
[03:50:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_pad_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0315 ms. Best GFLOPs: 0.0000
[03:50:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_pad_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0543 ms. Best GFLOPs: 0.0000
[03:50:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_pad_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 52
Total latency (us): 188.44

[03:50:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #0: GFLOPs: 73.6237. Time: 5.4533 ms. Best GFLOPs: 73.6237
[03:50:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(16, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + i1_3_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 16 // 8 * 104 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 8 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(58):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 + 0)
                                    v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 16 // 8 * 104 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 2968 // 28)
                                    v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 8 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 2968)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(16, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 16, 4, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + i1_3)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 16 // 8 * 104 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 8 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 16 // 8 * 104 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 8 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 2, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 26, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(416, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 * 2 + i2_3_init)
                            xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2496 // 1248)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 52 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1248 // 208)
                                        v3 = T.axis.spatial(210, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 208)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 * 2 + i2_3)
                                xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[52, 1, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 26 * 2 + i2_4_init)
                            xx = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 26 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 2520 // 1260)
                                    v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 1260 // 210)
                                    v3 = T.axis.spatial(210, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 210)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 2520)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 2, 1, 3, 3, 1, 4, 2, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 26 * 2 + i2_4)
                                xx = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 26 * 8 + i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 26 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 26 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[52, 1, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 104, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(256, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 16 * 2 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_4_init)
                            xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7488 // 3744)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3744 // 208)
                                        v3 = T.axis.spatial(210, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 208)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 6 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 192)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 2, 3, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 16 * 2 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i2_4)
                                xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 4, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 26, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(8, 8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1696 // 848)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 848 // 8)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1696)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 8, 8, 2, 3, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 26 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 8, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 1, 8, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 26, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 8 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 4 * 2 + i2_3_init)
                            xx = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 4 * 52 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 + 0)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3780 // 210)
                                        v3 = T.axis.spatial(210, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 210)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3780)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(16, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 2, 26, 1, 1, 3, 1, 8, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 8 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 4 * 2 + i2_3)
                                xx = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 4 * 52 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 52):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 4 * 52 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 26, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #7: GFLOPs: 676.5194. Time: 0.5935 ms. Best GFLOPs: 676.5194
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 104 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 13 + i2_4_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 + 0)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 104 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1060 // 10)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1060)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(16, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 144)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 104 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 13 + i2_4)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 52 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 52 // 26 * 104 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 13 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #9: GFLOPs: 3500.4145. Time: 0.1147 ms. Best GFLOPs: 3500.4145
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(832, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init in T.serial(2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 416 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3_init)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 416 // 2)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1664 // 104)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 416 // 2 + i5_0 + 0)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 104)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 416 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 416 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3)
                                    yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 416 // 2)
                                    xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 416 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 416 // 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[208, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l172)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i3_4_init in T.grid(13, 13, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 13 + i2_3_init)
                            xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 52 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(351):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 + 0)
                                    v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 11232 // 208)
                                    v3 = T.axis.spatial(210, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 208)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(16, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 13, 13, 1, 1, 1, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 13 + i2_3)
                                xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 52 + i3_3 * 4 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 52):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 13 + ax2)
                            v3 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 52 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 4, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 13, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(4, 4, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3456 // 864)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 864 // 54)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3456)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 4, 1, 3, 1, 4, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 4, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 13, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #13: GFLOPs: 609.6795. Time: 0.6585 ms. Best GFLOPs: 3500.4145
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #14: GFLOPs: 399.5459. Time: 1.0049 ms. Best GFLOPs: 3500.4145
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #15: GFLOPs: 2132.3938. Time: 0.1883 ms. Best GFLOPs: 3500.4145
[03:50:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(4, 2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3_init)
                            yy = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused // 4 * 26 + i2_4_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(44):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4200 // 2100)
                                        v2 = T.axis.spatial(210, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2100 // 10)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4200)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 3, 1, 4, 1, 2, 1, 1, 1, 1, 1, 26, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_3)
                                yy = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused // 4 * 26 + i2_4)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 26, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 26 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused // 4 * 26 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 26 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 1, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 26])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 4, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i2_4_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2160 // 540)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 540 // 54)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2160)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 2, 4, 3, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i2_4)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[26, 4, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 13, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #18: GFLOPs: 3095.3453. Time: 0.1297 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i2_3_init)
                            xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(117):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3744 // 468)
                                    v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 468 // 18)
                                    v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 18)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 24 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 2, 1, 2, 1, 3, 1, 4, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i2_3)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused // 13 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 13, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 8, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #20: GFLOPs: 479.0606. Time: 0.8381 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #21: GFLOPs: 1094.2845. Time: 0.3669 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i2_4_init)
                            xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 + 0)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 11340 // 210)
                                        v3 = T.axis.spatial(210, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 210)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 11340)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(16, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i2_4)
                                xx = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 4 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 26 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_1_i1_1_i2_1_i3_1_fused % 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 1, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 13, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 8, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #23: GFLOPs: 2422.0201. Time: 0.1658 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #24: GFLOPs: 344.3056. Time: 1.1661 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(416, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10176 // 636)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 636 // 106)
                                        v3 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 106)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 10176)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 144)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 144 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i3_4_init in T.grid(4, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 3, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                                xx = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 104 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 26 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 104 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 + ax2)
                            v3 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused % 2 * 104 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[52, 2, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 13, 4, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #26: GFLOPs: 82.2751. Time: 4.8798 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #27: GFLOPs: 2338.0402. Time: 0.1717 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #28: GFLOPs: 762.1060. Time: 0.5268 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #29: GFLOPs: 2758.2005. Time: 0.1456 ms. Best GFLOPs: 3500.4145
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 210, 210), "float32"], placeholder_1: T.Buffer[(32, 16, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 32, 208, 208), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 208, 208], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 210, 210], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 16, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                            yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 16 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4992 // 1248)
                                        v2 = T.axis.spatial(210, i0_0_i1_0_i2_0_i3_0_fused * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1248 // 208)
                                        v3 = T.axis.spatial(210, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 208)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 12 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 13, 4, 3, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                yy = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 16 * 13 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 210, 210], "float32"], ["TENSOR", [32, 16, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(208, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(208, i0_2_i1_2_i2_2_i3_2_fused % 16 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[52, 2, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 16, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 128])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:50:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"] Trial #31: GFLOPs: 4561.2331. Time: 0.0880 ms. Best GFLOPs: 4561.2331
[03:51:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 84
Total latency (us): 276.462

[03:51:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_pad_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0386 ms. Best GFLOPs: 0.0000
[03:51:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_pad_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0879 ms. Best GFLOPs: 0.0000
[03:51:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_pad_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0795 ms. Best GFLOPs: 0.0000
[03:51:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_pad_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0698 ms. Best GFLOPs: 0.0000
[03:51:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_pad_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0472 ms. Best GFLOPs: 0.0000
[03:51:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_pad_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 89
Total latency (us): 315.068

[03:51:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 63.3425. Time: 0.0219 ms. Best GFLOPs: 63.3425
[03:51:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 40.7092. Time: 0.0340 ms. Best GFLOPs: 63.3425
[03:51:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 67.3124. Time: 0.0206 ms. Best GFLOPs: 67.3124
[03:51:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 74.9472. Time: 0.0185 ms. Best GFLOPs: 74.9472
[03:51:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 14.4299. Time: 0.0959 ms. Best GFLOPs: 74.9472
[03:51:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_max_pool2d_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 94
Total latency (us): 333.541

[03:51:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_pad_3"] Trial #0: GFLOPs: 0.0000. Time: 0.0368 ms. Best GFLOPs: 0.0000
[03:51:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_pad_3"] Trial #1: GFLOPs: 0.0000. Time: 0.0146 ms. Best GFLOPs: 0.0000
[03:51:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_pad_3"] Trial #2: GFLOPs: 0.0000. Time: 0.0082 ms. Best GFLOPs: 0.0000
[03:51:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_pad_3"] Trial #3: GFLOPs: 0.0000. Time: 0.0329 ms. Best GFLOPs: 0.0000
[03:51:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_pad_3"] Trial #4: GFLOPs: 0.0000. Time: 0.0054 ms. Best GFLOPs: 0.0000
[03:52:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_pad_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 99
Total latency (us): 338.96

[03:52:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused)
                            xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1272 // 318)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 318 // 106)
                                        v3 = T.axis.spatial(106, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 106)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1272)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 2, 4, 1, 3, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused)
                                xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[104, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 26, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:52:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 4, 2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 52 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4_init)
                                yy = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + i2_3_init * 2 + i2_4_init)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 6656 // 208)
                                        v2 = T.axis.spatial(106, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 208 // 2)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 52 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 2):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 52 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4)
                                    yy = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 52 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[52, 1, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 208])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 208])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l172)
[03:52:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + i1_3_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 4 * 2 + i2_3_init)
                            xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(27):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 11024 // 5512)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5512 // 106)
                                        v3 = T.axis.spatial(106, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 106)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 11024)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 13, 2, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + i1_3)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 4 * 2 + i2_3)
                                xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 13, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 4, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(4, 8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(164):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 8480 // 1060)
                                    v2 = T.axis.spatial(106, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 1060 // 10)
                                    v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 10)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 8480)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 72)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 72 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 1152)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 4, 1, 8, 8, 3, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 1, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 52, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 8, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #4: GFLOPs: 3021.2681. Time: 0.1324 ms. Best GFLOPs: 3021.2681
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #5: GFLOPs: 180.0341. Time: 2.2224 ms. Best GFLOPs: 3021.2681
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #6: GFLOPs: 74.7904. Time: 5.3497 ms. Best GFLOPs: 3021.2681
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #7: GFLOPs: 150.9530. Time: 2.6505 ms. Best GFLOPs: 3021.2681
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #8: GFLOPs: 115.0472. Time: 3.4778 ms. Best GFLOPs: 3021.2681
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 8 + i1_3_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                            xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(82):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 8480 // 1060)
                                    v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 1060 // 106)
                                    v3 = T.axis.spatial(106, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 106)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 8480)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 72)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 72 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 2304)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 8, 1, 1, 2, 1, 3, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 8 + i1_3)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13)
                                xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 8 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 + ax2)
                            v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 4, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 13, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 104])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(338, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 169 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(61):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5832 // 2916)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused // 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2916 // 54)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5832)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 4, 2, 1, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 169 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 * 4 + i2_3)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 169 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 * 4 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #11: GFLOPs: 851.1326. Time: 0.4701 ms. Best GFLOPs: 3021.2681
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(4, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 8 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1200 // 150)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 150 // 15)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 8 * 13 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1200)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 8 * 13 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 8 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 2, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 * 2 + i2_3_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1728 // 432)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 432 // 54)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1728)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 12 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 768)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 2, 2, 2, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 * 2 + i2_3)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 13 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 13, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 104])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #14: GFLOPs: 3760.8543. Time: 0.1064 ms. Best GFLOPs: 3760.8543
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #15: GFLOPs: 1495.1365. Time: 0.2676 ms. Best GFLOPs: 3760.8543
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(13, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3024 // 1512)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1512 // 54)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3024)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 13, 1, 2, 1, 3, 1, 8, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 26, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 1, 13, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 52):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(54):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6912 // 432)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 432 // 54)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 1, 1, 1, 4, 1, 1, 1, 2, 2, 52):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 52):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 52])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(88):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 11232 // 2808)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2808 // 52)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 11232)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 12 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 2, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #19: GFLOPs: 847.6316. Time: 0.4720 ms. Best GFLOPs: 3760.8543
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(676, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 338 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused % 338 // 13 * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(676, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11448 // 5724)
                                        v2 = T.axis.spatial(106, ((ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5724 // 54)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + ((ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 11448)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(676, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 1, 2, 1, 3, 1, 2, 2, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 338 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused % 338 // 13 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 338 * 4 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused * 52 + i0_2_i1_2_i2_2_i3_2_fused % 338 // 13 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 2 * 52 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 2, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 26, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 676, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 676, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #21: GFLOPs: 288.7330. Time: 1.3857 ms. Best GFLOPs: 3760.8543
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #22: GFLOPs: 2958.5760. Time: 0.1352 ms. Best GFLOPs: 3760.8543
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #23: GFLOPs: 2726.1905. Time: 0.1468 ms. Best GFLOPs: 3760.8543
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 4 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 26 * 2 + i2_4_init)
                            xx = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3392 // 848)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 848 // 106)
                                        v3 = T.axis.spatial(106, ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 106)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3392)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 4, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 4 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 26 * 2 + i2_4)
                                xx = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 4 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 52 // 26 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_2_i1_2_i2_2_i3_2_fused % 26 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 208, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 208, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(338, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 169 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 52 + i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(345):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 11024 // 5512)
                                    v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused * 52 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 5512 // 106)
                                    v3 = T.axis.spatial(106, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 106)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 11024)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 6 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 4, 2, 1, 1, 3, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 169 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 52 + i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 * 4 + i2_3)
                                xx = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 169 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 52 + i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 * 4 + ax2)
                            v3 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused % 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 13, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #26: GFLOPs: 481.9535. Time: 0.8302 ms. Best GFLOPs: 3760.8543
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(169, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i3_4_init in T.grid(2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 400 // 100)
                                    v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused // 13 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 100 // 10)
                                    v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 10)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 400)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 2, 1, 2, 2, 1, 3, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 13 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 13 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4_init)
                            yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 52 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 832 // 104)
                                        v2 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused // 52 * 52 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 104 // 2)
                                        v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 8, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_4)
                                yy = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 52 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                            v2 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused // 52 * 52 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused % 52 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 26, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[52, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:52:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 106, 106), "float32"], placeholder_1: T.Buffer[(64, 32, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 64, 104, 104), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 104, 104], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 106, 106], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 32, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init, i3_4_init in T.grid(2, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i2_4_init)
                            xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(33):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 2080 // 1040)
                                    v2 = T.axis.spatial(106, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 1040 // 10)
                                    v3 = T.axis.spatial(106, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 10)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 2080)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 6 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 8, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i2_4)
                                xx = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 106, 106], "float32"], ["TENSOR", [64, 32, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 8, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(104, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax2)
                            v3 = T.axis.spatial(104, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 8])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 2, 2, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 64])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:52:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #30: GFLOPs: 1858.2195. Time: 0.2153 ms. Best GFLOPs: 3760.8543
[03:52:22] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"] Trial #31: GFLOPs: 222.9836. Time: 1.7943 ms. Best GFLOPs: 3760.8543
[03:52:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 131
Total latency (us): 445.347

[03:52:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_pad_4"] Trial #0: GFLOPs: 0.0000. Time: 0.0532 ms. Best GFLOPs: 0.0000
[03:52:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_pad_4"] Trial #1: GFLOPs: 0.0000. Time: 0.0496 ms. Best GFLOPs: 0.0000
[03:52:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_pad_4"] Trial #2: GFLOPs: 0.0000. Time: 0.0151 ms. Best GFLOPs: 0.0000
[03:52:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_pad_4"] Trial #3: GFLOPs: 0.0000. Time: 0.0577 ms. Best GFLOPs: 0.0000
[03:52:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_pad_4"] Trial #4: GFLOPs: 0.0000. Time: 0.0423 ms. Best GFLOPs: 0.0000
[03:53:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_pad_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 136
Total latency (us): 460.411

[03:53:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_2"] Trial #0: GFLOPs: 104.9468. Time: 0.0066 ms. Best GFLOPs: 104.9468
[03:53:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_2"] Trial #1: GFLOPs: 94.1049. Time: 0.0074 ms. Best GFLOPs: 104.9468
[03:53:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_2"] Trial #2: GFLOPs: 48.7002. Time: 0.0142 ms. Best GFLOPs: 104.9468
[03:53:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_2"] Trial #3: GFLOPs: 99.8265. Time: 0.0069 ms. Best GFLOPs: 104.9468
[03:53:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_max_pool2d_2"] Trial #4: GFLOPs: 49.8867. Time: 0.0139 ms. Best GFLOPs: 104.9468
[03:53:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 141
Total latency (us): 467.007

[03:53:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_pad_5"] Trial #0: GFLOPs: 0.0000. Time: 0.0061 ms. Best GFLOPs: 0.0000
[03:53:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_pad_5"] Trial #1: GFLOPs: 0.0000. Time: 0.0062 ms. Best GFLOPs: 0.0000
[03:53:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_pad_5"] Trial #2: GFLOPs: 0.0000. Time: 0.0375 ms. Best GFLOPs: 0.0000
[03:53:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_pad_5"] Trial #3: GFLOPs: 0.0000. Time: 0.0558 ms. Best GFLOPs: 0.0000
[03:53:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_pad_5"] Trial #4: GFLOPs: 0.0000. Time: 0.0067 ms. Best GFLOPs: 0.0000
[03:53:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_pad_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |            N/A |          N/A |                   N/A |      0 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 146
Total latency (us): 473.061

[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(416, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 2, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i3_3_init * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(416, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 3024 // 1512)
                                    v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 1512 // 28)
                                    v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused * 26 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 < 3024)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(416, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) // 18)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 18 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 < 2304)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 1, 2, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + i3_3 * 13 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 416])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 416])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2592 // 324)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 324 // 54)
                                        v3 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2592)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 2, 1, 2, 8, 3, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 13 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 13 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 1, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #2: GFLOPs: 773.5311. Time: 0.5164 ms. Best GFLOPs: 773.5311
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) // 2916)
                                    v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 2916 // 54)
                                    v3 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 54)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 < 5832)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 2, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 26, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 208])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 208, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #4: GFLOPs: 2286.4416. Time: 0.1747 ms. Best GFLOPs: 2286.4416
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #5: GFLOPs: 1454.9835. Time: 0.2745 ms. Best GFLOPs: 2286.4416
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #6: GFLOPs: 1250.5734. Time: 0.3194 ms. Best GFLOPs: 2286.4416
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #7: GFLOPs: 312.8519. Time: 1.2767 ms. Best GFLOPs: 2286.4416
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5408 // 676)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 676 // 26)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #9: GFLOPs: 634.1333. Time: 0.6299 ms. Best GFLOPs: 2286.4416
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #10: GFLOPs: 10.3314. Time: 38.6603 ms. Best GFLOPs: 2286.4416
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 13, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 5408 // 2704)
                                    v2 = T.axis.spatial(54, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 2704 // 52)
                                    v3 = T.axis.spatial(54, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 52)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 2, 13, 1, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 2 * 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 13, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 104])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(4, 4, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 832 // 208)
                                        v2 = T.axis.spatial(54, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 208 // 4)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused * 4 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + i1_3_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 52 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 864 // 216)
                                    v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 216 // 4)
                                    v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 864)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 1152)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 2, 1, 1, 4, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + i1_3)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 52 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 32 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 52 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 52 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 2, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 104])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:53:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(208, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i3_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 52 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(52):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10816 // 2704)
                                        v2 = T.axis.spatial(54, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2704 // 52)
                                        v3 = T.axis.spatial(54, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 52 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 52 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 52 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4_init)
                            yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i2_4_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2916)
                                        v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2916 // 54)
                                        v3 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5832)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 2, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_4)
                                yy = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + i2_4)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 26 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 26, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 208, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 208, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(8, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i2_4_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 672 // 168)
                                    v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 168 // 28)
                                    v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 672)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 36)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 36 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 1, 2, 3, 1, 1, 8, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + i2_4)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 26 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #17: GFLOPs: 237.2922. Time: 1.6832 ms. Best GFLOPs: 2286.4416
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #18: GFLOPs: 1531.0086. Time: 0.2609 ms. Best GFLOPs: 2286.4416
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5824 // 728)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 728 // 26)
                                        v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 312 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 1, 2, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 26 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused // 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 26 // 13 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 13, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(2, 32, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 32 + i1_4_init)
                            yy = T.axis.spatial(52, i2_3_init * 26 + i2_4_init)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(57):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0)
                                    v2 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 54)
                                    v3 = T.axis.spatial(54, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 54)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 2916)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(64, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 576)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 32, 26, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 32 + i1_4)
                                yy = T.axis.spatial(52, i2_3 * 26 + i2_4)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 52, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 32 + ax1)
                            v2 = T.axis.spatial(52, ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused * 26 + i0_2_i1_2_i2_2_i3_2_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 26])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 26, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #21: GFLOPs: 3324.7954. Time: 0.1201 ms. Best GFLOPs: 3324.7954
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #22: GFLOPs: 855.2950. Time: 0.4670 ms. Best GFLOPs: 3324.7954
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(26, 2, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i2_3_init)
                            xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3024 // 1512)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1512 // 54)
                                        v3 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 54)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3024)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 18)
                                    v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 18 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 1152)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 26, 2, 1, 3, 1, 1, 8, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i2_3)
                                xx = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 26, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax2)
                            v3 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 13 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 26, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(16, 2, 4, 4, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i3_3_init * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(27):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5616 // 2808)
                                        v2 = T.axis.spatial(54, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2808 // 52)
                                        v3 = T.axis.spatial(54, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(64, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 16, 2, 4, 2, 1, 1, 1, 4, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + i2_3)
                                xx = T.axis.spatial(52, i3_3 * 13 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 2, 52):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_2_i1_2_i2_2_i3_2_fused // 26 * 64 + ax1)
                            v2 = T.axis.spatial(52, i0_2_i1_2_i2_2_i3_2_fused % 26 * 2 + ax2)
                            v3 = T.axis.spatial(52, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 16, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #25: GFLOPs: 1406.0781. Time: 0.2841 ms. Best GFLOPs: 3324.7954
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #26: GFLOPs: 2029.6957. Time: 0.1968 ms. Best GFLOPs: 3324.7954
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #27: GFLOPs: 1267.4586. Time: 0.3151 ms. Best GFLOPs: 3324.7954
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(128, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4992 // 312)
                                        v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 312 // 52)
                                        v3 = T.axis.spatial(54, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 52)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                xx = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax2)
                            v3 = T.axis.spatial(52, i0_1_i1_1_i2_1_i3_1_fused % 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 64, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 13, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #29: GFLOPs: 1041.7366. Time: 0.3834 ms. Best GFLOPs: 3324.7954
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 54, 54), "float32"], placeholder_1: T.Buffer[(128, 64, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 128, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 128, 52, 52), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 128, 52, 52], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 54, 54], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([128, 64, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(4, 2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init)
                            yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i2_3_init)
                            xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 2912 // 728)
                                    v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 26 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 728 // 28)
                                    v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(64, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 3, 1, 4, 2, 26, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3)
                                yy = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i2_3)
                                xx = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 54, 54], "float32"], ["TENSOR", [128, 64, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(128, i0_0_i1_0_i2_0_i3_0_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 26 + i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax2)
                            v3 = T.axis.spatial(52, i0_0_i1_0_i2_0_i3_0_fused % 2 * 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 26, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:54:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"] Trial #31: GFLOPs: 3964.9364. Time: 0.1007 ms. Best GFLOPs: 3964.9364
[03:54:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 178
Total latency (us): 573.798

[03:54:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_pad_6"] Trial #0: GFLOPs: 0.0000. Time: 0.0276 ms. Best GFLOPs: 0.0000
[03:54:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_pad_6"] Trial #1: GFLOPs: 0.0000. Time: 0.0085 ms. Best GFLOPs: 0.0000
[03:54:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_pad_6"] Trial #2: GFLOPs: 0.0000. Time: 0.0099 ms. Best GFLOPs: 0.0000
[03:54:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_pad_6"] Trial #3: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[03:54:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_pad_6"] Trial #4: GFLOPs: 0.0000. Time: 0.0158 ms. Best GFLOPs: 0.0000
[03:54:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_pad_6"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 183
Total latency (us): 577.733

[03:54:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_3"] Trial #0: GFLOPs: 45.2894. Time: 0.0076 ms. Best GFLOPs: 45.2894
[03:54:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_3"] Trial #1: GFLOPs: 16.0239. Time: 0.0216 ms. Best GFLOPs: 45.2894
[03:54:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_3"] Trial #2: GFLOPs: 9.0507. Time: 0.0382 ms. Best GFLOPs: 45.2894
[03:54:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_3"] Trial #3: GFLOPs: 53.9629. Time: 0.0064 ms. Best GFLOPs: 53.9629
[03:54:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_max_pool2d_3"] Trial #4: GFLOPs: 12.1354. Time: 0.0285 ms. Best GFLOPs: 53.9629
[03:55:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 188
Total latency (us): 584.147

[03:55:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_pad_7"] Trial #0: GFLOPs: 0.0000. Time: 0.0072 ms. Best GFLOPs: 0.0000
[03:55:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_pad_7"] Trial #1: GFLOPs: 0.0000. Time: 0.0166 ms. Best GFLOPs: 0.0000
[03:55:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_pad_7"] Trial #2: GFLOPs: 0.0000. Time: 0.0263 ms. Best GFLOPs: 0.0000
[03:55:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_pad_7"] Trial #3: GFLOPs: 0.0000. Time: 0.0151 ms. Best GFLOPs: 0.0000
[03:55:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_pad_7"] Trial #4: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[03:55:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_pad_7"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |            N/A |          N/A |                   N/A |      0 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 193
Total latency (us): 591.356

[03:55:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #0: GFLOPs: 37.8457. Time: 10.5446 ms. Best GFLOPs: 37.8457
[03:55:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1560 // 195)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 1, 13, 8, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused // 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:55:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 16, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(26, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 420)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 13 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 420 // 28)
                                        v3 = T.axis.spatial(28, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1680)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(178):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 9216)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 2, 1, 1, 2, 3, 1, 1, 16, 1, 26):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(26, i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 32 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(26, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 26])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(338, thread="threadIdx.x"):
                    for i1_3_init in T.serial(64):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                            xx = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(338, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1456 // 364)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 13 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 364 // 28)
                                        v3 = T.axis.spatial(28, ((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1456)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(338, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 64, 1, 1, 4, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused // 26)
                                xx = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused * 13 + i0_2_i1_2_i2_2_i3_2_fused // 26 + ax2)
                            v3 = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 64, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 338, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 338, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:55:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 1560 // 390)
                                    v2 = T.axis.spatial(28, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 390 // 15)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 1560)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 64, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #5: GFLOPs: 229.3753. Time: 1.7398 ms. Best GFLOPs: 229.3753
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i2_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6720 // 420)
                                        v2 = T.axis.spatial(28, ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 420 // 15)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + ((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6720)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) // 144)
                                    v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 144 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 < 2304)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 2, 1, 4, 3, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 208, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 208])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 13 + i2_3_init)
                            xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 26)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1248 // 78)
                                        v2 = T.axis.spatial(28, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 78 // 3)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 26 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(128, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 13, 1, 4, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 13 + i2_3)
                                xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 26)
                                rc = T.axis.reduce(128, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 26 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 13 + ax2)
                            v3 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[26, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #8: GFLOPs: 3127.7798. Time: 0.1276 ms. Best GFLOPs: 3127.7798
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #9: GFLOPs: 2972.7064. Time: 0.1342 ms. Best GFLOPs: 3127.7798
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(2, 26):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(26, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 1680 // 420)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 420 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 1680)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) // 36)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 26):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(26, i3_4)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(26, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 26])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 104])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #11: GFLOPs: 1498.3931. Time: 0.2663 ms. Best GFLOPs: 3127.7798
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4_init)
                            yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 26 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 1456 // 728)
                                    v2 = T.axis.spatial(28, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 728 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 6 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 192)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4)
                                yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 26 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 26 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 26 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 26, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i2_4_init)
                            xx = T.axis.spatial(26, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(85):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2704 // 676)
                                    v2 = T.axis.spatial(28, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 676 // 26)
                                    v3 = T.axis.spatial(28, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 26)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 2704)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 26, 4, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i2_4)
                                xx = T.axis.spatial(26, i3_3)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax2)
                            v3 = T.axis.spatial(26, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 26, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #14: GFLOPs: 412.2990. Time: 0.9679 ms. Best GFLOPs: 3127.7798
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #15: GFLOPs: 9.3322. Time: 42.7623 ms. Best GFLOPs: 3127.7798
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(208, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 8, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4_init)
                            yy = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i2_4_init)
                            xx = T.axis.spatial(26, i3_3_init * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 5824 // 728)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 728 // 26)
                                    v3 = T.axis.spatial(28, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(208, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 624 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 8, 2, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_4)
                                yy = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + i2_4)
                                xx = T.axis.spatial(26, i3_3 * 13 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 13 * 2 + ax2)
                            v3 = T.axis.spatial(26, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 208])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 208, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #17: GFLOPs: 767.2915. Time: 0.5201 ms. Best GFLOPs: 3127.7798
[03:55:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i3_4_init in T.grid(2, 8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_4_init)
                            yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            xx = T.axis.spatial(26, i3_3_init * 13 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1456 // 728)
                                        v2 = T.axis.spatial(28, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 728 // 28)
                                        v3 = T.axis.spatial(28, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1456)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 8, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_4)
                                yy = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(26, i3_3 * 13 + i3_4)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 26):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(26, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3120 // 390)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 390 // 26)
                                        v3 = T.axis.spatial(28, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 24 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 1536)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 1, 1, 2, 4, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 2 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 52])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #20: GFLOPs: 3303.6402. Time: 0.1208 ms. Best GFLOPs: 3303.6402
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init in T.grid(2, 13, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(26, i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 2912 // 728)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 728 // 26)
                                    v3 = T.axis.spatial(28, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 26)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 12 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 1536)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 13, 1, 2, 3, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(26, i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 26, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(26, ax2)
                            v3 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 2 * 13 + i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 52])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #22: GFLOPs: 3665.8281. Time: 0.1089 ms. Best GFLOPs: 3665.8281
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #23: GFLOPs: 22.3788. Time: 17.8324 ms. Best GFLOPs: 3665.8281
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #24: GFLOPs: 5.6845. Time: 70.2027 ms. Best GFLOPs: 3665.8281
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #25: GFLOPs: 1031.8268. Time: 0.3868 ms. Best GFLOPs: 3665.8281
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(416, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_4_init)
                            yy = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26)
                            xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 2912 // 728)
                                    v2 = T.axis.spatial(28, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 728 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(128, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + i1_4)
                                yy = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26)
                                xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i3_3)
                                rc = T.axis.reduce(128, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused // 13 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 2 + ax1)
                            v2 = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26 + ax2)
                            v3 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 104])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 104, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init in T.serial(16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_4_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                            xx = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 896 // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 112 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1 < 896)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(128, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 1, 1, 8, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + i1_4)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                xx = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26)
                                rc = T.axis.reduce(128, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 26 * 16 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax2)
                            v3 = T.axis.spatial(26, i0_2_i1_2_i2_2_i3_2_fused % 26 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 2, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 2, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 26, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 52])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 52, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_3_init)
                            xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1664 // 52)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 52 // 26)
                                        v3 = T.axis.spatial(28, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 26)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(128, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 32, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_3)
                                xx = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(128, i4_0 * 32 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + ax2)
                            v3 = T.axis.spatial(26, i0_1_i1_1_i2_1_i3_1_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #29: GFLOPs: 1417.5074. Time: 0.2815 ms. Best GFLOPs: 3665.8281
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 128, 28, 28), "float32"], placeholder_1: T.Buffer[(256, 128, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 256, 26, 26), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 256, 26, 26], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 128, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([256, 128, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(338, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init in T.serial(16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_4_init)
                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                            xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32 // 16)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16 // 4)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(128, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + i1_4)
                                yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(128, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 128, 28, 28], "float32"], ["TENSOR", [256, 128, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 169 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 16 + ax1)
                            v2 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:55:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"] Trial #31: GFLOPs: 2471.4114. Time: 0.1615 ms. Best GFLOPs: 3665.8281
[03:56:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 225
Total latency (us): 700.217

[03:56:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_pad_8"] Trial #0: GFLOPs: 0.0000. Time: 0.0691 ms. Best GFLOPs: 0.0000
[03:56:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_pad_8"] Trial #1: GFLOPs: 0.0000. Time: 0.0113 ms. Best GFLOPs: 0.0000
[03:56:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_pad_8"] Trial #2: GFLOPs: 0.0000. Time: 0.0227 ms. Best GFLOPs: 0.0000
[03:56:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_pad_8"] Trial #3: GFLOPs: 0.0000. Time: 0.0650 ms. Best GFLOPs: 0.0000
[03:56:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_pad_8"] Trial #4: GFLOPs: 0.0000. Time: 0.0092 ms. Best GFLOPs: 0.0000
[03:56:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_pad_8"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 230
Total latency (us): 709.459

[03:56:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_max_pool2d_4"] Trial #0: GFLOPs: 11.1682. Time: 0.0155 ms. Best GFLOPs: 11.1682
[03:56:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_max_pool2d_4"] Trial #1: GFLOPs: 7.5571. Time: 0.0229 ms. Best GFLOPs: 11.1682
[03:56:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_max_pool2d_4"] Trial #2: GFLOPs: 49.9587. Time: 0.0035 ms. Best GFLOPs: 49.9587
[03:56:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_max_pool2d_4"] Trial #3: GFLOPs: 39.8182. Time: 0.0043 ms. Best GFLOPs: 49.9587
[03:56:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_max_pool2d_4"] Trial #4: GFLOPs: 21.3790. Time: 0.0081 ms. Best GFLOPs: 49.9587
[03:57:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_max_pool2d_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 235
Total latency (us): 712.923

[03:57:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_pad_9"] Trial #0: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[03:57:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_pad_9"] Trial #1: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[03:57:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_pad_9"] Trial #2: GFLOPs: 0.0000. Time: 0.0120 ms. Best GFLOPs: 0.0000
[03:57:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_pad_9"] Trial #3: GFLOPs: 0.0000. Time: 0.0217 ms. Best GFLOPs: 0.0000
[03:57:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_pad_9"] Trial #4: GFLOPs: 0.0000. Time: 0.0042 ms. Best GFLOPs: 0.0000
[03:57:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_pad_9"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 240
Total latency (us): 716.234

[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #0: GFLOPs: 1182.5033. Time: 0.3373 ms. Best GFLOPs: 1182.5033
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #1: GFLOPs: 25.4222. Time: 15.6908 ms. Best GFLOPs: 1182.5033
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #2: GFLOPs: 2564.8029. Time: 0.1555 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #3: GFLOPs: 1528.6947. Time: 0.2609 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #4: GFLOPs: 25.2816. Time: 15.7780 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(676, thread="threadIdx.x"):
                    for i1_3_init in T.serial(8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 8 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 169 // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(676, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) % 780 // 195)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) % 195 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1 < 780)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(676, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 1352 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 676 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 8 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 169 // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 169 * 8 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 169 // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 676])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 676, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #6: GFLOPs: 19.3010. Time: 20.6670 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #7: GFLOPs: 4.8288. Time: 82.6069 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #8: GFLOPs: 911.0500. Time: 0.4378 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(104, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init in T.serial(8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 780 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 780)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 1, 1, 4, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 8 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 4, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #10: GFLOPs: 203.8753. Time: 1.9566 ms. Best GFLOPs: 2564.8029
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(13, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i0_0_i1_0_i2_0_i3_0_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 156 // 39)
                                        v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 39 // 3)
                                        v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 156)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 13, 1, 2, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_3, i0_0_i1_0_i2_0_i3_0_fused])
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #12: GFLOPs: 3314.6682. Time: 0.1203 ms. Best GFLOPs: 3314.6682
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 780 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 780)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 13, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i3_3)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 64, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(169, thread="threadIdx.x"):
                    for i1_3_init in T.serial(32):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 780 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 780)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(169, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(256, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 338 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 169 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 32, 1, 1, 4, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(256, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 16, 1, 32, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 169, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 169, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(832, thread="threadIdx.x"):
                    for i2_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(832, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 390 // 195)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 390)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(832, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 13, 1, 2, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 64, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 832, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 832, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 8, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3120 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 16, 3, 1, 1, 8, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #17: GFLOPs: 1290.5357. Time: 0.3091 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(13, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_4_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3120 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3120)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(256, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_4])
                                rc = T.axis.reduce(256, i4_0 * 16 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #19: GFLOPs: 646.3671. Time: 0.6171 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #20: GFLOPs: 2732.0686. Time: 0.1460 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(13, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1800)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(256, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 3, 1, 1, 13, 13, 1, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_3, i3_3])
                                rc = T.axis.reduce(256, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #22: GFLOPs: 651.4962. Time: 0.6123 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #23: GFLOPs: 8.8005. Time: 45.3264 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #24: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3_init * 8 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 + 0)
                                    v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 39 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 39)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(256, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3 * 8 + i1_4)
                                yy, xx, rc, ry, rx = T.axis.remap("SSRRR", [i0_0_i1_0_i2_0_i3_0_fused, i0_1_i1_1_i2_1_i3_1_fused, i4_0, i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #25: GFLOPs: 593.0297. Time: 0.6726 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #26: GFLOPs: 554.5907. Time: 0.7193 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6240 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 96)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 96 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 2, 13, 1, 1, 1, 3, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(256, i4_0 * 32 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #28: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(52, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_4_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 450)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(256, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(256, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_1_i1_1_i2_1_i3_1_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #29: GFLOPs: 17.3937. Time: 22.9333 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #30: GFLOPs: 410.8642. Time: 0.9709 ms. Best GFLOPs: 3314.6682
[03:57:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 15, 15), "float32"], placeholder_1: T.Buffer[(512, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 512, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 512, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 512, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([512, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(169):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 5408 // 169)
                                    v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 169 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(256, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(256, i4_0 * 32 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 15, 15], "float32"], ["TENSOR", [512, 256, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(512, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 32, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 16, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[03:58:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 272
Total latency (us): 836.576

[03:58:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_pad_10"] Trial #0: GFLOPs: 0.0000. Time: 0.0049 ms. Best GFLOPs: 0.0000
[03:58:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_pad_10"] Trial #1: GFLOPs: 0.0000. Time: 0.0034 ms. Best GFLOPs: 0.0000
[03:58:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_pad_10"] Trial #2: GFLOPs: 0.0000. Time: 0.0035 ms. Best GFLOPs: 0.0000
[03:58:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_pad_10"] Trial #3: GFLOPs: 0.0000. Time: 0.0149 ms. Best GFLOPs: 0.0000
[03:58:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_pad_10"] Trial #4: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[03:58:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_pad_10"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 277
Total latency (us): 840.006

[03:58:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_max_pool2d_5"] Trial #0: GFLOPs: 101.5952. Time: 0.0034 ms. Best GFLOPs: 101.5952
[03:58:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_max_pool2d_5"] Trial #1: GFLOPs: 7.9854. Time: 0.0433 ms. Best GFLOPs: 101.5952
[03:58:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_max_pool2d_5"] Trial #2: GFLOPs: 41.7377. Time: 0.0083 ms. Best GFLOPs: 101.5952
[03:58:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_max_pool2d_5"] Trial #3: GFLOPs: 10.3708. Time: 0.0334 ms. Best GFLOPs: 101.5952
[03:58:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_max_pool2d_5"] Trial #4: GFLOPs: 65.9009. Time: 0.0053 ms. Best GFLOPs: 101.5952
[03:59:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_max_pool2d_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 282
Total latency (us): 843.413

[03:59:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_pad_11"] Trial #0: GFLOPs: 0.0000. Time: 0.0056 ms. Best GFLOPs: 0.0000
[03:59:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_pad_11"] Trial #1: GFLOPs: 0.0000. Time: 0.0262 ms. Best GFLOPs: 0.0000
[03:59:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_pad_11"] Trial #2: GFLOPs: 0.0000. Time: 0.0084 ms. Best GFLOPs: 0.0000
[03:59:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_pad_11"] Trial #3: GFLOPs: 0.0000. Time: 0.0034 ms. Best GFLOPs: 0.0000
[03:59:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_pad_11"] Trial #4: GFLOPs: 0.0000. Time: 0.0062 ms. Best GFLOPs: 0.0000
[04:00:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_pad_11"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 287
Total latency (us): 846.844

[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #0: GFLOPs: 39.5554. Time: 40.3290 ms. Best GFLOPs: 39.5554
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #1: GFLOPs: 21.4493. Time: 74.3720 ms. Best GFLOPs: 39.5554
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #2: GFLOPs: 2971.4623. Time: 0.5369 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #3: GFLOPs: 214.6695. Time: 7.4311 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(16, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 13 * 512 + i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 390 // 195)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 195 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 390)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 16, 1, 1, 1, 3, 1, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 13 * 512 + i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i3_4)
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused // 13 * 512 + i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #5: GFLOPs: 1648.3003. Time: 0.9678 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 13, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_4_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 780 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 780)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(512, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 13, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_4])
                                rc = T.axis.reduce(512, i4_0 * 4 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 64, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #7: GFLOPs: 585.7831. Time: 2.7232 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3120 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(512, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 208 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 3, 1, 1, 4, 1, 1, 8, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(512, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 4, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #9: GFLOPs: 289.6111. Time: 5.5082 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #10: GFLOPs: 23.0172. Time: 69.3061 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #11: GFLOPs: 34.8090. Time: 45.8282 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #12: GFLOPs: 2581.6257. Time: 0.6179 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #13: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 + 0)
                                    v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax0_ax1_ax2_ax3_fused_1 % 39 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + ax0_ax1_ax2_ax3_fused_1 % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 39)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(512, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                xx, rc, ry, rx = T.axis.remap("SRRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_0, i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 128, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #14: GFLOPs: 1066.2774. Time: 1.4961 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1800)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 72 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 8, 3, 3, 1, 2, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_4, i0_1_i1_1_i2_1_i3_1_fused])
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #16: GFLOPs: 100.9130. Time: 15.8080 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #17: GFLOPs: 1563.4055. Time: 1.0204 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #18: GFLOPs: 607.1643. Time: 2.6273 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #19: GFLOPs: 2704.7224. Time: 0.5898 ms. Best GFLOPs: 2971.4623
[04:00:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #20: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(52, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 + 0)
                                    v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax0_ax1_ax2_ax3_fused_1 % 39 // 13)
                                    v3 = T.axis.spatial(15, i6_0 + ax0_ax1_ax2_ax3_fused_1 % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 39)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(512, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                xx, rc, ry, rx = T.axis.remap("SRRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_0, i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 256 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 128, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #21: GFLOPs: 18.7064. Time: 85.2771 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #22: GFLOPs: 137.8753. Time: 11.5701 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #23: GFLOPs: 1299.8933. Time: 1.2272 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #24: GFLOPs: 74.6311. Time: 21.3749 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(26, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i3_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            xx = T.axis.spatial(13, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 390 // 195)
                                    v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 195 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 < 390)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(512, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 6 // 3)
                                    v2 = T.axis.spatial(3, i5_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 3, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                xx = T.axis.spatial(13, i3_3)
                                rc = T.axis.reduce(512, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 256 + i0_1_i1_1_i2_1_i3_1_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 128, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 128])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #26: GFLOPs: 93.9389. Time: 16.9816 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #27: GFLOPs: 12.4040. Time: 128.6066 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #28: GFLOPs: 2612.2707. Time: 0.6107 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 512, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 512, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 512, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 512, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(13, 2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1800)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(144):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 72)
                                    v1 = T.axis.spatial(512, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 72 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 1, 13, 8, 1, 1, 1, 2, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_3])
                                rc = T.axis.reduce(512, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 512, 15, 15], "float32"], ["TENSOR", [1024, 512, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #30: GFLOPs: 1003.7579. Time: 1.5893 ms. Best GFLOPs: 2971.4623
[04:00:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"] Trial #31: GFLOPs: 1867.1749. Time: 0.8544 ms. Best GFLOPs: 2971.4623
[04:01:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 319
Total latency (us): 1383.69

[04:01:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_pad_12"] Trial #0: GFLOPs: 0.0000. Time: 0.0176 ms. Best GFLOPs: 0.0000
[04:01:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_pad_12"] Trial #1: GFLOPs: 0.0000. Time: 0.0499 ms. Best GFLOPs: 0.0000
[04:01:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_pad_12"] Trial #2: GFLOPs: 0.0000. Time: 0.0080 ms. Best GFLOPs: 0.0000
[04:01:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_pad_12"] Trial #3: GFLOPs: 0.0000. Time: 0.0219 ms. Best GFLOPs: 0.0000
[04:01:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_pad_12"] Trial #4: GFLOPs: 0.0000. Time: 0.0160 ms. Best GFLOPs: 0.0000
[04:01:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_pad_12"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 324
Total latency (us): 1391.74

[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #0: GFLOPs: 197.6633. Time: 16.1391 ms. Best GFLOPs: 197.6633
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #1: GFLOPs: 13.5184. Time: 235.9836 ms. Best GFLOPs: 197.6633
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #2: GFLOPs: 32.8346. Time: 97.1570 ms. Best GFLOPs: 197.6633
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #3: GFLOPs: 396.4474. Time: 8.0468 ms. Best GFLOPs: 396.4474
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i3_4_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 + 0)
                                    v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused + i5_0 + 0)
                                    v3 = T.axis.spatial(15, i6_0 + ax0_ax1_ax2_ax3_fused_1 % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy, xx, rc, ry, rx = T.axis.remap("SSRRR", [i0_0_i1_0_i2_0_i3_0_fused, i3_4, i4_0, i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #5: GFLOPs: 2550.1019. Time: 1.2510 ms. Best GFLOPs: 2550.1019
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(16, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_4_init)
                            yy = T.axis.spatial(13, i2_4_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 450)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(89):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 18 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 9216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 16, 13, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_4)
                                yy = T.axis.spatial(13, i2_4)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_1, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 4, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 13])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(208, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 312 // 39)
                                        v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 39 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 1, 1, 1, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #8: GFLOPs: 930.6687. Time: 3.4278 ms. Best GFLOPs: 2550.1019
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #9: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(13, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(512, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i0_0_i1_0_i2_0_i3_0_fused, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(1024, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 + 0)
                                    v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused + i5_0 + 0)
                                    v3 = T.axis.spatial(15, i6_0 + ax0_ax1_ax2_ax3_fused_1 % 13)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 13)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2)
                                        v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx, rc, ry, rx = T.axis.remap("SSRRR", [i0_0_i1_0_i2_0_i3_0_fused, i0_1_i1_1_i2_1_i3_1_fused, i4_0, i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 512, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1024, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 512])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 512, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #10: GFLOPs: 36.8662. Time: 86.5322 ms. Best GFLOPs: 2550.1019
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #11: GFLOPs: 1467.6756. Time: 2.1736 ms. Best GFLOPs: 2550.1019
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #12: GFLOPs: 677.9790. Time: 4.7053 ms. Best GFLOPs: 2550.1019
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #13: GFLOPs: 76.4580. Time: 41.7238 ms. Best GFLOPs: 2550.1019
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 360 // 45)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 45 // 3)
                                    v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 360)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(288):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 72)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 72 // 9)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 13, 1, 8, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 4, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #15: GFLOPs: 3987.3082. Time: 0.8001 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #16: GFLOPs: 147.7595. Time: 21.5899 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #17: GFLOPs: 2217.7050. Time: 1.4385 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(104, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 120 // 15)
                                    v2 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    v3 = T.axis.spatial(15, i6_0 + i0_0_i1_0_i2_0_i3_0_fused % 13 + 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 120)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(96):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 24 // 3)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 13, 1, 2, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_1_i1_1_i2_1_i3_1_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 32, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #19: GFLOPs: 1929.6942. Time: 1.6532 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(416, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 13, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(416, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 676 // 169)
                                        v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 169 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 676)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(416, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(1024, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 416 + ax0_ax1_ax2_ax3_fused_1 < 2048)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 13, 1, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 512 + i0_1_i1_1_i2_1_i3_1_fused * 128 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 32, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 416, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 416])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #21: GFLOPs: 363.9615. Time: 8.7650 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #22: GFLOPs: 1590.5192. Time: 2.0057 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #23: GFLOPs: 366.4706. Time: 8.7050 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #24: GFLOPs: 1062.3254. Time: 3.0030 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #25: GFLOPs: 1865.0746. Time: 1.7104 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(13, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4_init)
                            yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            xx = T.axis.spatial(13, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6240 // 195)
                                        v2 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 195 // 13)
                                        v3 = T.axis.spatial(15, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 96)
                                        v1 = T.axis.spatial(1024, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 96 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 156 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 13, 8, 3, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + i1_4)
                                yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                xx = T.axis.spatial(13, i3_3)
                                rc = T.axis.reduce(1024, i4_0 * 32 + i4_1 * 8 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 13):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 4 + ax1)
                            v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax2)
                            v3 = T.axis.spatial(13, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 2, 4, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #27: GFLOPs: 1766.6984. Time: 1.8057 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #28: GFLOPs: 455.1703. Time: 7.0086 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 15, 15), "float32"], placeholder_1: T.Buffer[(1024, 1024, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1024, 1, 1), "float32"], T_leaky_relu: T.Buffer[(1, 1024, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1024, 1024, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(104, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(512, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 390 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 390)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(104, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(1024, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 13, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + i1_3)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(1024, i4_0 * 2 + i4_1)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 15, 15], "float32"], ["TENSOR", [1024, 1024, 3, 3], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 2 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_leaky_relu[v0, v1, v2, v3])
                            T_leaky_relu[v0, v1, v2, v3] = T.Select(T.float32(0) < conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], (conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]) * T.float32(0.10000000149011612))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_leaky_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 4, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[512, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 104, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 104, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #30: GFLOPs: 202.1112. Time: 15.7840 ms. Best GFLOPs: 3987.3082
[04:01:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"] Trial #31: GFLOPs: 717.7849. Time: 4.4444 ms. Best GFLOPs: 3987.3082
[04:02:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 356
Total latency (us): 2191.8

[04:02:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_pad_13"] Trial #0: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[04:02:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_pad_13"] Trial #1: GFLOPs: 0.0000. Time: 0.0035 ms. Best GFLOPs: 0.0000
[04:02:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_pad_13"] Trial #2: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[04:02:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_pad_13"] Trial #3: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[04:02:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_pad_13"] Trial #4: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[04:02:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_pad_13"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |            N/A |          N/A |                   N/A |      0 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 361
Total latency (us): 2195.12

[04:03:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 269.0343. Time: 0.1609 ms. Best GFLOPs: 269.0343
[04:03:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 237.3837. Time: 0.1823 ms. Best GFLOPs: 269.0343
[04:03:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 333.2051. Time: 0.1299 ms. Best GFLOPs: 333.2051
[04:03:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 259.6272. Time: 0.1667 ms. Best GFLOPs: 333.2051
[04:03:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_conv2d_add"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1024, 13, 13), "float32"], placeholder_1: T.Buffer[(125, 1024, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 125, 1, 1), "float32"], T_add: T.Buffer[(1, 125, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 125, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1024, 13, 13], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([125, 1024, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(13, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(125, thread="threadIdx.x"):
                    for i2_3_init in T.serial(13):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff, yy, xx = T.axis.remap("SSS", [i0_2_i1_2_i2_2_i3_2_fused, i2_3_init, i0_0_i1_0_i2_0_i3_0_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 13, 13], "float32"], ["TENSOR", [125, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(125, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1024, i4_0 * 8 + ax0_ax1_ax2_ax3_fused_1 // 13)
                                    v2 = T.axis.spatial(13, ax0_ax1_ax2_ax3_fused_1 % 13)
                                    v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 104)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(125, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(125, (ax0_ax1_ax2_ax3_fused_0 * 250 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(1024, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 250 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 13, 1, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff, yy, xx = T.axis.remap("SSS", [i0_2_i1_2_i2_2_i3_2_fused, i2_3, i0_0_i1_0_i2_0_i3_0_fused])
                                rc = T.axis.reduce(1024, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1024, 13, 13], "float32"], ["TENSOR", [125, 1024, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 13, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(125, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 125, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[128, 2, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 125])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 125, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 31.0757. Time: 1.3929 ms. Best GFLOPs: 333.2051
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #6: GFLOPs: 120.9905. Time: 0.3578 ms. Best GFLOPs: 333.2051
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #7: GFLOPs: 240.7159. Time: 0.1798 ms. Best GFLOPs: 333.2051
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 137.9599. Time: 0.3138 ms. Best GFLOPs: 333.2051
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #9: GFLOPs: 340.5309. Time: 0.1271 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #10: GFLOPs: 148.6871. Time: 0.2911 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 268.2262. Time: 0.1614 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 92.7022. Time: 0.4669 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #13: GFLOPs: 296.2597. Time: 0.1461 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 38.8614. Time: 1.1138 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 90.2468. Time: 0.4796 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #16: GFLOPs: 234.9300. Time: 0.1842 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #17: GFLOPs: 107.0663. Time: 0.4043 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #18: GFLOPs: 13.5284. Time: 3.1996 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 145.1839. Time: 0.2981 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 145.2388. Time: 0.2980 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #21: GFLOPs: 244.1605. Time: 0.1773 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 5.9347. Time: 7.2935 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #23: GFLOPs: 6.1099. Time: 7.0844 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 206.2120. Time: 0.2099 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 130.2299. Time: 0.3324 ms. Best GFLOPs: 340.5309
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 776.1098. Time: 0.0558 ms. Best GFLOPs: 776.1098
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #27: GFLOPs: 1092.0313. Time: 0.0396 ms. Best GFLOPs: 1092.0313
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #28: GFLOPs: 10.0744. Time: 4.2966 ms. Best GFLOPs: 1092.0313
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #29: GFLOPs: 128.2128. Time: 0.3376 ms. Best GFLOPs: 1092.0313
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 285.4376. Time: 0.1516 ms. Best GFLOPs: 1092.0313
[04:03:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add"] Trial #31: GFLOPs: 145.8084. Time: 0.2969 ms. Best GFLOPs: 1092.0313
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_nn_conv2d_add"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |            
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |            
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |            
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |            
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |            
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |            
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |            
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_nn_conv2d_add_nn_leaky_relu_7"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #27 has finished. Remaining task(s): 29
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_conv2d_add_nn_leaky_relu_6"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #25 has finished. Remaining task(s): 28
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_conv2d_add_nn_leaky_relu_5"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 27
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_nn_leaky_relu_4"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 26
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_nn_leaky_relu_2"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 25
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_leaky_relu_3"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 24
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_leaky_relu_1"
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 23
[04:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_pad"
[04:03:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:03:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:04:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:04:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:04:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:04:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:05:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:05:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:06:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:06:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:06:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |            
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_leaky_relu"
[04:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 22
[04:06:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_max_pool2d"
[04:06:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:06:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:06:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:06:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:06:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:07:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:07:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:08:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:08:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:08:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_max_pool2d"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |            
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #29: "fused_nn_conv2d_add"
[04:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #29 has finished. Remaining task(s): 21
[04:08:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_pad_2"
[04:08:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:08:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:09:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:09:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:09:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:09:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:10:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:10:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:10:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:10:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:10:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:10:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:10:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:10:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_pad_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:10:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_pad"
[04:10:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:10:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:10:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:10:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:11:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:11:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:12:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:12:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:12:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:12:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:12:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:12:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_max_pool2d"
[04:12:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:12:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:12:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:12:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:13:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:13:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:14:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:14:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:15:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_max_pool2d"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_pad_1"
[04:15:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:15:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:16:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:16:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:18:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:19:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:20:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:22:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:23:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:23:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:23:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:23:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:23:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:23:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_pad_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:23:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_pad_2"
[04:23:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:23:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:23:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:23:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:23:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:24:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:24:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:25:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:25:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:25:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:25:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:25:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:25:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:25:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_pad_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:25:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_pad"
[04:25:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:25:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:25:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:25:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:25:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:26:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:26:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:27:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:27:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:27:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:27:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:27:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:27:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:27:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d_1"
[04:27:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:27:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:27:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:27:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:28:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:28:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:29:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:30:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:30:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:30:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:30:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_max_pool2d_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:30:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_max_pool2d"
[04:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:30:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:30:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:30:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:31:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:31:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:32:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:33:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:33:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:33:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:33:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:33:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:33:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:33:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_max_pool2d"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:33:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_pad_4"
[04:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:33:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[04:33:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:34:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[04:34:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[04:34:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[04:35:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[04:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:35:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:35:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:35:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:35:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_pad_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:35:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_pad"
[04:35:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:35:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:35:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:35:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:36:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:36:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:36:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:36:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:37:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_pad_2"
[04:37:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:37:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:37:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:37:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:37:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:37:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:38:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:38:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:38:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:38:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:38:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_pad_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:38:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_max_pool2d"
[04:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:38:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:38:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:39:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:39:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:40:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:41:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:41:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:41:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:41:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:41:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:41:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:41:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_max_pool2d"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |            
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:41:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_pad"
[04:41:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:41:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:41:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:41:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:41:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:42:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:42:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:42:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc8ed68)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffcc2608)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdc92a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffd04c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffb94a08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe14758)]: 0 failure(s)
[04:42:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:42:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:42:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:42:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 20
[04:42:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_pad_1"
[04:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:43:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:43:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:45:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:46:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:48:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:49:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[04:50:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:50:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:50:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:50:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:50:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:50:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_pad_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:50:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_pad_2"
[04:50:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:50:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:50:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:50:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:51:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:51:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:51:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:52:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[04:52:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:52:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:52:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:52:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:52:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:52:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_pad_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |            
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:52:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_max_pool2d"
[04:52:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:52:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:52:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:52:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:53:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:53:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:53:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:54:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc25dc8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc97dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc19778)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffed94d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffc63678)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdf0c88)]: 0 failure(s)
[04:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:54:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 19
[04:54:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_pad_8"
[04:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:54:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:55:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[04:55:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:55:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[04:55:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[04:55:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[04:55:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[04:56:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:56:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:56:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:56:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:56:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:56:19] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_pad_8"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:56:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d_1"
[04:56:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:56:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:56:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:56:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:56:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:57:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:57:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:58:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[04:59:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:59:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:59:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:59:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:59:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:59:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_max_pool2d_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[04:59:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_nn_pad"
[04:59:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:59:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:00:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[05:00:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:02:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[05:04:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[05:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[05:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[05:08:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:08:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:08:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:08:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:08:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:08:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:08:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_pad_12"
[05:08:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:08:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:09:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[05:09:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:10:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[05:11:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[05:12:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[05:13:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[05:14:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:14:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:14:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:14:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:14:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_pad_12"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |            
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:14:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_pad_2"
[05:14:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:14:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:14:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[05:14:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:15:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[05:15:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[05:15:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[05:16:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff19068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd54268)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff2bf88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcbbf98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdc82b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffc9e2e8)]: 0 failure(s)
[05:16:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:16:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:16:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:16:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 18
[05:16:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_pad_4"
[05:16:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:16:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:16:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:16:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:16:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:17:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:17:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:17:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:18:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:18:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:18:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:18:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:18:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:18:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_pad_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:18:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_pad_7"
[05:18:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:18:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:18:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[05:18:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:19:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[05:21:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[05:21:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[05:23:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[05:23:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:23:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:23:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:23:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:23:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:23:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_pad_7"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:23:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_pad_1"
[05:23:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:23:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:24:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:24:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:26:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:27:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:28:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:29:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:30:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:30:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:30:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:30:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:30:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:30:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_pad_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:30:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_2"
[05:30:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:30:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:30:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[05:30:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:30:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[05:31:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[05:31:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[05:31:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[05:32:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:32:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:32:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:32:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:32:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:32:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:32:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_3"
[05:32:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:32:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:32:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[05:32:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:32:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[05:32:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[05:33:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[05:33:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[05:33:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:33:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:33:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:33:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:33:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:33:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:33:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d_1"
[05:33:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:33:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:33:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:33:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:34:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:34:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:34:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:35:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:35:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:35:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:35:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_max_pool2d_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_pad_5"
[05:35:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:35:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:36:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[05:36:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:37:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[05:38:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[05:39:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[05:39:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[05:40:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:40:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:40:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:40:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:40:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:40:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_pad_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:40:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_pad_3"
[05:40:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:40:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:41:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[05:41:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:42:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[05:43:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[05:44:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[05:46:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[05:46:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:46:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:46:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:46:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:46:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:46:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_pad_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:46:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_pad_1"
[05:46:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:46:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:47:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:47:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:48:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:49:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:50:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:51:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[05:52:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:52:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:52:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:52:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:52:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:52:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_pad_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:52:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_pad_4"
[05:52:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:52:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:52:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:52:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:52:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:53:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:53:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:53:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[05:54:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:54:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:54:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:54:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:54:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:54:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_pad_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:54:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_pad_8"
[05:54:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:54:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:54:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[05:54:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:54:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[05:54:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[05:54:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[05:55:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[05:55:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:55:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:55:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:55:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:55:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:55:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_pad_8"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:55:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d_1"
[05:55:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:55:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:55:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:55:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:55:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:56:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:56:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:56:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[05:57:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[05:57:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[05:57:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[05:57:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[05:57:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[05:57:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_max_pool2d_1"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[05:57:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_nn_pad"
[05:57:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[05:57:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[05:58:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[05:58:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[05:59:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:00:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:01:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:02:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:03:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:03:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:03:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |            
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:03:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_pad_1"
[06:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:03:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:04:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[06:04:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:05:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[06:06:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[06:07:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[06:08:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdca908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffef5638)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffcae718)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff890f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff0c5d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffcb6488)]: 0 failure(s)
[06:09:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:09:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:09:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:09:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 17
[06:09:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_pad_12"
[06:09:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:09:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:10:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[06:10:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:11:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[06:12:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[06:13:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[06:14:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[06:14:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:14:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:14:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_pad_12"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_pad_6"
[06:14:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:14:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:15:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[06:15:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:15:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[06:15:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[06:15:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[06:15:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[06:16:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:16:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:16:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:16:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:16:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:16:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_pad_6"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:16:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_pad_4"
[06:16:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:16:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:16:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:16:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:16:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:16:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:16:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:17:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:17:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:17:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:17:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_pad_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |            
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:17:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_max_pool2d_1"
[06:17:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:17:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:17:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[06:17:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:17:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[06:18:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[06:18:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[06:19:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc73e88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffd5d818)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe0db98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcdb308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffed1308)]: 0 failure(s)
[06:19:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:19:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:19:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:19:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 16
[06:19:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_pad_7"
[06:19:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:19:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:20:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[06:20:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:20:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[06:21:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[06:22:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[06:23:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[06:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:24:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:24:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:24:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_pad_7"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:24:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_max_pool2d_4"
[06:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:24:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:24:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[06:24:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:24:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[06:25:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[06:25:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[06:25:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[06:25:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:25:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:25:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:25:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:25:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:25:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_max_pool2d_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:25:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_pad_11"
[06:25:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:25:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:26:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[06:26:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:27:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[06:28:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[06:29:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[06:30:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[06:30:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:30:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:30:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:30:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:30:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:30:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_pad_11"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:30:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_pad_10"
[06:30:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:30:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:31:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[06:31:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:31:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[06:31:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[06:32:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[06:32:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[06:33:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:33:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:33:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:33:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:33:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:33:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_pad_10"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:33:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_max_pool2d_5"
[06:33:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:33:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:33:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[06:33:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:33:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[06:33:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[06:34:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[06:34:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[06:34:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:34:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:34:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:34:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:34:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:34:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_max_pool2d_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:34:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_pad_13"
[06:34:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:34:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:34:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[06:34:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:34:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[06:35:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[06:35:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[06:35:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[06:35:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_pad_13"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_pad_9"
[06:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:36:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[06:36:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:37:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[06:37:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[06:38:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[06:39:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[06:39:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:39:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:39:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:39:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:39:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:39:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_pad_9"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:39:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_2"
[06:39:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:39:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:39:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[06:39:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:40:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[06:40:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[06:40:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[06:41:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[06:41:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:41:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:41:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:41:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:41:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:41:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:41:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_3"
[06:41:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:41:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:41:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[06:41:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:41:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[06:42:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[06:42:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[06:42:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[06:42:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:42:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:42:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:42:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:42:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:42:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:42:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_pad_8"
[06:42:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:42:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:42:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[06:42:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:43:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[06:43:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[06:43:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[06:43:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[06:43:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:43:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:43:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_pad_8"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:43:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_pad_5"
[06:43:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:43:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:44:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[06:44:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:45:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[06:45:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[06:47:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[06:47:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[06:48:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:48:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:48:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:48:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:48:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:48:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_pad_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |            
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:48:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_pad_4"
[06:48:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:48:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:48:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:48:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:48:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:48:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:49:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:49:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe0bd38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe9e378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe517f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe4b138)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff59c18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe51718)]: 0 failure(s)
[06:49:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:49:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:49:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:49:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 15
[06:49:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_nn_pad"
[06:49:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:49:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:50:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:50:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:51:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:52:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:53:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:54:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[06:55:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[06:55:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[06:55:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[06:55:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[06:55:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[06:55:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[06:55:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_pad_3"
[06:55:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[06:55:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[06:56:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[06:56:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[06:56:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[06:57:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[06:59:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[06:59:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[07:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:00:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:00:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:00:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:00:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_pad_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:00:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_pad_12"
[07:00:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:00:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:01:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:01:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:02:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:03:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:03:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:04:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_pad_12"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:05:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_pad_7"
[07:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:05:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:05:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:05:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:06:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:07:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:08:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:09:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:10:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:10:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:10:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:10:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:10:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_pad_7"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:10:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_pad_8"
[07:10:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:10:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:10:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:10:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:10:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:10:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:10:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:10:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:10:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:10:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:10:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_pad_8"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:10:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_nn_pad"
[07:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:10:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:11:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:11:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:13:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:14:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:15:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:16:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:17:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:17:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:17:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_multiply_add_nn_pad"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:17:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_2"
[07:17:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:17:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:17:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:17:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:17:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:17:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:18:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:18:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:18:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_3"
[07:18:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:18:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:18:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[07:18:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:19:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[07:19:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[07:19:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[07:19:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[07:20:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:20:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:20:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:20:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:20:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:20:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:20:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_pad_5"
[07:20:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:20:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:20:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[07:20:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:21:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[07:22:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[07:23:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[07:24:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[07:24:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:24:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:24:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:24:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:24:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:24:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_pad_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:24:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_pad_12"
[07:24:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:24:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:25:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:25:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:26:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:26:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:27:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:28:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:28:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:28:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:28:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:28:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:28:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:28:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_pad_12"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:28:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_pad_6"
[07:28:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:28:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:28:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[07:28:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:29:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[07:29:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[07:29:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[07:29:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[07:29:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:29:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:29:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:29:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:29:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:29:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_pad_6"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |            
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:29:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_pad_8"
[07:29:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:29:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:29:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:29:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:30:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:30:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:30:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:30:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffef9968)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd0e9c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffe2baa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffb76c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x559700027558)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffe2bb48)]: 0 failure(s)
[07:30:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:30:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:30:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:30:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 14
[07:30:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_pad_3"
[07:30:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:30:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:31:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[07:31:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:32:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[07:33:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[07:34:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[07:34:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[07:35:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:35:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:35:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_pad_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:35:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_pad_7"
[07:35:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:35:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:36:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:36:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:36:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:37:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:37:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[07:38:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:38:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:38:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_pad_7"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |            
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_multiply_add_nn_pad"
[07:38:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:38:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:39:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:39:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:40:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:41:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:42:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:43:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff84d908)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe4b308)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff99958)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff2c768)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ff852fe8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffe0038)]: 0 failure(s)
[07:44:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:44:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:44:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 13
[07:44:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_max_pool2d_4"
[07:44:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:44:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:44:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[07:44:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:44:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[07:44:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[07:44:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[07:45:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[07:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_max_pool2d_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:45:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_pad_11"
[07:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:45:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:45:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[07:45:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:46:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[07:47:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[07:48:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[07:48:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[07:49:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:49:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:49:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:49:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:49:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:49:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_pad_11"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:49:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_pad_10"
[07:49:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:49:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:49:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[07:49:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:49:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[07:50:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[07:50:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[07:50:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[07:50:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:50:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:50:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:50:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:50:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:50:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_pad_10"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:50:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_max_pool2d_5"
[07:50:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:50:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:50:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[07:50:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:51:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[07:51:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[07:51:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[07:51:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[07:51:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:51:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:51:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:51:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:51:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:51:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_max_pool2d_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:51:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_pad_13"
[07:51:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:51:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:51:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[07:51:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:52:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[07:52:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[07:52:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[07:52:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[07:52:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:52:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:52:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:52:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:52:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:52:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_pad_13"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:52:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_pad_9"
[07:52:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:52:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:53:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[07:53:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:54:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[07:54:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[07:55:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[07:55:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[07:56:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:56:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:56:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:56:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:56:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:56:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_pad_9"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:56:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_2"
[07:56:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:56:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:56:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:56:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:56:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:56:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:56:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:56:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[07:56:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:56:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:56:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:56:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[07:56:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[07:56:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_max_pool2d_2"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |            
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[07:56:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_pad_12"
[07:56:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:56:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:57:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:57:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:57:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:58:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:58:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:59:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd3dc08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd5ea58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb3e78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x559700027e38)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe28408)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb3f18)]: 0 failure(s)
[07:59:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[07:59:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[07:59:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[07:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #26 has finished. Remaining task(s): 12
[07:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_3"
[07:59:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[07:59:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[07:59:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[07:59:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[07:59:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:00:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:00:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:00:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:00:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:00:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:00:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_max_pool2d_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:00:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_pad_5"
[08:00:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:00:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:01:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:01:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:01:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:02:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:02:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:03:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:03:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:03:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_pad_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |            
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:03:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_pad_7"
[08:03:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:03:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:03:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[08:03:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:04:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[08:04:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[08:05:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[08:05:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff55d08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff55248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff47af8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ff8641d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd93628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff47b98)]: 0 failure(s)
[08:05:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:05:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:05:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:05:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 11
[08:05:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_pad_3"
[08:05:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:05:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:06:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:06:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:07:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:07:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:08:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:09:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:09:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:09:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:09:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_pad_3"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |            
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:09:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_max_pool2d_2"
[08:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:09:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:09:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[08:09:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:09:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[08:09:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[08:10:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[08:10:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffd1c108)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff17d48)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbd1c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffee4108)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff879f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbd268)]: 0 failure(s)
[08:10:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:10:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:10:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:10:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 10
[08:10:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_pad_6"
[08:10:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:10:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:10:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:10:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:10:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:10:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:10:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:10:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:10:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:10:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:10:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:10:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:10:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:10:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_pad_6"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |            
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |            
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:10:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_max_pool2d_3"
[08:10:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:10:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:10:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:10:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:10:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:11:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:11:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:11:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2d218)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc85bf8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff46d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc63a18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcfc4d8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff46da8)]: 0 failure(s)
[08:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:11:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 9
[08:11:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_pad_5"
[08:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:11:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:11:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:12:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:13:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:13:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:14:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffc74118)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff50718)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffdbdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fff85348)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff68478)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffdbe038)]: 0 failure(s)
[08:14:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:14:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:14:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 8
[08:14:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_max_pool2d_4"
[08:14:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:14:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:14:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:14:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:14:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:14:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:14:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:15:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_max_pool2d_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_pad_11"
[08:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:15:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:15:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:15:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:16:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:16:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:16:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:17:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:17:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:17:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:17:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_pad_11"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:17:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_pad_10"
[08:17:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:17:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:18:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:18:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:18:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:18:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:18:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:19:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:19:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:19:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:19:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_pad_10"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:19:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_max_pool2d_5"
[08:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:19:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:19:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:19:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:19:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:19:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:19:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:19:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:20:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:20:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:20:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:20:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:20:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:20:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_max_pool2d_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:20:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_pad_13"
[08:20:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:20:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:20:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:20:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:20:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:20:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:20:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:20:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:20:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:20:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:20:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_pad_13"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:20:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_pad_9"
[08:20:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:20:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:20:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:20:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:21:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:21:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:22:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:23:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:23:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:23:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:23:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:23:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:23:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:23:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_pad_9"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |            
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:23:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_pad_3"
[08:23:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:23:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:23:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:23:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:24:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:24:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:25:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:26:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffdf6c58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ff8a93c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffc6d8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcec3c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffcc4958)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffa4ad8)]: 0 failure(s)
[08:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:26:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 7
[08:26:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_pad_6"
[08:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:26:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:26:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:26:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:26:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:26:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:26:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:27:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:27:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:27:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:27:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:27:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:27:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:27:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_pad_6"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:27:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_max_pool2d_4"
[08:27:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:27:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:27:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:27:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:27:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:27:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:27:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:28:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:28:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:28:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:28:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_max_pool2d_4"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_pad_11"
[08:28:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:28:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:28:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:28:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:29:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:29:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:30:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:30:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:31:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:31:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:31:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:31:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:31:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:31:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_pad_11"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:31:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_pad_10"
[08:31:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:31:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:31:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:31:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:31:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:31:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:31:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:32:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:32:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:32:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:32:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:32:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:32:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:32:22] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_pad_10"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:32:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_max_pool2d_5"
[08:32:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:32:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:32:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:32:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:32:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:32:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:32:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:33:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:33:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:33:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:33:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:33:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:33:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:33:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_max_pool2d_5"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:33:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_pad_13"
[08:33:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:33:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:33:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:33:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:33:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:33:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:33:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:33:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:33:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:33:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:33:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:33:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:33:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:33:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_pad_13"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:33:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_pad_9"
[08:33:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:33:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:34:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:34:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:34:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:35:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:35:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:36:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:36:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:36:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:36:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:36:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[08:36:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[08:36:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_pad_9"
 ID |                                Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
----------------------------------------------------------------------------------------------------------------------------------------------
  0 |           fused_multiply_add_nn_pad |    1038336 |      1 |       116.4897 |       8.9135 |                8.9135 |      5 |          Y 
  1 |   fused_nn_conv2d_add_nn_leaky_relu |  155058176 |      1 |      2925.8817 |      52.9954 |               52.9954 |     32 |          Y 
  2 |                        fused_nn_pad |          1 |      1 |         0.0000 |      57.3894 |               57.3894 |      5 |          Y 
  3 |                 fused_nn_max_pool2d |    2768896 |      1 |        57.6583 |      48.0225 |               48.0225 |      5 |          Y 
  4 |                      fused_nn_pad_1 |          1 |      1 |         0.0000 |      21.1190 |               21.1190 |      5 |          Y 
  5 | fused_nn_conv2d_add_nn_leaky_relu_1 |  401489920 |      1 |      4561.2331 |      88.0222 |               88.0222 |     32 |          Y 
  6 |                      fused_nn_pad_2 |          1 |      1 |         0.0000 |      38.6062 |               38.6062 |      5 |          Y 
  7 |               fused_nn_max_pool2d_1 |    1384448 |      1 |        74.9472 |      18.4723 |               18.4723 |      5 |          Y 
  8 |                      fused_nn_pad_3 |          1 |      1 |         0.0002 |       5.4192 |                5.4192 |      5 |          Y 
  9 | fused_nn_conv2d_add_nn_leaky_relu_2 |  400105472 |      1 |      3760.8543 |     106.3869 |              106.3869 |     32 |          Y 
 10 |                      fused_nn_pad_4 |          1 |      1 |         0.0001 |      15.0645 |               15.0645 |      5 |          Y 
 11 |               fused_nn_max_pool2d_2 |     692224 |      1 |       104.9468 |       6.5960 |                6.5960 |      5 |          Y 
 12 |                      fused_nn_pad_5 |          1 |      1 |         0.0002 |       6.0540 |                6.0540 |      5 |          Y 
 13 | fused_nn_conv2d_add_nn_leaky_relu_3 |  399413248 |      1 |      3964.9364 |     100.7364 |              100.7364 |     32 |          Y 
 14 |                      fused_nn_pad_6 |          1 |      1 |         0.0003 |       3.9355 |                3.9355 |      5 |            
 15 |               fused_nn_max_pool2d_3 |     346112 |      1 |        53.9629 |       6.4139 |                6.4139 |      5 |          Y 
 16 |                      fused_nn_pad_7 |          1 |      1 |         0.0001 |       7.2088 |                7.2088 |      5 |          Y 
 17 | fused_nn_conv2d_add_nn_leaky_relu_4 |  399067136 |      1 |      3665.8281 |     108.8614 |              108.8614 |     32 |          Y 
 18 |                      fused_nn_pad_8 |          1 |      1 |         0.0001 |       9.2418 |                9.2418 |      5 |          Y 
 19 |               fused_nn_max_pool2d_4 |     173056 |      1 |        49.9587 |       3.4640 |                3.4640 |      5 |            
 20 |                      fused_nn_pad_9 |          1 |      1 |         0.0003 |       3.3108 |                3.3108 |      5 |            
 21 | fused_nn_conv2d_add_nn_leaky_relu_5 |  398894080 |      1 |      3314.6682 |     120.3421 |              120.3421 |     32 |          Y 
 22 |                     fused_nn_pad_10 |          1 |      1 |         0.0003 |       3.4300 |                3.4300 |      5 |            
 23 |               fused_nn_max_pool2d_5 |     346112 |      1 |       101.5952 |       3.4068 |                3.4068 |      5 |            
 24 |                     fused_nn_pad_11 |          1 |      1 |         0.0003 |       3.4320 |                3.4320 |      5 |            
 25 | fused_nn_conv2d_add_nn_leaky_relu_6 | 1595230208 |      1 |      2971.4623 |     536.8502 |              536.8502 |     32 |          Y 
 26 |                     fused_nn_pad_12 |          1 |      1 |         0.0001 |       8.0423 |                8.0423 |      5 |          Y 
 27 | fused_nn_conv2d_add_nn_leaky_relu_7 | 3190114304 |      1 |      3987.3082 |     800.0671 |              800.0671 |     32 |          Y 
 28 |                     fused_nn_pad_13 |          1 |      1 |         0.0003 |       3.3177 |                3.3177 |      5 |            
 29 |                 fused_nn_conv2d_add |   43285125 |      1 |      1092.0313 |      39.6373 |               39.6373 |     32 |          Y 
----------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 393
Total latency (us): 2234.76

[08:36:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_pad_6"
[08:36:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:36:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:36:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:36:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:36:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:36:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:36:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:37:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596fff2cfd8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffde32e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff0f188)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fffb6288)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff66778)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff0f228)]: 0 failure(s)
[08:37:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:37:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:37:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:37:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 6
[08:37:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_max_pool2d_4"
[08:37:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:37:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:37:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:37:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:37:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:37:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:37:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ff8a8d98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc1f888)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff973c8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffc70c28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5aef8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff97468)]: 0 failure(s)
[08:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:38:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 5
[08:38:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_pad_11"
[08:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:38:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:38:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:38:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:38:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:39:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:39:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:40:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffecb888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596fff040d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fffb2298)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe6f8b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe81e68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fffb2338)]: 0 failure(s)
[08:40:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:40:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:40:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:40:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #24 has finished. Remaining task(s): 4
[08:40:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_pad_10"
[08:40:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:40:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:40:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:40:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:41:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:41:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:41:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffe74778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffc7f728)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700010a68)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596fe6296a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffdfc048)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x559700010b08)]: 0 failure(s)
[08:41:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:41:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:41:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:41:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 3
[08:41:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_max_pool2d_5"
[08:41:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:41:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:41:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:41:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:41:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:42:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:42:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:42:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcbd1a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffd1c4e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x559700011858)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffcce548)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffd4e198)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5597000118f8)]: 0 failure(s)
[08:42:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:42:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:42:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:42:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 2
[08:42:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_pad_13"
[08:42:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:42:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:42:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:42:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:42:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:42:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:42:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:42:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x5596ffcc5298)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe2db18)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596ffff82e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe35cb8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596ffe19338)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596ffff8388)]: 0 failure(s)
[08:42:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:42:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:42:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:42:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #28 has finished. Remaining task(s): 1
[08:42:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_pad_9"
[08:42:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[08:42:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[08:43:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:43:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[08:43:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:44:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:44:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:44:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x559700026e28)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x5596ffe29248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x5596fff981b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x5596ffe29478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x5596fff5b498)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x5596fff98258)]: 0 failure(s)
[08:45:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[08:45:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[08:45:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[08:45:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[[[[-1.05577931e-01  1.29884541e-01  6.44573420e-02 ... -1.95678547e-01
    -2.07924619e-01  6.42575398e-02]
   [-4.29713577e-01  1.39095411e-01  1.89111248e-01 ...  1.17356502e-01
     2.11699784e-01  5.19537032e-01]
   [-7.20241725e-01  2.07379922e-01  1.73242524e-01 ...  1.23485327e-01
     2.87670702e-01  5.08654177e-01]
   ...
   [-7.94247150e-01  5.57271428e-02  1.84007108e-01 ...  1.84569180e-01
     3.29756767e-01  5.46127200e-01]
   [-5.52445829e-01  5.64718544e-02  1.13385580e-01 ...  5.36722355e-02
     2.05632210e-01  2.48754129e-01]
   [-4.54829603e-01  1.03293017e-01  8.19068998e-02 ...  3.85733359e-02
     2.14384831e-02  1.91248342e-01]]

  [[-3.17135930e-01 -5.39510429e-01 -5.71407616e-01 ... -6.87172949e-01
    -6.47279799e-01 -4.07979161e-01]
   [-1.84784472e-01 -6.57396853e-01 -7.14283943e-01 ... -6.27969861e-01
    -2.40286082e-01 -5.81234507e-02]
   [ 7.75867999e-02 -2.54049957e-01 -1.24446057e-01 ...  7.81264901e-02
     4.11044955e-01  2.82154679e-01]
   ...
   [ 2.93110251e-01  1.40940100e-01  3.14229243e-02 ...  1.68308765e-02
     2.28949383e-01  1.59012452e-01]
   [ 1.43362641e-01  4.38630939e-01  3.88097018e-01 ...  4.64308381e-01
     3.21926236e-01  9.73899961e-02]
   [-1.11295134e-02  4.14858401e-01  4.07695889e-01 ...  4.49591726e-01
     1.81699708e-01  5.51133528e-02]]

  [[-4.45173949e-01  2.26342082e-02  2.03175023e-01 ...  2.88486034e-01
     6.11546114e-02 -3.68879557e-01]
   [-9.19871747e-01 -1.90776497e-01  1.76535733e-02 ...  2.06465018e-03
    -1.54091462e-01 -7.76759267e-01]
   [-1.12780309e+00 -2.74912566e-01 -9.61504504e-02 ... -1.72110528e-01
    -3.42053205e-01 -9.96647596e-01]
   ...
   [-1.24818027e+00 -5.83230019e-01 -4.61238325e-01 ... -5.42889178e-01
    -6.68705642e-01 -1.23918784e+00]
   [-1.07999396e+00 -4.29293662e-01 -2.89329410e-01 ... -2.87182868e-01
    -5.05156755e-01 -9.89904583e-01]
   [-6.55381501e-01 -1.17484644e-01  1.16059303e-01 ...  1.51954502e-01
    -4.83748242e-02 -4.46880400e-01]]

  ...

  [[-2.17334121e-01  2.94933349e-01  5.13248801e-01 ...  5.69386244e-01
     1.51194274e-01  5.71574755e-02]
   [ 7.05206871e-01  1.88432276e+00  2.61356926e+00 ...  2.79046488e+00
     1.97728848e+00  1.22179210e+00]
   [ 7.59618640e-01  2.16802764e+00  3.05618811e+00 ...  3.21559858e+00
     2.16363740e+00  1.22338414e+00]
   ...
   [ 8.98616135e-01  1.71266544e+00  1.81405938e+00 ...  3.28168917e+00
     2.54653001e+00  1.72299457e+00]
   [ 4.06522632e-01  4.98527080e-01  4.65264887e-01 ...  1.64589643e+00
     1.45312726e+00  1.01896250e+00]
   [ 3.49198550e-01  6.78654850e-01  1.02392364e+00 ...  1.60147583e+00
     1.29156387e+00  6.77800477e-01]]

  [[-2.95962244e-01 -3.49855959e-01 -3.15456204e-02 ... -3.08031261e-01
    -4.56837773e-01 -3.45297992e-01]
   [ 5.59258282e-01  2.35520210e-02  5.49147844e-01 ... -2.72451103e-01
    -3.71081680e-01 -4.54073101e-01]
   [ 1.10181689e+00  4.66978639e-01  1.09713447e+00 ... -1.29783973e-01
    -1.08498834e-01 -2.22062081e-01]
   ...
   [ 6.42959476e-02 -2.66642570e-01  1.29509091e-01 ... -8.08980525e-01
    -5.37401438e-01 -3.50728124e-01]
   [-3.16665381e-01 -1.85098574e-01  4.17734861e-01 ... -2.81182081e-01
    -4.69165802e-01 -3.96683514e-01]
   [-7.57486746e-02 -2.96979658e-02  3.30827206e-01 ... -3.70753765e-01
    -7.19704270e-01 -5.62645555e-01]]

  [[ 9.16073695e-02  4.80789214e-01  6.22210205e-01 ...  5.32023072e-01
     2.46475250e-01 -1.43133298e-01]
   [ 2.80908674e-01  7.71233737e-01  9.73824918e-01 ...  1.36330676e+00
     8.51145566e-01  7.43894398e-01]
   [ 5.11018991e-01  7.87330747e-01  1.00856745e+00 ...  2.16713858e+00
     1.40616870e+00  8.64750922e-01]
   ...
   [ 6.09251618e-01  1.14987862e+00  1.83263969e+00 ...  3.01064801e+00
     2.23195887e+00  1.66677475e+00]
   [ 1.47087380e-01  3.49709988e-01  6.36551797e-01 ...  1.41345954e+00
     9.39422548e-01  9.06845629e-01]
   [ 2.49107853e-01  4.62940961e-01  6.77936852e-01 ...  9.49220657e-01
     6.00924134e-01  5.88678122e-01]]]]
[[[[-1.05577998e-01  1.29885599e-01  6.44576475e-02 ... -1.95678771e-01
    -2.07924426e-01  6.42585531e-02]
   [-4.29713607e-01  1.39095068e-01  1.89111084e-01 ...  1.17356598e-01
     2.11699679e-01  5.19536614e-01]
   [-7.20241725e-01  2.07379609e-01  1.73242673e-01 ...  1.23485036e-01
     2.87671089e-01  5.08654475e-01]
   ...
   [-7.94247448e-01  5.57268970e-02  1.84007287e-01 ...  1.84569031e-01
     3.29756767e-01  5.46127319e-01]
   [-5.52445889e-01  5.64719960e-02  1.13384999e-01 ...  5.36735207e-02
     2.05631867e-01  2.48754218e-01]
   [-4.54829216e-01  1.03293099e-01  8.19063038e-02 ...  3.85738201e-02
     2.14378797e-02  1.91248015e-01]]

  [[-3.17135185e-01 -5.39510369e-01 -5.71408093e-01 ... -6.87172472e-01
    -6.47279263e-01 -4.07979280e-01]
   [-1.84784219e-01 -6.57397866e-01 -7.14283764e-01 ... -6.27969861e-01
    -2.40285963e-01 -5.81233539e-02]
   [ 7.75865167e-02 -2.54050285e-01 -1.24445982e-01 ...  7.81260207e-02
     4.11045551e-01  2.82154948e-01]
   ...
   [ 2.93110639e-01  1.40940353e-01  3.14229093e-02 ...  1.68305952e-02
     2.28949770e-01  1.59012467e-01]
   [ 1.43361956e-01  4.38631922e-01  3.88097137e-01 ...  4.64308798e-01
     3.21926147e-01  9.73899737e-02]
   [-1.11292852e-02  4.14857984e-01  4.07695860e-01 ...  4.49590743e-01
     1.81699321e-01  5.51129952e-02]]

  [[-4.45174217e-01  2.26346347e-02  2.03175023e-01 ...  2.88486272e-01
     6.11544400e-02 -3.68879735e-01]
   [-9.19872463e-01 -1.90776691e-01  1.76536199e-02 ...  2.06453563e-03
    -1.54092059e-01 -7.76759863e-01]
   [-1.12780261e+00 -2.74912864e-01 -9.61508602e-02 ... -1.72110617e-01
    -3.42053592e-01 -9.96647298e-01]
   ...
   [-1.24818003e+00 -5.83229423e-01 -4.61238235e-01 ... -5.42887986e-01
    -6.68706238e-01 -1.23918891e+00]
   [-1.07999337e+00 -4.29293215e-01 -2.89329410e-01 ... -2.87182510e-01
    -5.05156338e-01 -9.89904225e-01]
   [-6.55380726e-01 -1.17484443e-01  1.16059393e-01 ...  1.51954874e-01
    -4.83743101e-02 -4.46880668e-01]]

  ...

  [[-2.17334881e-01  2.94933885e-01  5.13248265e-01 ...  5.69386482e-01
     1.51194394e-01  5.71561828e-02]
   [ 7.05205441e-01  1.88432145e+00  2.61356878e+00 ...  2.79046273e+00
     1.97729003e+00  1.22179258e+00]
   [ 7.59617388e-01  2.16802549e+00  3.05619049e+00 ...  3.21559715e+00
     2.16363811e+00  1.22338367e+00]
   ...
   [ 8.98617744e-01  1.71266568e+00  1.81405914e+00 ...  3.28168774e+00
     2.54653120e+00  1.72299469e+00]
   [ 4.06522274e-01  4.98528838e-01  4.65264767e-01 ...  1.64589632e+00
     1.45312679e+00  1.01896262e+00]
   [ 3.49199533e-01  6.78655863e-01  1.02392387e+00 ...  1.60147703e+00
     1.29156673e+00  6.77802444e-01]]

  [[-2.95960903e-01 -3.49856228e-01 -3.15469727e-02 ... -3.08032483e-01
    -4.56839055e-01 -3.45298409e-01]
   [ 5.59259415e-01  2.35536974e-02  5.49145877e-01 ... -2.72452116e-01
    -3.71082842e-01 -4.54072803e-01]
   [ 1.10181737e+00  4.66978371e-01  1.09713364e+00 ... -1.29785746e-01
    -1.08499423e-01 -2.22062454e-01]
   ...
   [ 6.42978847e-02 -2.66641557e-01  1.29508331e-01 ... -8.08983326e-01
    -5.37402928e-01 -3.50729287e-01]
   [-3.16663951e-01 -1.85097456e-01  4.17734742e-01 ... -2.81183034e-01
    -4.69166428e-01 -3.96683842e-01]
   [-7.57490918e-02 -2.96976529e-02  3.30826133e-01 ... -3.70753646e-01
    -7.19703138e-01 -5.62645316e-01]]

  [[ 9.16074142e-02  4.80789572e-01  6.22212648e-01 ...  5.32022119e-01
     2.46475846e-01 -1.43132880e-01]
   [ 2.80908316e-01  7.71234691e-01  9.73826587e-01 ...  1.36330521e+00
     8.51147056e-01  7.43893564e-01]
   [ 5.11017561e-01  7.87328482e-01  1.00856900e+00 ...  2.16713786e+00
     1.40616643e+00  8.64750326e-01]
   ...
   [ 6.09250307e-01  1.14987755e+00  1.83263862e+00 ...  3.01064563e+00
     2.23196030e+00  1.66677582e+00]
   [ 1.47086084e-01  3.49706739e-01  6.36552334e-01 ...  1.41346085e+00
     9.39423025e-01  9.06845272e-01]
   [ 2.49105707e-01  4.62941110e-01  6.77935421e-01 ...  9.49219942e-01
     6.00923777e-01  5.88678479e-01]]]]
