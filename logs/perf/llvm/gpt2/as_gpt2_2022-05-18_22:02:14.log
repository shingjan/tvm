----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 1
Sample Initial Population	#s: 288	fail_ct: 1760	Time elapsed: 1.23
GA Iter: 0	Max score: 0.9948	Min score: 0.5656	#Pop: 128	#M+: 0	#M-: 0
[22:06:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:10] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:10] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:12] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:12] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:06:12] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:06:12] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 1.0000	Min score: 0.9703	#Pop: 128	#M+: 509	#M-: 5796
EvolutionarySearch		#s: 128	Time elapsed: 4.33
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.42 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 4.40 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 1975	fail_ct: 5	Time elapsed: 1.72
GA Iter: 0	Max score: 0.9999	Min score: 0.9219	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9868	#Pop: 128	#M+: 1397	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 3.77
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
...............................................................Get devices for measurement successfully!
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/text_machine_comprehension_gpt2.onnx
file existed. Skipping downloading.
/home/ubuntu/cache-workloads/gpt2.onnx
Workload: gpt2
  input_name: input1
  input_shape: [1, 1, 8]
  input_dtype: int64
==== Task 0: vm_mod_fused_nn_softmax (weight 12 key: ["d7b65649a4dd54becea0a52aabbc5af5", [96, 8], [96, 8]]) =====
placeholder = PLACEHOLDER [96, 8]
T_softmax_maxelem(i0) max= placeholder[i0, k]
T_softmax_exp(i0, i1) = tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))
T_softmax_expsum(i0) += T_softmax_exp[i0, k]
T_softmax_norm(i0, i1) = (T_softmax_exp[i0, i1]/T_softmax_expsum[i0])

==== Task 1: vm_mod_fused_nn_dense_add_2 (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 768], [3072, 768], [1, 3072], [8, 3072]]) =====
placeholder = PLACEHOLDER [8, 768]
placeholder = PLACEHOLDER [3072, 768]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 3072]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])

==== Task 2: vm_mod_fused_power_mean (weight 25 key: ["db3f35d71c2b463d338eb14acd963ed2", [1, 8, 768], [1, 8, 1]]) =====
placeholder = PLACEHOLDER [1, 8, 768]
compile_engine_const() = 2f
T_power(ax0, ax1, ax2) = tir.pow(placeholder[ax0, ax1, ax2], compile_engine_const[])
T_power_red(ax0, ax1, ax2) += T_power[ax0, ax1, k2]
T_divide(ax0, ax1, ax2) = (T_power_red[ax0, ax1, ax2]/768f)

==== Task 3: vm_mod_fused_mean (weight 25 key: ["2dde9ffcbf97381c0f0307643e09dac5", [1, 8, 768], [1, 8, 1]]) =====
placeholder = PLACEHOLDER [1, 8, 768]
placeholder_red(ax0, ax1, ax2) += placeholder[ax0, ax1, k2]
T_divide(ax0, ax1, ax2) = (placeholder_red[ax0, ax1, ax2]/768f)

==== Task 4: vm_mod_fused_nn_dense_add_1 (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 768], [768, 768], [1, 768], [8, 768]]) =====
placeholder = PLACEHOLDER [8, 768]
placeholder = PLACEHOLDER [768, 768]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 768]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])

==== Task 5: vm_mod_fused_nn_batch_matmul_1 (weight 12 key: ["32a5ff201401d5600f77ced9f22defaa", [12, 8, 8], [12, 64, 8], [12, 8, 64]]) =====
placeholder = PLACEHOLDER [12, 8, 8]
placeholder = PLACEHOLDER [12, 64, 8]
T_batch_matmul_NT(b, i, j) += (placeholder[b, i, k]*placeholder[b, j, k])

==== Task 6: vm_mod_fused_nn_dense_add (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 768], [2304, 768], [1, 2304], [8, 2304]]) =====
placeholder = PLACEHOLDER [8, 768]
placeholder = PLACEHOLDER [2304, 768]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 2304]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])

==== Task 7: vm_mod_fused_nn_batch_matmul (weight 12 key: ["32a5ff201401d5600f77ced9f22defaa", [12, 8, 64], [12, 8, 64], [12, 8, 8]]) =====
placeholder = PLACEHOLDER [12, 8, 64]
placeholder = PLACEHOLDER [12, 8, 64]
T_batch_matmul_NT(b, i, j) += (placeholder[b, i, k]*placeholder[b, j, k])

==== Task 8: vm_mod_fused_nn_dense_add_3 (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 3072], [768, 3072], [1, 768], [8, 768]]) =====
placeholder = PLACEHOLDER [8, 3072]
placeholder = PLACEHOLDER [768, 3072]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 768]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |            - |              - |      0 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |            - |              - |      0 |
|    2 |                                       vm_mod_fused_power_mean |            - |              - |      0 |
|    3 |                                             vm_mod_fused_mean |            - |              - |      0 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |            - |              - |      0 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |            - |              - |      0 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 0	Used time : 1 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |            - |              - |      0 |
|    2 |                                       vm_mod_fused_power_mean |            - |              - |      0 |
|    3 |                                             vm_mod_fused_mean |            - |              - |      0 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |            - |              - |      0 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |            - |              - |      0 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 64	Used time : 24 s	Next ID: 1	
.T***************************************************************Time elapsed for measurement: 31.05 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 7.90 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 866	fail_ct: 865	Time elapsed: 0.76
GA Iter: 0	Max score: 0.9997	Min score: 0.8570	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9865	#Pop: 128	#M+: 1380	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.10
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.43 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 4.43 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 845	fail_ct: 864	Time elapsed: 0.41
GA Iter: 0	Max score: 0.9994	Min score: 0.8360	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9864	#Pop: 128	#M+: 1380	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 2.25
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.98 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 12.97 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 1984	fail_ct: 10	Time elapsed: 0.80
GA Iter: 0	Max score: 0.9998	Min score: 0.9404	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9998	Min score: 0.9856	#Pop: 128	#M+: 1381	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.05
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.19 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 3.11 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 1976	fail_ct: 10	Time elapsed: 0.45
GA Iter: 0	Max score: 0.9985	Min score: 0.9273	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9866	#Pop: 128	#M+: 1372	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 1.81
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.37 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 2.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 1989	fail_ct: 10	Time elapsed: 0.65
GA Iter: 0	Max score: 0.9998	Min score: 0.9412	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9878	#Pop: 128	#M+: 1379	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 2.36
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.48 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 13.69 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |            - |              - |      0 |
|    3 |                                             vm_mod_fused_mean |            - |              - |      0 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |            - |              - |      0 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |            - |              - |      0 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 128	Used time : 69 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |            - |              - |      0 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |            - |              - |      0 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |            - |              - |      0 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 192	Used time : 90 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |            - |              - |      0 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |            - |              - |      0 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 256	Used time : 120 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |            - |              - |      0 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 320	Used time : 145 s	Next ID: 5	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |            - |              - |      0 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 384	Used time : 165 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 1969	fail_ct: 15	Time elapsed: 0.41
GA Iter: 0	Max score: 0.9997	Min score: 0.9305	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0000	Min score: 0.9880	#Pop: 128	#M+: 1370	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 2.18
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.43 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 10.00 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 3
Sample Initial Population	#s: 1976	fail_ct: 14	Time elapsed: 0.46
GA Iter: 0	Max score: 0.9998	Min score: 0.9306	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9873	#Pop: 128	#M+: 1377	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 1.57
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................***********************************************Time elapsed for measurement: 13.04 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 13.37 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 9	Time elapsed: 0.71
GA Iter: 0	Max score: 0.9996	Min score: 0.9364	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9891	#Pop: 128	#M+: 1381	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 3.18
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 21.54 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 16.36 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1986	fail_ct: 9	Time elapsed: 1.33
GA Iter: 0	Max score: 0.8679	Min score: 0.5781	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9763	Min score: 0.7728	#Pop: 128	#M+: 1370	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.58
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 22.28 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1968	fail_ct: 9	Time elapsed: 0.87
GA Iter: 0	Max score: 0.8352	Min score: 0.4523	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9869	Min score: 0.7232	#Pop: 128	#M+: 1374	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 3.85
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.48 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 9.92 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 8	Time elapsed: 1.26
GA Iter: 0	Max score: 0.7797	Min score: 0.4798	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8824	Min score: 0.6866	#Pop: 128	#M+: 1380	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.91
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 21.31 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 18.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.030 |         932.57 |     64 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |            - |              - |      0 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 448	Used time : 200 s	Next ID: 7	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.030 |         932.57 |     64 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 512	Used time : 233 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |     64 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.030 |         932.57 |     64 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.027 |        1389.50 |     64 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.486 ms	Trials: 576	Used time : 262 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |    128 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.030 |         932.57 |     64 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.027 |        1389.50 |     64 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.486 ms	Trials: 640	Used time : 304 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |    128 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.027 |        1389.50 |     64 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.317 ms	Trials: 704	Used time : 351 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.037 |        1034.26 |    128 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.027 |        1394.60 |    128 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.316 ms	Trials: 768	Used time : 389 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1982	fail_ct: 10	Time elapsed: 1.00
GA Iter: 0	Max score: 0.7166	Min score: 0.4154	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0079	Min score: 0.7673	#Pop: 128	#M+: 1387	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.22 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 22.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1960	fail_ct: 15	Time elapsed: 0.91
GA Iter: 0	Max score: 0.9564	Min score: 0.4993	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9925	Min score: 0.7619	#Pop: 128	#M+: 1372	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.16
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.48 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 16.22 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1971	fail_ct: 13	Time elapsed: 1.22
GA Iter: 0	Max score: 0.9226	Min score: 0.4139	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9778	Min score: 0.8658	#Pop: 128	#M+: 1394	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.65
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 17.37 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.55 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 7	Time elapsed: 1.44
GA Iter: 0	Max score: 0.8736	Min score: 0.4201	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0167	Min score: 0.6733	#Pop: 128	#M+: 1385	#M-: 81
EvolutionarySearch		#s: 128	Time elapsed: 4.81
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 21.15 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 14.86 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 3	Time elapsed: 1.39
GA Iter: 0	Max score: 0.7566	Min score: 0.3946	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0345	Min score: 0.6915	#Pop: 128	#M+: 1390	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.49
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.27 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 23.99 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.025 |        1496.25 |    192 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.027 |        1394.60 |    128 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.181 ms	Trials: 832	Used time : 435 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.025 |        1496.25 |    192 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.011 |         872.43 |     64 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    192 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.161 ms	Trials: 896	Used time : 479 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.025 |        1496.25 |    192 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    192 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.120 ms	Trials: 960	Used time : 520 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.025 |        1496.25 |    192 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    256 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.120 ms	Trials: 1024	Used time : 569 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1550.62 |    256 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    128 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    256 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.109 ms	Trials: 1088	Used time : 612 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1550.62 |    256 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    192 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    256 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 6	Time elapsed: 0.86
GA Iter: 0	Max score: 0.9069	Min score: 0.3858	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9324	Min score: 0.6820	#Pop: 128	#M+: 1376	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 4.91
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 25.87 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 28.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1972	fail_ct: 14	Time elapsed: 1.30
GA Iter: 0	Max score: 0.7154	Min score: 0.3810	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9695	Min score: 0.8206	#Pop: 128	#M+: 1376	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.63
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.59 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 18.03 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 9	Time elapsed: 1.19
GA Iter: 0	Max score: 0.7874	Min score: 0.3638	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9830	Min score: 0.7349	#Pop: 128	#M+: 1382	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 4.84
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.45 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.76 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 896	fail_ct: 854	Time elapsed: 0.76
GA Iter: 0	Max score: 0.9948	Min score: 0.9448	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0041	Min score: 0.9873	#Pop: 128	#M+: 1374	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.17
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.35 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 13.18 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1999	fail_ct: 5	Time elapsed: 1.50
GA Iter: 0	Max score: 0.7147	Min score: 0.3900	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9894	Min score: 0.6972	#Pop: 128	#M+: 1393	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.81 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 17.12 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 859	fail_ct: 870	Time elapsed: 0.88
GA Iter: 0	Max score: 0.9858	Min score: 0.9197	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0079	Min score: 0.9655	#Pop: 128	#M+: 1387	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 5.18
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.69 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 26.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.109 ms	Trials: 1152	Used time : 660 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    320 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    192 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    256 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.099 ms	Trials: 1216	Used time : 721 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    320 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    192 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.099 ms	Trials: 1280	Used time : 759 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |     64 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    192 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.099 ms	Trials: 1344	Used time : 806 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.016 |        1739.07 |    192 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.099 ms	Trials: 1408	Used time : 835 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.36 |     64 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    256 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.058 ms	Trials: 1472	Used time : 873 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1962	fail_ct: 9	Time elapsed: 1.44
GA Iter: 0	Max score: 0.7085	Min score: 0.3448	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9691	Min score: 0.8207	#Pop: 128	#M+: 1384	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.61
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.14 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 10.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1987	fail_ct: 10	Time elapsed: 1.02
GA Iter: 0	Max score: 0.6192	Min score: 0.3214	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8583	Min score: 0.6087	#Pop: 128	#M+: 1379	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.09
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.56 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 21.78 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1964	fail_ct: 13	Time elapsed: 1.52
GA Iter: 0	Max score: 0.7880	Min score: 0.3396	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9715	Min score: 0.8461	#Pop: 128	#M+: 1378	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 5.00
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.32 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 17.18 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 5	Time elapsed: 0.50
GA Iter: 0	Max score: 0.7518	Min score: 0.3943	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9868	Min score: 0.7364	#Pop: 128	#M+: 1385	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 2.38
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.53 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 15.95 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1972	fail_ct: 17	Time elapsed: 0.56
GA Iter: 0	Max score: 0.7196	Min score: 0.4088	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7537	Min score: 0.6646	#Pop: 128	#M+: 1373	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 3.69
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.44 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 14.45 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 10	Time elapsed: 0.76
GA Iter: 0	Max score: 0.7550	Min score: 0.3348	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9691	Min score: 0.8563	#Pop: 128	#M+: 1380	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 3.61
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.03 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 19.99 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    256 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1485.62 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.057 ms	Trials: 1536	Used time : 920 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    256 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    384 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.055 ms	Trials: 1600	Used time : 952 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    384 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.055 ms	Trials: 1664	Used time : 990 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    384 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    448 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.055 ms	Trials: 1728	Used time : 1027 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    448 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1272.30 |    128 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    448 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.055 ms	Trials: 1792	Used time : 1066 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    448 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    448 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.051 ms	Trials: 1856	Used time : 1094 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 8	Time elapsed: 1.08
GA Iter: 0	Max score: 0.7890	Min score: 0.3888	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9625	Min score: 0.7375	#Pop: 128	#M+: 1378	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 5.20
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 24.79 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1965	fail_ct: 14	Time elapsed: 1.07
GA Iter: 0	Max score: 0.6889	Min score: 0.3500	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9686	Min score: 0.8856	#Pop: 128	#M+: 1369	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.52
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.59 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 11.83 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1996	fail_ct: 9	Time elapsed: 0.49
GA Iter: 0	Max score: 0.6898	Min score: 0.3269	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9372	Min score: 0.5996	#Pop: 128	#M+: 1380	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 2.09
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 12.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 17.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1956	fail_ct: 16	Time elapsed: 0.95
GA Iter: 0	Max score: 0.7203	Min score: 0.3502	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9673	Min score: 0.8781	#Pop: 128	#M+: 1382	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.61
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.66 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 22.46 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 848	fail_ct: 882	Time elapsed: 0.87
GA Iter: 0	Max score: 0.9932	Min score: 0.9168	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0008	Min score: 0.9647	#Pop: 128	#M+: 1388	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.48
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 10.42 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 14.82 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.024 |        1606.26 |    448 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    512 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 1.051 ms	Trials: 1920	Used time : 1133 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    512 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.973 ms	Trials: 1984	Used time : 1179 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2203.53 |    320 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    576 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.973 ms	Trials: 2048	Used time : 1208 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1492.57 |    576 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.973 ms	Trials: 2112	Used time : 1241 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.35 |    128 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.971 ms	Trials: 2176	Used time : 1294 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.21 |     64 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 4	Time elapsed: 1.15
GA Iter: 0	Max score: 1.0026	Min score: 0.8528	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0464	Min score: 0.9282	#Pop: 128	#M+: 1394	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.70
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
...............................................................|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.970 ms	Trials: 2240	Used time : 1324 s	Next ID: 5	
.T***************************************************************Time elapsed for measurement: 25.41 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 7.30 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1937	fail_ct: 13	Time elapsed: 0.63
GA Iter: 0	Max score: 1.0031	Min score: 0.8212	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0076	Min score: 0.9067	#Pop: 128	#M+: 1384	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 2.23
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
..............................................................
|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.28 |     64 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.969 ms	Trials: 2304	Used time : 1363 s	Next ID: 7	
.T.T***********************************************Time elapsed for measurement: 25.42 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 14.17 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1986	fail_ct: 5	Time elapsed: 0.94
GA Iter: 0	Max score: 0.7171	Min score: 0.2903	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8720	Min score: 0.5673	#Pop: 128	#M+: 1396	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 3.74
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.55 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 26.07 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 853	fail_ct: 860	Time elapsed: 0.78
GA Iter: 0	Max score: 0.9814	Min score: 0.8853	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9814	Min score: 0.9329	#Pop: 128	#M+: 1371	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 4.83
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.92 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 23.47 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 284	fail_ct: 1764	Time elapsed: 2.10
GA Iter: 0	Max score: 1.0155	Min score: 0.4732	#Pop: 128	#M+: 0	#M-: 0
[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
T_softmax_maxelem auto_unroll: 16
parallel i0 (None)
  for k (None)
    T_softmax_maxelem = ...
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:03] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:03] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:04] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:04] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
T_softmax_maxelem auto_unroll: 16
parallel i0 (None)
  for k (None)
    T_softmax_maxelem = ...
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:31:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:31:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 1.0238	Min score: 0.9551	#Pop: 128	#M+: 541	#M-: 5670
EvolutionarySearch		#s: 128	Time elapsed: 5.75
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.80 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 16.95 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1963	fail_ct: 13	Time elapsed: 1.18
GA Iter: 0	Max score: 0.7271	Min score: 0.3679	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9603	Min score: 0.8756	#Pop: 128	#M+: 1381	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.98 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.00 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1994	fail_ct: 8	Time elapsed: 1.09
GA Iter: 0	Max score: 0.5722	Min score: 0.2846	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9249	Min score: 0.5913	#Pop: 128	#M+: 1380	#M-: 67
EvolutionarySearch		#s: 128	Time elapsed: 3.55
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.88 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 2.12 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    512 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.969 ms	Trials: 2368	Used time : 1406 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    576 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    128 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.969 ms	Trials: 2432	Used time : 1451 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |     64 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    576 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.969 ms	Trials: 2496	Used time : 1493 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    576 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1501.03 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.969 ms	Trials: 2560	Used time : 1533 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    576 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1524.20 |    704 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.964 ms	Trials: 2624	Used time : 1580 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    640 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2004	fail_ct: 2	Time elapsed: 0.68
GA Iter: 0	Max score: 0.5942	Min score: 0.3732	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8624	Min score: 0.6592	#Pop: 128	#M+: 1389	#M-: 68
EvolutionarySearch		#s: 128	Time elapsed: 4.53
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.86 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 30.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 13	Time elapsed: 0.81
GA Iter: 0	Max score: 0.7604	Min score: 0.4101	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9422	Min score: 0.6841	#Pop: 128	#M+: 1382	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 4.46
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.01 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.41 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1961	fail_ct: 11	Time elapsed: 1.67
GA Iter: 0	Max score: 0.8176	Min score: 0.3725	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9412	Min score: 0.8510	#Pop: 128	#M+: 1378	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.61
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.74 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 28.25 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1978	fail_ct: 8	Time elapsed: 0.95
GA Iter: 0	Max score: 0.6160	Min score: 0.3025	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8911	Min score: 0.6057	#Pop: 128	#M+: 1392	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 4.05
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 11.94 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 24.61 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1965	fail_ct: 15	Time elapsed: 1.36
GA Iter: 0	Max score: 0.7717	Min score: 0.3657	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9192	Min score: 0.8388	#Pop: 128	#M+: 1377	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................***********************************************Time elapsed for measurement: 12.52 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 10.75 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 8	Time elapsed: 0.44
GA Iter: 0	Max score: 0.8221	Min score: 0.4735	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9484	Min score: 0.7859	#Pop: 128	#M+: 1386	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 2.27
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 26.85 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2209.83 |    384 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1524.20 |    704 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.964 ms	Trials: 2688	Used time : 1600 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    640 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1343.69 |    192 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1524.20 |    704 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.961 ms	Trials: 2752	Used time : 1649 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    640 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1397.09 |    256 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1524.20 |    704 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.958 ms	Trials: 2816	Used time : 1708 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2214.92 |    640 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1397.09 |    256 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1531.08 |    768 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.956 ms	Trials: 2880	Used time : 1757 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1397.09 |    256 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.025 |        1531.08 |    768 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.954 ms	Trials: 2944	Used time : 1799 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.007 |        1397.09 |    256 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.020 |        1924.87 |    832 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.893 ms	Trials: 3008	Used time : 1829 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1972	fail_ct: 10	Time elapsed: 0.78
GA Iter: 0	Max score: 0.6546	Min score: 0.2997	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0223	Min score: 0.6822	#Pop: 128	#M+: 1373	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.87
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.92 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 29.60 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1972	fail_ct: 15	Time elapsed: 1.51
GA Iter: 0	Max score: 0.6685	Min score: 0.2745	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9761	Min score: 0.5961	#Pop: 128	#M+: 1384	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 5.42
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.64 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 14.07 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2002	fail_ct: 5	Time elapsed: 0.51
GA Iter: 0	Max score: 0.6683	Min score: 0.3356	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8509	Min score: 0.6603	#Pop: 128	#M+: 1388	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 2.56
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.71 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 31.24 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 867	fail_ct: 879	Time elapsed: 0.51
GA Iter: 0	Max score: 1.0450	Min score: 0.9178	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0450	Min score: 0.9426	#Pop: 128	#M+: 1376	#M-: 86
EvolutionarySearch		#s: 128	Time elapsed: 2.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.27 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 19.24 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 894	fail_ct: 842	Time elapsed: 0.30
GA Iter: 0	Max score: 0.9721	Min score: 0.9076	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0936	Min score: 0.9442	#Pop: 128	#M+: 1373	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 2.45
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 16.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.020 |        1924.87 |    832 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.890 ms	Trials: 3072	Used time : 1872 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |    896 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.861 ms	Trials: 3136	Used time : 1919 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    448 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |    960 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.861 ms	Trials: 3200	Used time : 1954 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    192 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    512 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |    960 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.861 ms	Trials: 3264	Used time : 2011 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    192 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    512 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |    960 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.861 ms	Trials: 3328	Used time : 2045 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    512 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |    960 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1957	fail_ct: 12	Time elapsed: 1.08
GA Iter: 0	Max score: 0.6320	Min score: 0.3037	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0079	Min score: 0.6128	#Pop: 128	#M+: 1380	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.67
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.93 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 47.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1989	fail_ct: 4	Time elapsed: 1.38
GA Iter: 0	Max score: 0.6875	Min score: 0.3491	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9398	Min score: 0.7102	#Pop: 128	#M+: 1373	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 4.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.80 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 24.51 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1984	fail_ct: 10	Time elapsed: 1.22
GA Iter: 0	Max score: 0.6373	Min score: 0.2965	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9627	Min score: 0.6081	#Pop: 128	#M+: 1380	#M-: 81
EvolutionarySearch		#s: 128	Time elapsed: 6.05
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.12 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 28.20 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1978	fail_ct: 10	Time elapsed: 0.71
GA Iter: 0	Max score: 0.5840	Min score: 0.2942	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9740	Min score: 0.6427	#Pop: 128	#M+: 1380	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 3.72
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.56 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 17.23 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1996	fail_ct: 7	Time elapsed: 0.76
GA Iter: 0	Max score: 0.6781	Min score: 0.3561	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9686	Min score: 0.7218	#Pop: 128	#M+: 1377	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 3.05
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.52 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 39.98 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1976	fail_ct: 13	Time elapsed: 0.83
GA Iter: 0	Max score: 0.8346	Min score: 0.4299	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9641	Min score: 0.7877	#Pop: 128	#M+: 1384	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.07
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.28 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 32.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.861 ms	Trials: 3392	Used time : 2075 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.013 |        2254.83 |    512 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.861 ms	Trials: 3456	Used time : 2145 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    704 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    576 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.859 ms	Trials: 3520	Used time : 2193 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2240.28 |    768 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    576 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.859 ms	Trials: 3584	Used time : 2248 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    576 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.859 ms	Trials: 3648	Used time : 2281 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1459.44 |    320 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.859 ms	Trials: 3712	Used time : 2339 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.39 |    256 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 860	fail_ct: 875	Time elapsed: 0.61
GA Iter: 0	Max score: 0.9809	Min score: 0.9176	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9843	Min score: 0.9430	#Pop: 128	#M+: 1384	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.10
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.28 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 34.84 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 869	fail_ct: 844	Time elapsed: 0.93
GA Iter: 0	Max score: 0.9682	Min score: 0.9008	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9685	Min score: 0.9431	#Pop: 128	#M+: 1376	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.36
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.12 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 31.07 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1987	fail_ct: 13	Time elapsed: 1.06
GA Iter: 0	Max score: 0.9825	Min score: 0.8423	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0000	Min score: 0.9221	#Pop: 128	#M+: 1379	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 3.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
..............................................................|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.853 ms	Trials: 3776	Used time : 2392 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    256 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.852 ms	Trials: 3840	Used time : 2445 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          36.81 |    128 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.852 ms	Trials: 3904	Used time : 2495 s	Next ID: 5	
.T.T**************************************************************Time elapsed for measurement: 23.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 20.82 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1959	fail_ct: 14	Time elapsed: 0.96
GA Iter: 0	Max score: 0.9510	Min score: 0.8282	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0134	Min score: 0.9313	#Pop: 128	#M+: 1381	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.30
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.14 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 21.40 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 289	fail_ct: 1759	Time elapsed: 1.26
GA Iter: 0	Max score: 0.9738	Min score: 0.4613	#Pop: 128	#M+: 0	#M-: 0
[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:24] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:24] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:25] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:25] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:26] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:26] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[22:49:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [22:49:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9830	Min score: 0.9310	#Pop: 128	#M+: 543	#M-: 5598
EvolutionarySearch		#s: 128	Time elapsed: 4.58
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.97 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 32.95 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1982	fail_ct: 7	Time elapsed: 1.45
GA Iter: 0	Max score: 0.6663	Min score: 0.2837	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9982	Min score: 0.6828	#Pop: 128	#M+: 1384	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 5.28
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 14.85 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 22.60 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 13	Time elapsed: 0.72
GA Iter: 0	Max score: 0.6211	Min score: 0.2969	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9647	Min score: 0.6510	#Pop: 128	#M+: 1387	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.78 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 20.09 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 6	Time elapsed: 1.74
GA Iter: 0	Max score: 0.7367	Min score: 0.3670	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9667	Min score: 0.7501	#Pop: 128	#M+: 1385	#M-: 65
EvolutionarySearch		#s: 128	Time elapsed: 6.47
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.67 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 34.97 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          36.88 |    128 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.852 ms	Trials: 3968	Used time : 2545 s	Next ID: 7	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    128 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.850 ms	Trials: 4032	Used time : 2595 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2242.89 |    832 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.850 ms	Trials: 4096	Used time : 2644 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2273.37 |    896 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.848 ms	Trials: 4160	Used time : 2689 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2273.37 |    896 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2285.85 |    640 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.848 ms	Trials: 4224	Used time : 2726 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.017 |        2273.37 |    896 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1983	fail_ct: 7	Time elapsed: 0.83
GA Iter: 0	Max score: 0.6195	Min score: 0.2958	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0346	Min score: 0.7454	#Pop: 128	#M+: 1374	#M-: 65
EvolutionarySearch		#s: 128	Time elapsed: 4.63
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.80 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 31.36 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2000	fail_ct: 2	Time elapsed: 1.19
GA Iter: 0	Max score: 0.6527	Min score: 0.3581	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9811	Min score: 0.7462	#Pop: 128	#M+: 1372	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.45
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.31 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 36.01 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1991	fail_ct: 7	Time elapsed: 1.26
GA Iter: 0	Max score: 0.6519	Min score: 0.2990	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9887	Min score: 0.7387	#Pop: 128	#M+: 1371	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.37
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................*************************************************Time elapsed for measurement: 16.44 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 14.14 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1965	fail_ct: 14	Time elapsed: 0.44
GA Iter: 0	Max score: 0.8139	Min score: 0.4140	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9853	Min score: 0.7642	#Pop: 128	#M+: 1386	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.28
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 21.52 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 22.73 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 866	fail_ct: 852	Time elapsed: 1.01
GA Iter: 0	Max score: 0.9586	Min score: 0.8995	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9753	Min score: 0.9411	#Pop: 128	#M+: 1376	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.88
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.90 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 39.10 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 11	Time elapsed: 0.82
GA Iter: 0	Max score: 0.6195	Min score: 0.2916	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9873	Min score: 0.7549	#Pop: 128	#M+: 1366	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.01
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.61 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 29.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2343.57 |    704 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.844 ms	Trials: 4288	Used time : 2790 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |    960 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2343.57 |    704 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.838 ms	Trials: 4352	Used time : 2846 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |    960 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    768 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.836 ms	Trials: 4416	Used time : 2902 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1024 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1583.79 |    384 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    768 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.836 ms	Trials: 4480	Used time : 2939 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1024 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    320 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    768 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.835 ms	Trials: 4544	Used time : 2989 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1024 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    768 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.835 ms	Trials: 4608	Used time : 3047 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 842	fail_ct: 869	Time elapsed: 0.75
GA Iter: 0	Max score: 0.9722	Min score: 0.8803	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9746	Min score: 0.9429	#Pop: 128	#M+: 1381	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 5.02
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.61 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 19.23 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 7	Time elapsed: 1.28
GA Iter: 0	Max score: 0.7612	Min score: 0.3646	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9889	Min score: 0.7871	#Pop: 128	#M+: 1375	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.18
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.81 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 27.85 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1956	fail_ct: 14	Time elapsed: 1.33
GA Iter: 0	Max score: 0.6287	Min score: 0.2916	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9695	Min score: 0.6922	#Pop: 128	#M+: 1386	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 5.23
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.02 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 29.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 7	Time elapsed: 1.28
GA Iter: 0	Max score: 0.6660	Min score: 0.2937	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9717	Min score: 0.7512	#Pop: 128	#M+: 1382	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 5.52
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.38 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.13 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1954	fail_ct: 17	Time elapsed: 1.11
GA Iter: 0	Max score: 0.7096	Min score: 0.3013	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9586	Min score: 0.6944	#Pop: 128	#M+: 1376	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 3.91
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.14 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 22.32 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1088 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    320 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    768 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.835 ms	Trials: 4672	Used time : 3100 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1088 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    768 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.835 ms	Trials: 4736	Used time : 3138 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1088 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    832 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2195.30 |   1088 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.835 ms	Trials: 4800	Used time : 3186 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1088 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    832 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2204.24 |   1152 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.834 ms	Trials: 4864	Used time : 3246 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1152 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    832 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2204.24 |   1152 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.834 ms	Trials: 4928	Used time : 3308 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1152 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2375.99 |    832 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2204.24 |   1216 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1989	fail_ct: 7	Time elapsed: 1.73
GA Iter: 0	Max score: 0.8040	Min score: 0.3420	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9758	Min score: 0.8050	#Pop: 128	#M+: 1376	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.11
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.87 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 30.35 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1959	fail_ct: 14	Time elapsed: 1.26
GA Iter: 0	Max score: 0.7050	Min score: 0.4022	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9727	Min score: 0.7667	#Pop: 128	#M+: 1382	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 3.85
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.16 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 30.33 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1962	fail_ct: 16	Time elapsed: 1.15
GA Iter: 0	Max score: 0.6048	Min score: 0.2852	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9514	Min score: 0.6901	#Pop: 128	#M+: 1373	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 4.72
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.99 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 43.89 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1975	fail_ct: 11	Time elapsed: 1.37
GA Iter: 0	Max score: 0.7381	Min score: 0.2923	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9333	Min score: 0.7414	#Pop: 128	#M+: 1387	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.30
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.39 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 39.23 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 872	fail_ct: 841	Time elapsed: 0.89
GA Iter: 0	Max score: 0.9659	Min score: 0.8936	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9717	Min score: 0.9484	#Pop: 128	#M+: 1380	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.69
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.53 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 32.98 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 829	fail_ct: 891	Time elapsed: 0.73
GA Iter: 0	Max score: 0.9445	Min score: 0.8896	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9729	Min score: 0.9346	#Pop: 128	#M+: 1373	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 4.98
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.06 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 45.99 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.834 ms	Trials: 4992	Used time : 3349 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1152 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1609.14 |    448 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2204.24 |   1216 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.834 ms	Trials: 5056	Used time : 3403 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1152 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2204.24 |   1216 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.832 ms	Trials: 5120	Used time : 3453 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1152 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.832 ms	Trials: 5184	Used time : 3516 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    384 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.832 ms	Trials: 5248	Used time : 3579 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.41 |    384 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.832 ms	Trials: 5312	Used time : 3628 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    192 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 289	fail_ct: 1759	Time elapsed: 1.69
GA Iter: 0	Max score: 0.9641	Min score: 0.4374	#Pop: 128	#M+: 0	#M-: 0
[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
T_softmax_maxelem auto_unroll: 16
parallel i0 (None)
  for k (None)
    T_softmax_maxelem = ...
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:43] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:43] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:45] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:45] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
T_softmax_maxelem auto_unroll: 16
parallel i0 (None)
  for k (None)
    T_softmax_maxelem = ...
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:07:46] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:07:46] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9858	Min score: 0.9179	#Pop: 128	#M+: 542	#M-: 5663
EvolutionarySearch		#s: 128	Time elapsed: 6.03
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.10 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.86 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1997	fail_ct: 6	Time elapsed: 1.05
GA Iter: 0	Max score: 1.0792	Min score: 0.8397	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0792	Min score: 0.9198	#Pop: 128	#M+: 1393	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 24.95 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1996	fail_ct: 6	Time elapsed: 1.41
GA Iter: 0	Max score: 0.6806	Min score: 0.3279	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9874	Min score: 0.7645	#Pop: 128	#M+: 1379	#M-: 67
EvolutionarySearch		#s: 128	Time elapsed: 5.56
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.61 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 26.08 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1949	fail_ct: 9	Time elapsed: 0.61
GA Iter: 0	Max score: 0.6910	Min score: 0.2910	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9421	Min score: 0.6851	#Pop: 128	#M+: 1374	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 3.08
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.46 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 28.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1959	fail_ct: 16	Time elapsed: 0.93
GA Iter: 0	Max score: 0.9415	Min score: 0.8152	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9660	Min score: 0.9158	#Pop: 128	#M+: 1387	#M-: 65
EvolutionarySearch		#s: 128	Time elapsed: 4.82
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
..............................................................|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.831 ms	Trials: 5376	Used time : 3692 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    192 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.831 ms	Trials: 5440	Used time : 3746 s	Next ID: 5	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    896 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.831 ms	Trials: 5504	Used time : 3801 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    960 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1280 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.831 ms	Trials: 5568	Used time : 3849 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    960 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.003 |          38.52 |    192 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1344 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.831 ms	Trials: 5632	Used time : 3893 s	Next ID: 7	
.T.T**************************************************************Time elapsed for measurement: 23.19 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 37.39 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 12	Time elapsed: 1.06
GA Iter: 0	Max score: 0.5687	Min score: 0.2912	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8810	Min score: 0.7054	#Pop: 128	#M+: 1384	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 5.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.38 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.45 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1977	fail_ct: 9	Time elapsed: 0.83
GA Iter: 0	Max score: 0.6315	Min score: 0.2787	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9332	Min score: 0.6757	#Pop: 128	#M+: 1384	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 4.09
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.15 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 30.67 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1975	fail_ct: 11	Time elapsed: 0.67
GA Iter: 0	Max score: 0.6248	Min score: 0.2791	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9687	Min score: 0.6962	#Pop: 128	#M+: 1378	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 3.55
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.00 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 10	Time elapsed: 0.85
GA Iter: 0	Max score: 0.6507	Min score: 0.3464	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9717	Min score: 0.7599	#Pop: 128	#M+: 1375	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.51 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 41.84 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1965	fail_ct: 21	Time elapsed: 1.26
GA Iter: 0	Max score: 0.6291	Min score: 0.2740	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8519	Min score: 0.6709	#Pop: 128	#M+: 1383	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 5.75
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.09 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 46.26 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1216 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    960 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1344 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 5696	Used time : 3959 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1280 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    960 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1344 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 5760	Used time : 4008 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1280 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    960 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1408 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 5824	Used time : 4058 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1344 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |    960 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1408 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 5888	Used time : 4114 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1344 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1024 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1408 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 5952	Used time : 4176 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1344 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    512 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1959	fail_ct: 15	Time elapsed: 0.84
GA Iter: 0	Max score: 0.7927	Min score: 0.4006	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9572	Min score: 0.7397	#Pop: 128	#M+: 1378	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 3.81
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 12.02 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 18.38 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1980	fail_ct: 14	Time elapsed: 0.89
GA Iter: 0	Max score: 0.6606	Min score: 0.2987	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8531	Min score: 0.6924	#Pop: 128	#M+: 1379	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.69
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.75 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.20 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 886	fail_ct: 877	Time elapsed: 1.00
GA Iter: 0	Max score: 0.9612	Min score: 0.8940	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9720	Min score: 0.9327	#Pop: 128	#M+: 1375	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.00
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.70 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 34.31 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 843	fail_ct: 878	Time elapsed: 0.96
GA Iter: 0	Max score: 0.9334	Min score: 0.8793	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9646	Min score: 0.9245	#Pop: 128	#M+: 1380	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.00
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.96 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 26.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1992	fail_ct: 6	Time elapsed: 1.16
GA Iter: 0	Max score: 0.5838	Min score: 0.3492	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9663	Min score: 0.7555	#Pop: 128	#M+: 1382	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.86
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.99 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 52.96 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1943	fail_ct: 20	Time elapsed: 0.98
GA Iter: 0	Max score: 0.6049	Min score: 0.2772	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9492	Min score: 0.6658	#Pop: 128	#M+: 1375	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.56
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.01 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 16.47 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1024 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1472 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6016	Used time : 4244 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1344 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1024 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1472 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6080	Used time : 4279 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1408 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    448 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1024 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1472 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6144	Used time : 4330 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1408 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    448 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1024 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1472 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6208	Used time : 4384 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1408 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1024 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1472 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6272	Used time : 4428 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1408 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1088 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1472 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6336	Used time : 4503 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2003	fail_ct: 6	Time elapsed: 1.56
GA Iter: 0	Max score: 0.6960	Min score: 0.2887	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9206	Min score: 0.6771	#Pop: 128	#M+: 1381	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 6.16
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.50 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 32.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 8	Time elapsed: 0.70
GA Iter: 0	Max score: 0.5840	Min score: 0.2803	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7962	Min score: 0.6592	#Pop: 128	#M+: 1386	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 3.95
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 26.04 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 9	Time elapsed: 1.22
GA Iter: 0	Max score: 0.6160	Min score: 0.2843	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8578	Min score: 0.6615	#Pop: 128	#M+: 1378	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.72
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.81 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 29.98 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1994	fail_ct: 5	Time elapsed: 0.93
GA Iter: 0	Max score: 0.6645	Min score: 0.3548	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9897	Min score: 0.7639	#Pop: 128	#M+: 1377	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.90
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.76 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.16 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1979	fail_ct: 8	Time elapsed: 1.28
GA Iter: 0	Max score: 0.5476	Min score: 0.2685	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7886	Min score: 0.6337	#Pop: 128	#M+: 1373	#M-: 67
EvolutionarySearch		#s: 128	Time elapsed: 5.25
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.58 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.26 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1408 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1088 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1536 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6400	Used time : 4542 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1472 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1088 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1536 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6464	Used time : 4598 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1472 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1088 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1600 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6528	Used time : 4644 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1536 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1088 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1600 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6592	Used time : 4697 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1536 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1152 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1600 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6656	Used time : 4761 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1536 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1152 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 5	Time elapsed: 1.44
GA Iter: 0	Max score: 0.6311	Min score: 0.3089	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8800	Min score: 0.6858	#Pop: 128	#M+: 1399	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.37
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.69 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.35 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 842	fail_ct: 908	Time elapsed: 0.53
GA Iter: 0	Max score: 0.9584	Min score: 0.8919	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9732	Min score: 0.9313	#Pop: 128	#M+: 1375	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 10.77 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 44.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 852	fail_ct: 869	Time elapsed: 0.83
GA Iter: 0	Max score: 0.9549	Min score: 0.8850	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9634	Min score: 0.9241	#Pop: 128	#M+: 1391	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.78 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 38.58 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1999	fail_ct: 5	Time elapsed: 0.74
GA Iter: 0	Max score: 0.9492	Min score: 0.8185	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9821	Min score: 0.9179	#Pop: 128	#M+: 1374	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 3.68
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.08 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.38 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1987	fail_ct: 11	Time elapsed: 0.94
GA Iter: 0	Max score: 0.6197	Min score: 0.3318	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9795	Min score: 0.7715	#Pop: 128	#M+: 1364	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 3.47
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.99 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 50.90 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 280	fail_ct: 1768	Time elapsed: 1.70
GA Iter: 0	Max score: 0.9678	Min score: 0.4033	#Pop: 128	#M+: 0	#M-: 0
[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:39] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:39] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:40] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:40] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:41] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:41] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:42] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:42] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:31:44] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:31:44] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9817	Min score: 0.7536	#Pop: 128	#M+: 536	#M-: 5655
EvolutionarySearch		#s: 128	Time elapsed: 6.54
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.30 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.47 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6720	Used time : 4825 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    512 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1152 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6784	Used time : 4887 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    512 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1152 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6848	Used time : 4947 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    256 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1152 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6912	Used time : 5006 s	Next ID: 5	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1152 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 6976	Used time : 5061 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    256 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1216 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7040	Used time : 5130 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 10	Time elapsed: 1.12
GA Iter: 0	Max score: 0.7618	Min score: 0.3928	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9503	Min score: 0.7533	#Pop: 128	#M+: 1381	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 5.84
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.86 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 45.53 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 15	Time elapsed: 1.01
GA Iter: 0	Max score: 0.5760	Min score: 0.2956	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8053	Min score: 0.6300	#Pop: 128	#M+: 1374	#M-: 68
EvolutionarySearch		#s: 128	Time elapsed: 4.74
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.77 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 34.20 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1991	fail_ct: 12	Time elapsed: 1.21
GA Iter: 0	Max score: 0.6962	Min score: 0.2902	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9049	Min score: 0.6864	#Pop: 128	#M+: 1373	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 4.24
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.66 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 52.15 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 11	Time elapsed: 1.37
GA Iter: 0	Max score: 0.5866	Min score: 0.2746	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7853	Min score: 0.6162	#Pop: 128	#M+: 1387	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 3.72
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.28 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 27.13 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 5	Time elapsed: 1.63
GA Iter: 0	Max score: 0.7152	Min score: 0.3473	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9535	Min score: 0.7688	#Pop: 128	#M+: 1377	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 6.28
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 34.89 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1986	fail_ct: 11	Time elapsed: 1.00
GA Iter: 0	Max score: 0.6095	Min score: 0.2972	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9059	Min score: 0.6668	#Pop: 128	#M+: 1376	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 3.39
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.27 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.19 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    576 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1216 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7104	Used time : 5190 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1216 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1664 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7168	Used time : 5257 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1600 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1216 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1728 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7232	Used time : 5310 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1664 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1216 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1728 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7296	Used time : 5381 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1664 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1216 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1792 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7360	Used time : 5437 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1664 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1280 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1792 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7424	Used time : 5495 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1958	fail_ct: 9	Time elapsed: 1.32
GA Iter: 0	Max score: 0.6349	Min score: 0.2937	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7508	Min score: 0.6120	#Pop: 128	#M+: 1384	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.13 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.37 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1982	fail_ct: 14	Time elapsed: 0.75
GA Iter: 0	Max score: 0.6242	Min score: 0.2984	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8877	Min score: 0.6791	#Pop: 128	#M+: 1379	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.68
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.13 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.54 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1992	fail_ct: 5	Time elapsed: 1.49
GA Iter: 0	Max score: 0.6568	Min score: 0.3448	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9348	Min score: 0.7475	#Pop: 128	#M+: 1375	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.39
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.13 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 27.67 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 850	fail_ct: 888	Time elapsed: 0.37
GA Iter: 0	Max score: 0.9672	Min score: 0.8756	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9721	Min score: 0.9301	#Pop: 128	#M+: 1381	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 4.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.63 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 35.45 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 880	fail_ct: 863	Time elapsed: 0.77
GA Iter: 0	Max score: 0.9393	Min score: 0.8875	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9618	Min score: 0.9327	#Pop: 128	#M+: 1383	#M-: 65
EvolutionarySearch		#s: 128	Time elapsed: 5.86
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.75 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 45.39 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1728 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1280 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1792 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7488	Used time : 5537 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1728 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1280 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1856 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7552	Used time : 5605 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1792 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1280 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1856 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7616	Used time : 5666 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1792 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    576 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1856 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7680	Used time : 5713 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1792 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    576 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1856 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7744	Used time : 5766 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1792 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1966	fail_ct: 11	Time elapsed: 0.78
GA Iter: 0	Max score: 0.5797	Min score: 0.2711	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7842	Min score: 0.6038	#Pop: 128	#M+: 1374	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 3.70
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.30 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 46.65 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1961	fail_ct: 14	Time elapsed: 1.19
GA Iter: 0	Max score: 0.7051	Min score: 0.4003	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9565	Min score: 0.7439	#Pop: 128	#M+: 1378	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.34
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.36 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 43.57 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1979	fail_ct: 5	Time elapsed: 1.20
GA Iter: 0	Max score: 0.6173	Min score: 0.2783	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8833	Min score: 0.6616	#Pop: 128	#M+: 1376	#M-: 66
EvolutionarySearch		#s: 128	Time elapsed: 4.96
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.90 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 39.25 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1960	fail_ct: 22	Time elapsed: 1.34
GA Iter: 0	Max score: 0.5871	Min score: 0.2786	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7904	Min score: 0.5991	#Pop: 128	#M+: 1376	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.21
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.68 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 23.36 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1999	fail_ct: 8	Time elapsed: 1.35
GA Iter: 0	Max score: 0.7808	Min score: 0.3608	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8977	Min score: 0.7396	#Pop: 128	#M+: 1378	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 5.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 15.80 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.67 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1987	fail_ct: 3	Time elapsed: 1.17
GA Iter: 0	Max score: 0.5279	Min score: 0.2933	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9095	Min score: 0.6634	#Pop: 128	#M+: 1368	#M-: 85
EvolutionarySearch		#s: 128	Time elapsed: 5.11
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.56 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 49.32 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1856 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7808	Used time : 5830 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1792 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    640 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1920 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7872	Used time : 5895 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1792 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1920 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 7936	Used time : 5961 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1856 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1920 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8000	Used time : 6028 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1856 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1344 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1984 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8064	Used time : 6072 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1856 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1408 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1984 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8128	Used time : 6135 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1920 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1957	fail_ct: 9	Time elapsed: 1.49
GA Iter: 0	Max score: 0.6474	Min score: 0.2927	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6935	Min score: 0.5984	#Pop: 128	#M+: 1387	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 51.91 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1989	fail_ct: 6	Time elapsed: 1.15
GA Iter: 0	Max score: 0.6530	Min score: 0.3355	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9228	Min score: 0.7372	#Pop: 128	#M+: 1384	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.71
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.17 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.00 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1971	fail_ct: 10	Time elapsed: 1.23
GA Iter: 0	Max score: 0.6708	Min score: 0.2923	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9084	Min score: 0.6554	#Pop: 128	#M+: 1376	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.73
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.03 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 25.83 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 853	fail_ct: 871	Time elapsed: 0.29
GA Iter: 0	Max score: 0.9696	Min score: 0.8856	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9724	Min score: 0.9357	#Pop: 128	#M+: 1375	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 2.18
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.04 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 46.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1978	fail_ct: 7	Time elapsed: 1.14
GA Iter: 0	Max score: 0.6124	Min score: 0.2871	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7819	Min score: 0.5979	#Pop: 128	#M+: 1365	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 5.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.52 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 49.07 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1408 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   1984 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8192	Used time : 6207 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1920 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1408 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2048 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8256	Used time : 6279 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1920 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2048 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8320	Used time : 6334 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    640 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2048 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8384	Used time : 6379 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2048 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8448	Used time : 6436 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    640 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 844	fail_ct: 866	Time elapsed: 0.56
GA Iter: 0	Max score: 0.9528	Min score: 0.8879	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9608	Min score: 0.9295	#Pop: 128	#M+: 1377	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 4.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.19 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 47.75 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 6	Time elapsed: 1.33
GA Iter: 0	Max score: 0.9551	Min score: 0.8218	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9735	Min score: 0.9313	#Pop: 128	#M+: 1385	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.86
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.53 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 24.55 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 287	fail_ct: 1761	Time elapsed: 1.18
GA Iter: 0	Max score: 0.9634	Min score: 0.3679	#Pop: 128	#M+: 0	#M-: 0
[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:33] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:33] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:35] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:35] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:36] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:36] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[23:56:37] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [23:56:37] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9811	Min score: 0.5411	#Pop: 128	#M+: 550	#M-: 5546
EvolutionarySearch		#s: 128	Time elapsed: 5.15
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.46 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 51.41 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 14	Time elapsed: 0.97
GA Iter: 0	Max score: 0.7668	Min score: 0.4005	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9613	Min score: 0.7477	#Pop: 128	#M+: 1387	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.82
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 25.73 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.91 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 5	Time elapsed: 1.45
GA Iter: 0	Max score: 0.6339	Min score: 0.3181	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9121	Min score: 0.6652	#Pop: 128	#M+: 1386	#M-: 84
EvolutionarySearch		#s: 128	Time elapsed: 6.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 25.76 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 45.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1983	fail_ct: 7	Time elapsed: 0.65
GA Iter: 0	Max score: 0.5936	Min score: 0.3320	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9121	Min score: 0.7169	#Pop: 128	#M+: 1395	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 2.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.00 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 52.62 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8512	Used time : 6508 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    320 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8576	Used time : 6573 s	Next ID: 5	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    320 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8640	Used time : 6625 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    704 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8704	Used time : 6694 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   1984 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8768	Used time : 6768 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2048 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1472 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8832	Used time : 6848 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2048 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1978	fail_ct: 9	Time elapsed: 1.19
GA Iter: 0	Max score: 0.5950	Min score: 0.2704	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7661	Min score: 0.5973	#Pop: 128	#M+: 1377	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.44
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.33 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.97 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1994	fail_ct: 4	Time elapsed: 1.28
GA Iter: 0	Max score: 0.6279	Min score: 0.2984	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8746	Min score: 0.6626	#Pop: 128	#M+: 1380	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.99
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.56 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.52 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1958	fail_ct: 19	Time elapsed: 0.45
GA Iter: 0	Max score: 0.6130	Min score: 0.2844	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7090	Min score: 0.5944	#Pop: 128	#M+: 1388	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 2.14
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................***********************************************Time elapsed for measurement: 12.57 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 37.99 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1933	fail_ct: 16	Time elapsed: 0.81
GA Iter: 0	Max score: 0.8816	Min score: 0.5300	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8818	Min score: 0.6102	#Pop: 128	#M+: 1381	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.98
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
..............................................................|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1536 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2112 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8896	Used time : 6921 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2048 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1536 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2176 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 8960	Used time : 6984 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2112 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1536 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2176 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 9024	Used time : 7050 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2112 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1536 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.002 |          59.84 |    256 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2240 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.820 ms	Trials: 9088	Used time : 7103 s	Next ID: 7	
.T.T**************************************************************Time elapsed for measurement: 27.42 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 35.15 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1989	fail_ct: 3	Time elapsed: 0.69
GA Iter: 0	Max score: 0.7278	Min score: 0.3353	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9292	Min score: 0.7313	#Pop: 128	#M+: 1371	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 2.70
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.30 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.29 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 8	Time elapsed: 1.14
GA Iter: 0	Max score: 0.7066	Min score: 0.3148	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9128	Min score: 0.6641	#Pop: 128	#M+: 1383	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 4.73
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.88 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.16 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1960	fail_ct: 17	Time elapsed: 1.46
GA Iter: 0	Max score: 0.5426	Min score: 0.2897	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7189	Min score: 0.5905	#Pop: 128	#M+: 1384	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 6.72
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.78 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 47.48 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 867	fail_ct: 853	Time elapsed: 0.70
GA Iter: 0	Max score: 0.9481	Min score: 0.8744	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9741	Min score: 0.9328	#Pop: 128	#M+: 1388	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 3.45
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.70 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 28.46 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 868	fail_ct: 856	Time elapsed: 0.96
GA Iter: 0	Max score: 0.9569	Min score: 0.8764	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9601	Min score: 0.9295	#Pop: 128	#M+: 1363	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 4.66
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.68 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 63.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2112 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1536 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2240 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9152	Used time : 7172 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2112 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1600 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2240 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9216	Used time : 7221 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2176 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1600 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2240 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9280	Used time : 7282 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2176 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    704 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1600 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2304 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9344	Used time : 7356 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2176 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    704 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1600 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2304 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9408	Used time : 7402 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2176 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    768 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 19	Time elapsed: 0.94
GA Iter: 0	Max score: 0.7411	Min score: 0.4076	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9611	Min score: 0.7351	#Pop: 128	#M+: 1384	#M-: 68
EvolutionarySearch		#s: 128	Time elapsed: 4.62
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.46 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 54.42 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1987	fail_ct: 11	Time elapsed: 1.12
GA Iter: 0	Max score: 0.7109	Min score: 0.3232	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9009	Min score: 0.7214	#Pop: 128	#M+: 1374	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.63
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.56 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 49.83 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 13	Time elapsed: 1.58
GA Iter: 0	Max score: 0.8239	Min score: 0.2985	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9149	Min score: 0.6730	#Pop: 128	#M+: 1383	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.90
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.22 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 51.22 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 10	Time elapsed: 0.67
GA Iter: 0	Max score: 0.5887	Min score: 0.2889	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6703	Min score: 0.5843	#Pop: 128	#M+: 1375	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 2.71
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 41.12 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1971	fail_ct: 10	Time elapsed: 0.89
GA Iter: 0	Max score: 0.6371	Min score: 0.2838	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6895	Min score: 0.5826	#Pop: 128	#M+: 1383	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 5.52
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 24.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 65.59 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1972	fail_ct: 7	Time elapsed: 0.98
GA Iter: 0	Max score: 0.7107	Min score: 0.2849	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9479	Min score: 0.6648	#Pop: 128	#M+: 1378	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.88
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 57.22 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1600 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2304 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9472	Used time : 7486 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2176 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1600 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2304 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9536	Used time : 7564 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2176 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1664 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2304 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9600	Used time : 7638 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2240 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1664 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2304 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9664	Used time : 7717 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2240 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1664 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2368 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9728	Used time : 7777 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2240 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1664 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2432 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9792	Used time : 7874 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 8	Time elapsed: 1.16
GA Iter: 0	Max score: 0.6047	Min score: 0.3372	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9211	Min score: 0.7160	#Pop: 128	#M+: 1374	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 4.33
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.33 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 47.34 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1971	fail_ct: 14	Time elapsed: 1.47
GA Iter: 0	Max score: 0.5830	Min score: 0.2599	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6606	Min score: 0.5787	#Pop: 128	#M+: 1370	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 5.87
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.51 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 39.47 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1991	fail_ct: 9	Time elapsed: 1.20
GA Iter: 0	Max score: 0.6889	Min score: 0.3026	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9280	Min score: 0.6858	#Pop: 128	#M+: 1377	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.48
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.06 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 36.71 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 849	fail_ct: 867	Time elapsed: 1.06
GA Iter: 0	Max score: 1.1270	Min score: 0.5589	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.1270	Min score: 0.9380	#Pop: 128	#M+: 1366	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 5.65
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 46.25 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1975	fail_ct: 11	Time elapsed: 1.86
GA Iter: 0	Max score: 0.7402	Min score: 0.4154	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9503	Min score: 0.7408	#Pop: 128	#M+: 1375	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.62 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 41.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2304 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1664 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2432 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9856	Used time : 7960 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2304 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1728 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2432 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9920	Used time : 8033 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2304 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1728 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 9984	Used time : 8095 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    768 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1728 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10048	Used time : 8153 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    832 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1728 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10112	Used time : 8215 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1728 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 5	Time elapsed: 1.34
GA Iter: 0	Max score: 0.6301	Min score: 0.3246	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9573	Min score: 0.7253	#Pop: 128	#M+: 1382	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 5.52
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.38 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 47.29 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 861	fail_ct: 872	Time elapsed: 1.11
GA Iter: 0	Max score: 0.9600	Min score: 0.8755	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9600	Min score: 0.9173	#Pop: 128	#M+: 1374	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 5.88
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.10 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 46.30 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 283	fail_ct: 1765	Time elapsed: 1.67
GA Iter: 0	Max score: 0.9435	Min score: 0.3307	#Pop: 128	#M+: 0	#M-: 0
[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:27] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:27] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:26:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:26:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9758	Min score: 0.5053	#Pop: 128	#M+: 551	#M-: 5537
EvolutionarySearch		#s: 128	Time elapsed: 4.37
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.53 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 60.19 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1965	fail_ct: 12	Time elapsed: 1.24
GA Iter: 0	Max score: 0.5455	Min score: 0.2660	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6532	Min score: 0.5795	#Pop: 128	#M+: 1378	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 5.15
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 17.86 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 47.59 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 7	Time elapsed: 1.41
GA Iter: 0	Max score: 0.6224	Min score: 0.3064	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8938	Min score: 0.6825	#Pop: 128	#M+: 1375	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 6.34
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.84 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 43.88 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 11	Time elapsed: 0.84
GA Iter: 0	Max score: 0.9517	Min score: 0.8465	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9699	Min score: 0.9330	#Pop: 128	#M+: 1382	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 4.56
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
..............................................................-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10176	Used time : 8281 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    768 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10240	Used time : 8352 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    384 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10304	Used time : 8418 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2496 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10368	Used time : 8495 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2368 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2560 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10432	Used time : 8567 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2432 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    384 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2560 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10496	Used time : 8636 s	Next ID: 5	
.T.T**************************************************************Time elapsed for measurement: 26.31 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 50.45 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1963	fail_ct: 15	Time elapsed: 1.19
GA Iter: 0	Max score: 0.5696	Min score: 0.2607	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6817	Min score: 0.5768	#Pop: 128	#M+: 1394	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 3.75
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.74 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 40.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 6	Time elapsed: 0.92
GA Iter: 0	Max score: 0.6997	Min score: 0.3373	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9142	Min score: 0.7173	#Pop: 128	#M+: 1377	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 21.61 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 48.58 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 4	Time elapsed: 1.52
GA Iter: 0	Max score: 0.7915	Min score: 0.2913	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9027	Min score: 0.6703	#Pop: 128	#M+: 1378	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 5.17
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.13 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 56.48 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 7	Time elapsed: 1.26
GA Iter: 0	Max score: 0.7619	Min score: 0.2748	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7619	Min score: 0.5800	#Pop: 128	#M+: 1384	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.53
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.69 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 45.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1984	fail_ct: 9	Time elapsed: 0.69
GA Iter: 0	Max score: 0.7402	Min score: 0.3042	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8906	Min score: 0.6911	#Pop: 128	#M+: 1388	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.04
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.11 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 30.61 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2432 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2560 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10560	Used time : 8718 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2432 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1792 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2624 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10624	Used time : 8777 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2432 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1856 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2624 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10688	Used time : 8853 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2496 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1856 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2624 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10752	Used time : 8930 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2496 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1856 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2688 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10816	Used time : 9001 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2560 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    832 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 855	fail_ct: 843	Time elapsed: 0.75
GA Iter: 0	Max score: 0.9752	Min score: 0.5530	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9753	Min score: 0.9238	#Pop: 128	#M+: 1384	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 6.82
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.87 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 50.83 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1997	fail_ct: 5	Time elapsed: 1.24
GA Iter: 0	Max score: 0.7249	Min score: 0.3420	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9131	Min score: 0.7162	#Pop: 128	#M+: 1374	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.70
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.44 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 56.39 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1968	fail_ct: 17	Time elapsed: 1.30
GA Iter: 0	Max score: 0.7107	Min score: 0.4076	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8408	Min score: 0.7363	#Pop: 128	#M+: 1378	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 5.37
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.34 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 59.93 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 869	fail_ct: 863	Time elapsed: 0.70
GA Iter: 0	Max score: 0.9380	Min score: 0.8549	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9591	Min score: 0.9159	#Pop: 128	#M+: 1388	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.61
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.25 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.70 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 13	Time elapsed: 1.34
GA Iter: 0	Max score: 0.6011	Min score: 0.2831	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7053	Min score: 0.5677	#Pop: 128	#M+: 1385	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 6.63
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 21.15 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 70.78 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1977	fail_ct: 13	Time elapsed: 1.05
GA Iter: 0	Max score: 0.6980	Min score: 0.2924	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8798	Min score: 0.6691	#Pop: 128	#M+: 1376	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 6.29
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.59 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 45.11 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1856 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2688 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10880	Used time : 9047 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2560 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1856 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2688 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 10944	Used time : 9121 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2560 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    896 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1920 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2688 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11008	Used time : 9198 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2560 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    832 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1920 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2688 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11072	Used time : 9283 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2560 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1920 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2688 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11136	Used time : 9334 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2560 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1920 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2752 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11200	Used time : 9434 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1979	fail_ct: 13	Time elapsed: 1.32
GA Iter: 0	Max score: 0.6885	Min score: 0.2813	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6885	Min score: 0.5674	#Pop: 128	#M+: 1374	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.48 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 72.08 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1989	fail_ct: 6	Time elapsed: 1.68
GA Iter: 0	Max score: 0.6698	Min score: 0.3308	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9146	Min score: 0.7133	#Pop: 128	#M+: 1384	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 6.51
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.98 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 63.07 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 8	Time elapsed: 0.50
GA Iter: 0	Max score: 0.6544	Min score: 0.3151	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8750	Min score: 0.6747	#Pop: 128	#M+: 1382	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 2.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.83 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 42.84 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1943	fail_ct: 18	Time elapsed: 1.18
GA Iter: 0	Max score: 0.6319	Min score: 0.5054	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.1006	Min score: 0.5669	#Pop: 128	#M+: 1383	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.93
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 21.33 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 41.18 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 13	Time elapsed: 0.82
GA Iter: 0	Max score: 0.5308	Min score: 0.2700	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6820	Min score: 0.5663	#Pop: 128	#M+: 1388	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 5.07
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.24 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 65.17 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2624 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1920 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2752 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11264	Used time : 9507 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2624 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1920 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2816 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11328	Used time : 9601 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2624 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1984 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2816 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11392	Used time : 9695 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2688 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1984 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    320 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2816 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11456	Used time : 9756 s	Next ID: 7	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2688 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1984 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2816 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11520	Used time : 9825 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2688 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1984 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2880 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1991	fail_ct: 4	Time elapsed: 1.11
GA Iter: 0	Max score: 0.6582	Min score: 0.2912	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8936	Min score: 0.6582	#Pop: 128	#M+: 1382	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.09
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.66 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 19.10 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2005	fail_ct: 5	Time elapsed: 0.45
GA Iter: 0	Max score: 0.7232	Min score: 0.3197	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9176	Min score: 0.7118	#Pop: 128	#M+: 1382	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 2.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.05 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1979	fail_ct: 11	Time elapsed: 1.14
GA Iter: 0	Max score: 0.6395	Min score: 0.3939	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8508	Min score: 0.7307	#Pop: 128	#M+: 1389	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 3.10
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.14 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 66.76 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 871	fail_ct: 843	Time elapsed: 0.76
GA Iter: 0	Max score: 0.9733	Min score: 0.8242	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9733	Min score: 0.9264	#Pop: 128	#M+: 1389	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 3.25
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.35 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 53.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1963	fail_ct: 14	Time elapsed: 1.45
GA Iter: 0	Max score: 0.6158	Min score: 0.2824	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6676	Min score: 0.5563	#Pop: 128	#M+: 1377	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 6.71
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.00 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 55.86 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 843	fail_ct: 894	Time elapsed: 0.37
GA Iter: 0	Max score: 0.9637	Min score: 0.5257	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9637	Min score: 0.9134	#Pop: 128	#M+: 1374	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 3.74
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.46 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 76.01 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11584	Used time : 9913 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   1984 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2880 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11648	Used time : 9953 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |    960 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2880 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11712	Used time : 10036 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    896 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2880 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11776	Used time : 10120 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2880 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11840	Used time : 10188 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    896 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2944 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11904	Used time : 10271 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    448 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 280	fail_ct: 1768	Time elapsed: 1.26
GA Iter: 0	Max score: 0.6878	Min score: 0.3208	#Pop: 128	#M+: 0	#M-: 0
[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:49] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:49] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:51] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:51] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:52] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:52] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[00:58:53] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [00:58:53] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9735	Min score: 0.4514	#Pop: 128	#M+: 541	#M-: 5798
EvolutionarySearch		#s: 128	Time elapsed: 5.56
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.25 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 23.62 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1996	fail_ct: 6	Time elapsed: 0.41
GA Iter: 0	Max score: 0.6490	Min score: 0.3050	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8937	Min score: 0.6691	#Pop: 128	#M+: 1383	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 2.26
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.14 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 61.49 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1971	fail_ct: 11	Time elapsed: 1.16
GA Iter: 0	Max score: 0.9742	Min score: 0.8528	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9796	Min score: 0.9383	#Pop: 128	#M+: 1392	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.01 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 43.67 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1984	fail_ct: 6	Time elapsed: 1.60
GA Iter: 0	Max score: 0.7135	Min score: 0.3109	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9331	Min score: 0.7154	#Pop: 128	#M+: 1379	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 6.51
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 25.55 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 36.30 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1974	fail_ct: 12	Time elapsed: 0.95
GA Iter: 0	Max score: 0.6053	Min score: 0.2604	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7473	Min score: 0.5576	#Pop: 128	#M+: 1375	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.37
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 73.44 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 8	Time elapsed: 1.13
GA Iter: 0	Max score: 0.6686	Min score: 0.2821	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9049	Min score: 0.6673	#Pop: 128	#M+: 1382	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.88
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 24.00 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 82.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2944 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 11968	Used time : 10360 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2752 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2944 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12032	Used time : 10403 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2816 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    448 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2944 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12096	Used time : 10479 s	Next ID: 5	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2816 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2048 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2944 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12160	Used time : 10548 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2816 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2112 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   2944 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12224	Used time : 10618 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2816 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2112 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3008 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12288	Used time : 10716 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1966	fail_ct: 12	Time elapsed: 1.00
GA Iter: 0	Max score: 0.5525	Min score: 0.2755	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7884	Min score: 0.5524	#Pop: 128	#M+: 1392	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 5.30
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.37 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 68.53 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1979	fail_ct: 15	Time elapsed: 1.56
GA Iter: 0	Max score: 0.8225	Min score: 0.3184	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9724	Min score: 0.7297	#Pop: 128	#M+: 1388	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.59
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.60 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 54.19 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1977	fail_ct: 17	Time elapsed: 1.13
GA Iter: 0	Max score: 0.7127	Min score: 0.3918	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8788	Min score: 0.7218	#Pop: 128	#M+: 1383	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.99
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.54 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 48.77 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1997	fail_ct: 6	Time elapsed: 1.66
GA Iter: 0	Max score: 0.5847	Min score: 0.2875	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9109	Min score: 0.6813	#Pop: 128	#M+: 1388	#M-: 66
EvolutionarySearch		#s: 128	Time elapsed: 4.90
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.10 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 73.31 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 878	fail_ct: 834	Time elapsed: 1.08
GA Iter: 0	Max score: 0.9694	Min score: 0.5498	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9694	Min score: 0.9219	#Pop: 128	#M+: 1373	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 5.02
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 58.51 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2880 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2112 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3008 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12352	Used time : 10830 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2880 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2112 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3072 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12416	Used time : 10922 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2880 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1024 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3072 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12480	Used time : 11001 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2880 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3072 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12544	Used time : 11073 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2944 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |    960 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3072 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12608	Used time : 11168 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2944 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 14	Time elapsed: 1.28
GA Iter: 0	Max score: 0.5031	Min score: 0.2596	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6862	Min score: 0.5455	#Pop: 128	#M+: 1372	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 5.84
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.20 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 44.03 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 870	fail_ct: 867	Time elapsed: 0.60
GA Iter: 0	Max score: 0.9299	Min score: 0.5696	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9619	Min score: 0.9003	#Pop: 128	#M+: 1383	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 3.67
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.31 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 72.15 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1996	fail_ct: 6	Time elapsed: 1.03
GA Iter: 0	Max score: 0.7023	Min score: 0.2929	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9290	Min score: 0.6867	#Pop: 128	#M+: 1374	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.28
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.60 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 70.96 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1982	fail_ct: 10	Time elapsed: 1.12
GA Iter: 0	Max score: 0.6732	Min score: 0.3178	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9785	Min score: 0.7673	#Pop: 128	#M+: 1386	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.59 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 50.79 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 15	Time elapsed: 1.12
GA Iter: 0	Max score: 0.5755	Min score: 0.2699	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6601	Min score: 0.5472	#Pop: 128	#M+: 1386	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.91
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.03 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 48.96 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 6	Time elapsed: 1.10
GA Iter: 0	Max score: 0.5954	Min score: 0.3031	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8957	Min score: 0.6759	#Pop: 128	#M+: 1378	#M-: 66
EvolutionarySearch		#s: 128	Time elapsed: 5.63
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.07 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 60.39 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3072 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12672	Used time : 11243 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2944 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |    960 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3136 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12736	Used time : 11308 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   2944 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3136 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12800	Used time : 11398 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3008 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2176 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3136 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12864	Used time : 11488 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3008 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2240 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3136 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12928	Used time : 11567 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3008 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2240 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3200 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 12992	Used time : 11640 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3072 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 20	Time elapsed: 1.38
GA Iter: 0	Max score: 0.5728	Min score: 0.2752	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7198	Min score: 0.5489	#Pop: 128	#M+: 1377	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 5.47
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.69 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 60.55 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2001	fail_ct: 7	Time elapsed: 1.18
GA Iter: 0	Max score: 0.7164	Min score: 0.3278	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9687	Min score: 0.7725	#Pop: 128	#M+: 1382	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.97
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.09 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 41.09 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 12	Time elapsed: 1.12
GA Iter: 0	Max score: 0.6976	Min score: 0.3896	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9213	Min score: 0.7125	#Pop: 128	#M+: 1384	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 4.91
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.02 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 66.80 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 5	Time elapsed: 0.86
GA Iter: 0	Max score: 0.5681	Min score: 0.2807	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8987	Min score: 0.6793	#Pop: 128	#M+: 1383	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.62
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.92 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 33.19 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1952	fail_ct: 23	Time elapsed: 0.95
GA Iter: 0	Max score: 0.6479	Min score: 0.5028	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.1925	Min score: 0.7017	#Pop: 128	#M+: 1385	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.39
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.65 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 76.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2240 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3200 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13056	Used time : 11724 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3072 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2240 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3264 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13120	Used time : 11807 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3072 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1088 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2304 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3264 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13184	Used time : 11871 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3072 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2304 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3264 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13248	Used time : 11960 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2304 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          66.95 |    384 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3264 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13312	Used time : 12015 s	Next ID: 7	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2304 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3264 |
-----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 10	Time elapsed: 1.43
GA Iter: 0	Max score: 0.5629	Min score: 0.2608	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6700	Min score: 0.5378	#Pop: 128	#M+: 1390	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 6.91
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.28 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 81.85 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 862	fail_ct: 856	Time elapsed: 1.19
GA Iter: 0	Max score: 0.9356	Min score: 0.5075	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9537	Min score: 0.9116	#Pop: 128	#M+: 1385	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.73
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.08 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 61.54 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1999	fail_ct: 4	Time elapsed: 1.19
GA Iter: 0	Max score: 0.7585	Min score: 0.3230	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9673	Min score: 0.7317	#Pop: 128	#M+: 1379	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.93 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 66.32 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 829	fail_ct: 893	Time elapsed: 1.13
GA Iter: 0	Max score: 0.9164	Min score: 0.5110	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9596	Min score: 0.8890	#Pop: 128	#M+: 1380	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 6.37
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.71 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 62.33 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 283	fail_ct: 1765	Time elapsed: 1.30
GA Iter: 0	Max score: 0.6472	Min score: 0.3002	#Pop: 128	#M+: 0	#M-: 0
[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:05] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:05] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:06] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:06] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:07] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:07] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[01:34:09] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [01:34:09] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9692	Min score: 0.3411	#Pop: 128	#M+: 553	#M-: 5551
EvolutionarySearch		#s: 128	Time elapsed: 4.70
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.93 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 97.34 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 8	Time elapsed: 1.11
GA Iter: 0	Max score: 0.6727	Min score: 0.2816	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8779	Min score: 0.6817	#Pop: 128	#M+: 1374	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.14
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.94 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 62.43 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13376	Used time : 12115 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1024 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2304 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3328 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13440	Used time : 12221 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2304 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3328 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13504	Used time : 12300 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1024 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3328 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13568	Used time : 12394 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    512 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3328 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13632	Used time : 12476 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3136 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3328 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13696	Used time : 12591 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3200 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 12	Time elapsed: 1.15
GA Iter: 0	Max score: 0.5611	Min score: 0.2800	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6457	Min score: 0.5376	#Pop: 128	#M+: 1392	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.25
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 17.27 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 96.68 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1992	fail_ct: 9	Time elapsed: 0.88
GA Iter: 0	Max score: 0.9611	Min score: 0.8405	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0018	Min score: 0.9427	#Pop: 128	#M+: 1382	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 5.58
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
...............................................................|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3328 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13760	Used time : 12671 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3200 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    512 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3392 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13824	Used time : 12792 s	Next ID: 5	
.T***************************************************************Time elapsed for measurement: 29.00 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 56.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 11	Time elapsed: 1.05
GA Iter: 0	Max score: 0.7224	Min score: 0.2783	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8846	Min score: 0.6775	#Pop: 128	#M+: 1388	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.73
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.32 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 94.17 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1961	fail_ct: 15	Time elapsed: 1.34
GA Iter: 0	Max score: 0.5680	Min score: 0.2754	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7898	Min score: 0.5381	#Pop: 128	#M+: 1379	#M-: 67
EvolutionarySearch		#s: 128	Time elapsed: 4.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 30.68 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 80.59 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 3	Time elapsed: 1.00
GA Iter: 0	Max score: 0.7685	Min score: 0.3198	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9613	Min score: 0.7648	#Pop: 128	#M+: 1381	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 5.01
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.18 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 50.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1959	fail_ct: 17	Time elapsed: 1.38
GA Iter: 0	Max score: 0.6960	Min score: 0.4166	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8684	Min score: 0.7187	#Pop: 128	#M+: 1389	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 5.26
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.20 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 63.52 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1976	fail_ct: 9	Time elapsed: 1.41
GA Iter: 0	Max score: 0.7057	Min score: 0.2806	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8513	Min score: 0.6932	#Pop: 128	#M+: 1376	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.47
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.80 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 67.13 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3200 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3392 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13888	Used time : 12884 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3264 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3392 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 13952	Used time : 13005 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3264 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2368 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3456 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 14016	Used time : 13122 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3264 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1152 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2432 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3456 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 14080	Used time : 13195 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3264 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2432 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3456 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 14144	Used time : 13283 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3328 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1971	fail_ct: 14	Time elapsed: 1.15
GA Iter: 0	Max score: 0.5140	Min score: 0.2852	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7687	Min score: 0.5336	#Pop: 128	#M+: 1379	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.36 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 64.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 843	fail_ct: 885	Time elapsed: 0.86
GA Iter: 0	Max score: 0.9630	Min score: 0.5124	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9631	Min score: 0.9073	#Pop: 128	#M+: 1376	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.37 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 61.35 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 5	Time elapsed: 1.41
GA Iter: 0	Max score: 0.6853	Min score: 0.3406	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9647	Min score: 0.7612	#Pop: 128	#M+: 1378	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 6.62
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.51 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 60.95 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 836	fail_ct: 867	Time elapsed: 0.66
GA Iter: 0	Max score: 0.9394	Min score: 0.4952	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9605	Min score: 0.8857	#Pop: 128	#M+: 1391	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.53
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.32 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 84.49 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1983	fail_ct: 12	Time elapsed: 1.47
GA Iter: 0	Max score: 0.5352	Min score: 0.2762	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8720	Min score: 0.6817	#Pop: 128	#M+: 1377	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 5.36
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.08 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 74.55 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1980	fail_ct: 14	Time elapsed: 1.37
GA Iter: 0	Max score: 0.5588	Min score: 0.2635	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6623	Min score: 0.5216	#Pop: 128	#M+: 1374	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 4.49
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.80 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 99.82 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2432 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3456 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 14208	Used time : 13374 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3328 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.40 |   1088 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2432 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3520 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.818 ms	Trials: 14272	Used time : 13467 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3328 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2432 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3520 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14336	Used time : 13548 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3328 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1088 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2496 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3520 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14400	Used time : 13635 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3328 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2496 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3520 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14464	Used time : 13735 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3392 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2496 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3520 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14528	Used time : 13832 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1998	fail_ct: 5	Time elapsed: 1.36
GA Iter: 0	Max score: 0.7159	Min score: 0.3289	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9599	Min score: 0.7418	#Pop: 128	#M+: 1376	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.83
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.74 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 79.45 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1961	fail_ct: 13	Time elapsed: 0.61
GA Iter: 0	Max score: 0.5659	Min score: 0.2682	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7787	Min score: 0.5319	#Pop: 128	#M+: 1380	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 3.67
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.49 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 71.47 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1988	fail_ct: 7	Time elapsed: 1.23
GA Iter: 0	Max score: 0.6105	Min score: 0.2697	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8813	Min score: 0.6624	#Pop: 128	#M+: 1394	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 5.84
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.70 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 54.49 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 11	Time elapsed: 1.03
GA Iter: 0	Max score: 0.6521	Min score: 0.4089	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8663	Min score: 0.6996	#Pop: 128	#M+: 1378	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 6.61
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.49 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 80.55 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1963	fail_ct: 19	Time elapsed: 1.46
GA Iter: 0	Max score: 0.5617	Min score: 0.2530	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7707	Min score: 0.5200	#Pop: 128	#M+: 1387	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.74
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.67 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 71.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3392 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2496 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3584 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14592	Used time : 13951 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3392 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3584 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14656	Used time : 14055 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3392 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3648 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14720	Used time : 14143 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3456 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1216 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3648 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14784	Used time : 14223 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3456 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3648 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14848	Used time : 14330 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3456 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3712 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1994	fail_ct: 9	Time elapsed: 1.20
GA Iter: 0	Max score: 0.6142	Min score: 0.2889	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9036	Min score: 0.6856	#Pop: 128	#M+: 1382	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 5.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.91 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 51.56 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 844	fail_ct: 833	Time elapsed: 0.91
GA Iter: 0	Max score: 0.9470	Min score: 0.4935	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9470	Min score: 0.8902	#Pop: 128	#M+: 1382	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 6.66
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.11 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 72.18 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1977	fail_ct: 8	Time elapsed: 0.91
GA Iter: 0	Max score: 0.8016	Min score: 0.3331	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9512	Min score: 0.7319	#Pop: 128	#M+: 1379	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 4.96
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 27.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 90.14 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1964	fail_ct: 18	Time elapsed: 0.99
GA Iter: 0	Max score: 0.5418	Min score: 0.2649	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7107	Min score: 0.5234	#Pop: 128	#M+: 1386	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.85
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.28 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 79.30 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 839	fail_ct: 891	Time elapsed: 0.81
GA Iter: 0	Max score: 0.9309	Min score: 0.5025	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9589	Min score: 0.8730	#Pop: 128	#M+: 1375	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 4.84
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 56.54 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1989	fail_ct: 4	Time elapsed: 1.00
GA Iter: 0	Max score: 0.7653	Min score: 0.2759	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8735	Min score: 0.6801	#Pop: 128	#M+: 1381	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 4.23
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 35.48 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14912	Used time : 14429 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3520 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1152 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3712 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 14976	Used time : 14502 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3520 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2560 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3712 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15040	Used time : 14595 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3520 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2624 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3712 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15104	Used time : 14719 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3520 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1152 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2624 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3776 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15168	Used time : 14819 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3520 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2624 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3776 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15232	Used time : 14891 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    576 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3584 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 292	fail_ct: 1756	Time elapsed: 2.09
GA Iter: 0	Max score: 0.6048	Min score: 0.1333	#Pop: 112	#M+: 0	#M-: 0
[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:17] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:17] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[02:15:18] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [02:15:18] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9437	Min score: 0.1333	#Pop: 125	#M+: 555	#M-: 5705
EvolutionarySearch		#s: 125	Time elapsed: 6.72
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.45 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 87.65 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1975	fail_ct: 7	Time elapsed: 1.06
GA Iter: 0	Max score: 1.0093	Min score: 0.8607	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0257	Min score: 0.9489	#Pop: 128	#M+: 1379	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 5.92
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.02 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 103.44 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2000	fail_ct: 4	Time elapsed: 1.72
GA Iter: 0	Max score: 0.8565	Min score: 0.3271	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9422	Min score: 0.7352	#Pop: 128	#M+: 1377	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 6.73
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 21.41 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 94.13 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 9	Time elapsed: 0.61
GA Iter: 0	Max score: 0.6061	Min score: 0.2756	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7664	Min score: 0.5218	#Pop: 128	#M+: 1379	#M-: 86
EvolutionarySearch		#s: 128	Time elapsed: 2.60
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.17 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 89.40 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1982	fail_ct: 7	Time elapsed: 1.28
GA Iter: 0	Max score: 0.5194	Min score: 0.2743	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8822	Min score: 0.6736	#Pop: 128	#M+: 1391	#M-: 79
EvolutionarySearch		#s: 128	Time elapsed: 3.87
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.57 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 39.46 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 11	Time elapsed: 1.33
GA Iter: 0	Max score: 0.6955	Min score: 0.4246	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8805	Min score: 0.6937	#Pop: 128	#M+: 1388	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.29
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.41 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 68.51 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2624 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3776 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15296	Used time : 14944 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3584 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    576 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2624 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3776 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15360	Used time : 15052 s	Next ID: 5	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3584 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2624 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3776 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15424	Used time : 15181 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3584 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2688 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3776 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15488	Used time : 15305 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3584 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2688 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3840 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15552	Used time : 15414 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3648 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1280 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2688 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3840 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15616	Used time : 15477 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1963	fail_ct: 15	Time elapsed: 0.39
GA Iter: 0	Max score: 0.5699	Min score: 0.2692	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7859	Min score: 0.5217	#Pop: 128	#M+: 1384	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 2.22
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.24 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 73.85 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1992	fail_ct: 5	Time elapsed: 1.42
GA Iter: 0	Max score: 0.7160	Min score: 0.3047	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9548	Min score: 0.7293	#Pop: 128	#M+: 1385	#M-: 69
EvolutionarySearch		#s: 128	Time elapsed: 5.62
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.61 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 77.24 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2007	fail_ct: 7	Time elapsed: 1.58
GA Iter: 0	Max score: 0.6820	Min score: 0.2563	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9244	Min score: 0.6811	#Pop: 128	#M+: 1387	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 6.63
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.55 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 111.60 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 877	fail_ct: 854	Time elapsed: 0.79
GA Iter: 0	Max score: 0.9066	Min score: 0.4954	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9440	Min score: 0.8847	#Pop: 128	#M+: 1378	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 6.17
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.86 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 67.57 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1958	fail_ct: 14	Time elapsed: 0.77
GA Iter: 0	Max score: 0.5469	Min score: 0.2825	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7821	Min score: 0.5248	#Pop: 128	#M+: 1391	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.49
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.46 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 77.88 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3648 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2688 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3840 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15680	Used time : 15568 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3648 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2688 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3904 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15744	Used time : 15659 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3648 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2752 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3904 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15808	Used time : 15761 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3712 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1216 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2752 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3904 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15872	Used time : 15904 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3712 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2752 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3904 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 15936	Used time : 15992 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3712 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2752 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1993	fail_ct: 8	Time elapsed: 1.00
GA Iter: 0	Max score: 0.6360	Min score: 0.2697	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9078	Min score: 0.6901	#Pop: 128	#M+: 1380	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 5.46
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.15 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 81.14 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 896	fail_ct: 796	Time elapsed: 0.64
GA Iter: 0	Max score: 0.9435	Min score: 0.4892	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9558	Min score: 0.8723	#Pop: 128	#M+: 1386	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 6.65
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.65 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 73.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1992	fail_ct: 5	Time elapsed: 1.09
GA Iter: 0	Max score: 0.7231	Min score: 0.3341	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9328	Min score: 0.7288	#Pop: 128	#M+: 1378	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.99
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.58 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 115.53 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1977	fail_ct: 15	Time elapsed: 1.21
GA Iter: 0	Max score: 0.5533	Min score: 0.2605	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7060	Min score: 0.5167	#Pop: 128	#M+: 1381	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 5.68
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.94 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 79.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 9	Time elapsed: 1.47
GA Iter: 0	Max score: 0.6303	Min score: 0.2750	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9105	Min score: 0.6868	#Pop: 128	#M+: 1388	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 5.55
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.70 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 79.56 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 12	Time elapsed: 1.44
GA Iter: 0	Max score: 0.7306	Min score: 0.4112	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8751	Min score: 0.6697	#Pop: 128	#M+: 1390	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 6.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.41 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 118.21 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3968 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16000	Used time : 16089 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3776 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1216 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2752 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3968 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16064	Used time : 16190 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3776 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2752 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3968 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16128	Used time : 16284 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3776 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2816 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   3968 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16192	Used time : 16422 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3776 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2816 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4032 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16256	Used time : 16523 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3840 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1344 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2816 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4032 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16320	Used time : 16625 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3840 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1952	fail_ct: 15	Time elapsed: 1.40
GA Iter: 0	Max score: 0.5721	Min score: 0.2636	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6920	Min score: 0.5124	#Pop: 128	#M+: 1391	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 6.86
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.85 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 103.50 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1999	fail_ct: 6	Time elapsed: 1.08
GA Iter: 0	Max score: 0.7423	Min score: 0.3191	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9176	Min score: 0.7365	#Pop: 128	#M+: 1380	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.83 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1983	fail_ct: 10	Time elapsed: 1.43
GA Iter: 0	Max score: 0.6610	Min score: 0.2740	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9029	Min score: 0.6825	#Pop: 128	#M+: 1372	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.07
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.31 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 97.09 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 865	fail_ct: 851	Time elapsed: 0.68
GA Iter: 0	Max score: 0.9116	Min score: 0.4980	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9329	Min score: 0.8812	#Pop: 128	#M+: 1383	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.03
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.62 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 86.65 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1956	fail_ct: 9	Time elapsed: 1.11
GA Iter: 0	Max score: 0.5655	Min score: 0.2614	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6945	Min score: 0.5290	#Pop: 128	#M+: 1394	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.94
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.73 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.26 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2816 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4032 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16384	Used time : 16775 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3840 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2816 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4096 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16448	Used time : 16911 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3840 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2880 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4096 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16512	Used time : 17006 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3904 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1280 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2880 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4096 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16576	Used time : 17123 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3904 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2880 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4096 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16640	Used time : 17229 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3904 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2880 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4160 |
-----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 10	Time elapsed: 1.52
GA Iter: 0	Max score: 0.6809	Min score: 0.2824	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9197	Min score: 0.7106	#Pop: 128	#M+: 1386	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 5.22
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 20.49 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 97.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1999	fail_ct: 6	Time elapsed: 1.30
GA Iter: 0	Max score: 0.6356	Min score: 0.2970	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9212	Min score: 0.7177	#Pop: 128	#M+: 1382	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 5.73
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 26.71 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 73.04 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 869	fail_ct: 881	Time elapsed: 0.86
GA Iter: 0	Max score: 0.9468	Min score: 0.4928	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9617	Min score: 0.8653	#Pop: 128	#M+: 1364	#M-: 68
EvolutionarySearch		#s: 128	Time elapsed: 5.04
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 71.68 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 286	fail_ct: 1762	Time elapsed: 2.04
GA Iter: 0	Max score: 0.6270	Min score: 0.1790	#Pop: 64	#M+: 0	#M-: 0
[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:11] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:11] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:13] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:13] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:14] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:14] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:15] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:15] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:00:16] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:00:16] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.9474	Min score: 0.1790	#Pop: 83	#M+: 555	#M-: 5774
EvolutionarySearch		#s: 83	Time elapsed: 7.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.67 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 128.30 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1979	fail_ct: 9	Time elapsed: 1.51
GA Iter: 0	Max score: 0.5568	Min score: 0.2681	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7738	Min score: 0.5187	#Pop: 128	#M+: 1377	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 6.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 23.00 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 70.99 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1973	fail_ct: 15	Time elapsed: 1.04
GA Iter: 0	Max score: 0.9911	Min score: 0.8578	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0032	Min score: 0.9470	#Pop: 128	#M+: 1390	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 4.20
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
.............................................................Estimated total latency: 0.817 ms	Trials: 16704	Used time : 17319 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2880 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4160 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16768	Used time : 17444 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1280 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4160 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16832	Used time : 17551 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    640 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4160 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16896	Used time : 17641 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4160 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 16960	Used time : 17793 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.59 |    640 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4224 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17024	Used time : 17895 s	Next ID: 5	
.T.T.T*************************************************************Time elapsed for measurement: 25.91 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 99.87 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1955	fail_ct: 16	Time elapsed: 1.18
GA Iter: 0	Max score: 0.7211	Min score: 0.4176	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8984	Min score: 0.6946	#Pop: 128	#M+: 1382	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 5.70
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.18 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.60 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 7	Time elapsed: 0.77
GA Iter: 0	Max score: 0.6209	Min score: 0.2592	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9108	Min score: 0.6856	#Pop: 128	#M+: 1381	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 2.93
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.04 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 92.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1996	fail_ct: 3	Time elapsed: 1.02
GA Iter: 0	Max score: 0.6765	Min score: 0.3191	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9607	Min score: 0.7278	#Pop: 128	#M+: 1377	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.15
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.45 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 99.53 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1981	fail_ct: 9	Time elapsed: 1.28
GA Iter: 0	Max score: 0.5730	Min score: 0.2662	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7035	Min score: 0.5190	#Pop: 128	#M+: 1395	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 4.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.03 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 80.47 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1986	fail_ct: 5	Time elapsed: 0.71
GA Iter: 0	Max score: 0.5454	Min score: 0.2651	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8975	Min score: 0.6917	#Pop: 128	#M+: 1389	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 2.94
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.49 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 77.38 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1408 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4224 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17088	Used time : 18026 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   3968 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4224 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17152	Used time : 18125 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4032 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   2944 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4224 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17216	Used time : 18233 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4032 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   3008 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4224 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17280	Used time : 18351 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4032 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   3008 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4288 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17344	Used time : 18456 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4096 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 20	Time elapsed: 1.31
GA Iter: 0	Max score: 0.5599	Min score: 0.2527	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6846	Min score: 0.5144	#Pop: 128	#M+: 1378	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.88
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.44 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 90.32 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1994	fail_ct: 10	Time elapsed: 1.55
GA Iter: 0	Max score: 0.7788	Min score: 0.3091	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9275	Min score: 0.7330	#Pop: 128	#M+: 1399	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 6.86
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 31.35 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 67.45 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1978	fail_ct: 5	Time elapsed: 1.45
GA Iter: 0	Max score: 0.6844	Min score: 0.3156	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9585	Min score: 0.7224	#Pop: 128	#M+: 1384	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 5.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 19.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 112.79 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 8	Time elapsed: 1.49
GA Iter: 0	Max score: 0.7935	Min score: 0.2642	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8826	Min score: 0.6847	#Pop: 128	#M+: 1371	#M-: 70
EvolutionarySearch		#s: 128	Time elapsed: 6.89
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.74 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 106.03 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 835	fail_ct: 895	Time elapsed: 1.02
GA Iter: 0	Max score: 0.9009	Min score: 0.4855	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9025	Min score: 0.8677	#Pop: 128	#M+: 1391	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.79
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.43 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 67.62 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 861	fail_ct: 852	Time elapsed: 0.53
GA Iter: 0	Max score: 0.3495	Min score: 0.1390	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0003	Min score: 0.6968	#Pop: 128	#M+: 1378	#M-: 71
EvolutionarySearch		#s: 128	Time elapsed: 3.71
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.79 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 90.51 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   3008 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4288 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17408	Used time : 18550 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4096 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2381.50 |   3008 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.817 ms	Trials: 17472	Used time : 18664 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4096 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3072 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.816 ms	Trials: 17536	Used time : 18771 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4096 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.816 ms	Trials: 17600	Used time : 18911 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.003 |           2.42 |   1344 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.816 ms	Trials: 17664	Used time : 19044 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.001 |           9.86 |   1408 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.768 ms	Trials: 17728	Used time : 19130 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 861	fail_ct: 881	Time elapsed: 0.62
GA Iter: 0	Max score: 0.3139	Min score: 0.1224	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9917	Min score: 0.8368	#Pop: 128	#M+: 1386	#M-: 62
EvolutionarySearch		#s: 128	Time elapsed: 4.65
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.58 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 94.41 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 878	fail_ct: 837	Time elapsed: 0.56
GA Iter: 0	Max score: 0.2536	Min score: 0.0974	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9625	Min score: 0.7792	#Pop: 128	#M+: 1384	#M-: 42
EvolutionarySearch		#s: 128	Time elapsed: 4.32
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.09 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 84.15 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 825	fail_ct: 885	Time elapsed: 0.71
GA Iter: 0	Max score: 0.9612	Min score: 0.5559	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0717	Min score: 0.8561	#Pop: 128	#M+: 1375	#M-: 84
EvolutionarySearch		#s: 128	Time elapsed: 4.41
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.22 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 85.24 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 862	fail_ct: 865	Time elapsed: 0.53
GA Iter: 0	Max score: 0.1576	Min score: 0.0902	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9974	Min score: 0.6448	#Pop: 128	#M+: 1390	#M-: 54
EvolutionarySearch		#s: 128	Time elapsed: 2.11
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 10.57 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 43.65 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 887	fail_ct: 857	Time elapsed: 0.65
GA Iter: 0	Max score: 0.1480	Min score: 0.0959	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9787	Min score: 0.6378	#Pop: 128	#M+: 1381	#M-: 59
EvolutionarySearch		#s: 128	Time elapsed: 4.26
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 12.90 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 76.83 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          12.31 |   1472 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.765 ms	Trials: 17792	Used time : 19236 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1536 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.761 ms	Trials: 17856	Used time : 19348 s	Next ID: 2	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.003 |           2.43 |   1344 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.761 ms	Trials: 17920	Used time : 19450 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          18.83 |   1408 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.706 ms	Trials: 17984	Used time : 19553 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          18.85 |   1472 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.706 ms	Trials: 18048	Used time : 19610 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          18.87 |   1536 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 862	fail_ct: 858	Time elapsed: 0.80
GA Iter: 0	Max score: 0.1676	Min score: 0.0892	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9751	Min score: 0.5431	#Pop: 128	#M+: 1373	#M-: 51
EvolutionarySearch		#s: 128	Time elapsed: 5.22
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.56 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 85.74 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1962	fail_ct: 11	Time elapsed: 0.41
GA Iter: 0	Max score: 0.5945	Min score: 0.2724	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7219	Min score: 0.5206	#Pop: 128	#M+: 1389	#M-: 84
EvolutionarySearch		#s: 128	Time elapsed: 2.84
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.24 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 71.04 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 2001	fail_ct: 5	Time elapsed: 1.20
GA Iter: 0	Max score: 0.7506	Min score: 0.3154	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9248	Min score: 0.7227	#Pop: 128	#M+: 1388	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 4.08
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................********************************Time elapsed for measurement: 14.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 28.96 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1975	fail_ct: 11	Time elapsed: 0.39
GA Iter: 0	Max score: 0.7863	Min score: 0.4191	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9626	Min score: 0.7234	#Pop: 128	#M+: 1390	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 2.28
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 9.75 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 63.48 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1983	fail_ct: 8	Time elapsed: 1.45
GA Iter: 0	Max score: 0.6094	Min score: 0.2789	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8998	Min score: 0.6899	#Pop: 128	#M+: 1378	#M-: 80
EvolutionarySearch		#s: 128	Time elapsed: 4.47
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.25 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 81.91 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1976	fail_ct: 17	Time elapsed: 1.16
GA Iter: 0	Max score: 0.5617	Min score: 0.2787	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7770	Min score: 0.5176	#Pop: 128	#M+: 1382	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.01
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.30 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 75.86 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.706 ms	Trials: 18112	Used time : 19705 s	Next ID: 3	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4352 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18176	Used time : 19808 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3136 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4416 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18240	Used time : 19896 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1645.14 |   1472 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3200 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4416 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18304	Used time : 19945 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4160 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3200 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4416 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18368	Used time : 20021 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4224 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3200 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4416 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18432	Used time : 20121 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4224 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1985	fail_ct: 8	Time elapsed: 0.90
GA Iter: 0	Max score: 0.6968	Min score: 0.3101	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9622	Min score: 0.7206	#Pop: 128	#M+: 1372	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 3.33
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................*************************************************Time elapsed for measurement: 18.87 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 70.36 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 6	Time elapsed: 0.76
GA Iter: 0	Max score: 0.5716	Min score: 0.2837	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9064	Min score: 0.6712	#Pop: 128	#M+: 1379	#M-: 82
EvolutionarySearch		#s: 128	Time elapsed: 3.22
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 22.10 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 82.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 13	Time elapsed: 0.81
GA Iter: 0	Max score: 0.5688	Min score: 0.2795	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6473	Min score: 0.5086	#Pop: 128	#M+: 1382	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 4.24
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.55 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 76.94 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1978	fail_ct: 7	Time elapsed: 0.76
GA Iter: 0	Max score: 0.5933	Min score: 0.2753	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8816	Min score: 0.6646	#Pop: 128	#M+: 1372	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 3.94
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 11.65 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.43 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1968	fail_ct: 13	Time elapsed: 1.00
GA Iter: 0	Max score: 0.8393	Min score: 0.4057	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9549	Min score: 0.7293	#Pop: 128	#M+: 1389	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.98
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.15 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 49.22 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1976	fail_ct: 12	Time elapsed: 0.98
GA Iter: 0	Max score: 0.5315	Min score: 0.2864	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6362	Min score: 0.5224	#Pop: 128	#M+: 1382	#M-: 77
EvolutionarySearch		#s: 128	Time elapsed: 4.18
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 15.60 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 79.48 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3200 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4480 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18496	Used time : 20220 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4224 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4480 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18560	Used time : 20314 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4288 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4480 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18624	Used time : 20423 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4288 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4544 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18688	Used time : 20521 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4352 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1536 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4544 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18752	Used time : 20606 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4352 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4544 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18816	Used time : 20678 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 284	fail_ct: 1764	Time elapsed: 1.42
GA Iter: 0	Max score: 0.3199	Min score: 0.1636	#Pop: 18	#M+: 0	#M-: 0
[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:28] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:28] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:29] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:29] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 512
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 64
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:30] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:30] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 16
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  T_softmax_expsum auto_unroll: 512
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_maxelem auto_unroll: 16
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (None)
  T_softmax_expsum auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (None)
    for i0 (None)
      T_softmax_maxelem auto_unroll: 512
      for i0 (None)
        for k (None)
          T_softmax_maxelem = ...
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0 (None)
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  T_softmax_maxelem auto_unroll: 16
  for i0 (None)
    for k (None)
      T_softmax_maxelem = ...
  for i1 (None)
    for i0 (None)
      vectorize i1 (None)
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



[03:52:31] /home/ubuntu/tvm/src/auto_scheduler/compute_dag.cc:1375: Warning: InferBound fails on the state:
Placeholder: placeholder
parallel i0@ (0,96)
  T_softmax_expsum auto_unroll: 64
  for i0 (None)
    for k (None)
      T_softmax_expsum = ...
  for i1 (0,8)
    for i0 (None)
      for i1 (None)
        T_softmax_maxelem auto_unroll: 64
        for i0 (None)
          for k (None)
            T_softmax_maxelem = ...
        T_softmax_exp = ...
    T_softmax_norm = ...

with: [03:52:31] /home/ubuntu/tvm/src/te/schedule/bound.cc:175: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (found_attach || stage_attach.size() == 0) is false: Invalid Schedule, cannot find the producer compute(T_softmax_exp, body=[tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))], axis=[iter_var(i0, range(min=0, ext=96)), iter_var(i1, range(min=0, ext=8))], reduce_axis=[], tag=softmax_output, attrs={}) along the loop nest specified by compute_at of consumer compute(T_softmax_expsum, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_softmax_exp[i0, k]], init=[], axis=[iter_var(k, range(min=0, ext=8))], where=(bool)1, value_index=0)], axis=[iter_var(i0, range(min=0, ext=96))], reduce_axis=[iter_var(k, range(min=0, ext=8))], tag=softmax_output, attrs={})
Stack trace:
  0: tvm::te::InferRootBound(tvm::te::Stage const&, tvm::te::GraphContext const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*)
  1: tvm::te::InferBound(tvm::te::Schedule const&)
  2: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::auto_scheduler::State const&) const
  3: tvm::auto_scheduler::ComputeDAG::InferBound(tvm::runtime::Array<tvm::auto_scheduler::State, void> const&) const::{lambda(int)#1}::operator()(int) const
  4: _ZNSt17_Function_handlerIFSt10unique_ptrINSt13__future_base12_Result_baseENS2_8_DeleterEEvENS1_12_Task_setterIS0_INS1_7_ResultIvEES3_EZNS1_11_Task_stateIZN3tvm7support12parallel_for
  5: std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
  6: __pthread_once_slow
  7: std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::packaged_task<void (std::vector<int, std::allocator<int> > const&, std::function<void (int)> const&)>, std::vector<int, std::allocator<int> >, std::function<void (int)> > > >::_M_run()
  8: 0x00007fa5104596b3
  9: start_thread
  10: __clone
  11: 0xffffffffffffffff



GA Iter: 4	Max score: 0.6477	Min score: 0.1636	#Pop: 29	#M+: 548	#M-: 5635
EvolutionarySearch		#s: 29	Time elapsed: 5.98
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 29 programs to measure:
.............................**********************Time elapsed for measurement: 7.08 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 95.73 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1991	fail_ct: 11	Time elapsed: 1.12
GA Iter: 0	Max score: 0.9593	Min score: 0.8508	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9808	Min score: 0.9457	#Pop: 128	#M+: 1390	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.96
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
...............................................................-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    704 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4352 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4608 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18880	Used time : 20778 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4352 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    704 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4608 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18909	Used time : 20889 s	Next ID: 5	
.T***************************************************************Time elapsed for measurement: 26.36 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 89.77 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1983	fail_ct: 10	Time elapsed: 0.76
GA Iter: 0	Max score: 0.6810	Min score: 0.2851	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8810	Min score: 0.6646	#Pop: 128	#M+: 1382	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 3.42
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................*************************************************Time elapsed for measurement: 10.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 75.77 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1977	fail_ct: 14	Time elapsed: 1.26
GA Iter: 0	Max score: 0.5599	Min score: 0.2889	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7660	Min score: 0.5195	#Pop: 128	#M+: 1378	#M-: 78
EvolutionarySearch		#s: 128	Time elapsed: 4.76
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 16.33 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.08 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1995	fail_ct: 9	Time elapsed: 1.30
GA Iter: 0	Max score: 0.6042	Min score: 0.2728	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9000	Min score: 0.6625	#Pop: 128	#M+: 1370	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 5.58
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 73.70 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1969	fail_ct: 19	Time elapsed: 0.90
GA Iter: 0	Max score: 0.5956	Min score: 0.2722	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7839	Min score: 0.5125	#Pop: 128	#M+: 1381	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 4.25
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.65 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 61.67 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1987	fail_ct: 2	Time elapsed: 0.46
GA Iter: 0	Max score: 0.6711	Min score: 0.3099	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9700	Min score: 0.7267	#Pop: 128	#M+: 1386	#M-: 85
EvolutionarySearch		#s: 128	Time elapsed: 2.39
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................************************************************Time elapsed for measurement: 15.90 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 49.78 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4352 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4608 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 18973	Used time : 21011 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4416 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4608 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19037	Used time : 21102 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4416 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4672 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19101	Used time : 21193 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4480 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4672 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19165	Used time : 21289 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4480 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3264 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4736 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19229	Used time : 21373 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4480 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1600 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1970	fail_ct: 20	Time elapsed: 0.69
GA Iter: 0	Max score: 0.7487	Min score: 0.4399	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9493	Min score: 0.7468	#Pop: 128	#M+: 1380	#M-: 76
EvolutionarySearch		#s: 128	Time elapsed: 2.83
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.59 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 92.37 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1967	fail_ct: 17	Time elapsed: 0.82
GA Iter: 0	Max score: 0.5886	Min score: 0.2727	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.6401	Min score: 0.5035	#Pop: 128	#M+: 1390	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 3.85
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.85 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 86.39 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1994	fail_ct: 4	Time elapsed: 0.85
GA Iter: 0	Max score: 0.6190	Min score: 0.2844	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8562	Min score: 0.6591	#Pop: 128	#M+: 1384	#M-: 67
EvolutionarySearch		#s: 128	Time elapsed: 3.87
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.63 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 68.84 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1964	fail_ct: 11	Time elapsed: 1.14
GA Iter: 0	Max score: 0.6056	Min score: 0.2839	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7649	Min score: 0.5030	#Pop: 128	#M+: 1380	#M-: 72
EvolutionarySearch		#s: 128	Time elapsed: 3.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.67 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 89.51 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 9	Time elapsed: 1.60
GA Iter: 0	Max score: 0.8061	Min score: 0.2730	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8688	Min score: 0.6634	#Pop: 128	#M+: 1391	#M-: 75
EvolutionarySearch		#s: 128	Time elapsed: 3.04
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.54 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 83.57 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1976	fail_ct: 13	Time elapsed: 0.62
GA Iter: 0	Max score: 0.6667	Min score: 0.2995	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9841	Min score: 0.7133	#Pop: 128	#M+: 1380	#M-: 83
EvolutionarySearch		#s: 128	Time elapsed: 3.59
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
...............................................................|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3328 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4736 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19293	Used time : 21442 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4480 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3328 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4736 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19357	Used time : 21550 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4480 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3328 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4800 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19421	Used time : 21659 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4544 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3328 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4800 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19485	Used time : 21748 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4544 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3328 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4864 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19549	Used time : 21857 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4608 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2404.90 |   3328 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4864 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19613	Used time : 21960 s	Next ID: 6	
.T***************************************************************Time elapsed for measurement: 26.98 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 67.91 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1966	fail_ct: 14	Time elapsed: 0.82
GA Iter: 0	Max score: 0.5267	Min score: 0.2762	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7012	Min score: 0.5058	#Pop: 128	#M+: 1366	#M-: 81
EvolutionarySearch		#s: 128	Time elapsed: 3.66
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 12.77 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 72.77 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1990	fail_ct: 10	Time elapsed: 1.27
GA Iter: 0	Max score: 0.7113	Min score: 0.2909	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9910	Min score: 0.7241	#Pop: 128	#M+: 1372	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.80
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.74 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 81.77 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1974	fail_ct: 10	Time elapsed: 0.79
GA Iter: 0	Max score: 0.7025	Min score: 0.2734	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.8498	Min score: 0.6697	#Pop: 128	#M+: 1380	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 4.78
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 14.94 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 59.93 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1947	fail_ct: 17	Time elapsed: 0.83
GA Iter: 0	Max score: 0.6438	Min score: 0.4915	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.1292	Min score: 0.8168	#Pop: 128	#M+: 1388	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 3.47
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 17.38 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 21.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1957	fail_ct: 16	Time elapsed: 0.81
GA Iter: 0	Max score: 0.7393	Min score: 0.3952	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9635	Min score: 0.7312	#Pop: 128	#M+: 1377	#M-: 74
EvolutionarySearch		#s: 128	Time elapsed: 3.44
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 13.45 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 96.97 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4608 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2411.71 |   3392 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4864 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19677	Used time : 22059 s	Next ID: 8	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4608 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2411.71 |   3392 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4928 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19741	Used time : 22149 s	Next ID: 6	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4608 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2411.71 |   3456 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4928 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19805	Used time : 22256 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4672 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2411.71 |   3456 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    448 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4928 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19869	Used time : 22337 s	Next ID: 7	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4672 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1664 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2411.71 |   3456 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    512 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4928 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19933	Used time : 22380 s	Next ID: 4	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                       vm_mod_fused_nn_softmax |        0.003 |           1.16 |    768 |
|    1 |                                   vm_mod_fused_nn_dense_add_2 |        0.016 |        2341.26 |   4672 |
|    2 |                                       vm_mod_fused_power_mean |        0.000 |          17.92 |   1600 |
|    3 |                                             vm_mod_fused_mean |        0.000 |          19.35 |   1600 |
|    4 |                                   vm_mod_fused_nn_dense_add_1 |        0.006 |        1653.81 |   1728 |
|    5 |                                vm_mod_fused_nn_batch_matmul_1 |        0.003 |          37.62 |    768 |
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 1958	fail_ct: 14	Time elapsed: 1.14
GA Iter: 0	Max score: 0.5542	Min score: 0.2515	#Pop: 128	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.7073	Min score: 0.5051	#Pop: 128	#M+: 1394	#M-: 73
EvolutionarySearch		#s: 128	Time elapsed: 5.79
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************Time elapsed for measurement: 18.07 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 69.56 s
|    6 |                                     vm_mod_fused_nn_dense_add |        0.012 |        2411.71 |   3456 |
|    7 |                                  vm_mod_fused_nn_batch_matmul |        0.001 |          67.95 |    512 |
|    8 |                                   vm_mod_fused_nn_dense_add_3 |        0.017 |        2213.09 |   4928 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.705 ms	Trials: 19997	Used time : 22495 s	Next ID: 8	

==== Task 0: vm_mod_fused_nn_softmax (weight 12 key: ["d7b65649a4dd54becea0a52aabbc5af5", [96, 8], [96, 8]]) =====
placeholder = PLACEHOLDER [96, 8]
T_softmax_maxelem(i0) max= placeholder[i0, k]
T_softmax_exp(i0, i1) = tir.exp((placeholder[i0, i1] - T_softmax_maxelem[i0]))
T_softmax_expsum(i0) += T_softmax_exp[i0, k]
T_softmax_norm(i0, i1) = (T_softmax_exp[i0, i1]/T_softmax_expsum[i0])


Trace for this task is: 
T_softmax_maxelem_i0, T_softmax_maxelem_k = tuple(T_softmax_maxelem.op.axis) + tuple(T_softmax_maxelem.op.reduce_axis)
T_softmax_exp_i0, T_softmax_exp_i1 = tuple(T_softmax_exp.op.axis) + tuple(T_softmax_exp.op.reduce_axis)
T_softmax_expsum_i0, T_softmax_expsum_k = tuple(T_softmax_expsum.op.axis) + tuple(T_softmax_expsum.op.reduce_axis)
T_softmax_norm_i0, T_softmax_norm_i1 = tuple(T_softmax_norm.op.axis) + tuple(T_softmax_norm.op.reduce_axis)
s[T_softmax_expsum].compute_at(s[T_softmax_norm], T_softmax_norm_i0)
s[T_softmax_exp].compute_at(s[T_softmax_norm], T_softmax_norm_i0)
s[T_softmax_maxelem].compute_at(s[T_softmax_exp], T_softmax_exp_i1)
s[T_softmax_norm].parallel(T_softmax_norm_i0)
s[T_softmax_maxelem].pragma(T_softmax_maxelem_i0, "auto_unroll_max_step", 64)
s[T_softmax_maxelem].pragma(T_softmax_maxelem_i0, "unroll_explicit", True)
s[T_softmax_expsum].pragma(T_softmax_expsum_i0, "auto_unroll_max_step", 64)
s[T_softmax_expsum].pragma(T_softmax_expsum_i0, "unroll_explicit", True)
s[T_softmax_norm].vectorize(T_softmax_norm_i1)


The best replacement found is:
@main = primfn(placeholder_1: handle, T_softmax_norm_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_2: Pointer(float32), float32, [768], []),
             T_softmax_norm: Buffer(T_softmax_norm_2: Pointer(float32), float32, [768], [])}
  buffer_map = {placeholder_1: placeholder, T_softmax_norm_1: T_softmax_norm}
  preflattened_buffer_map = {placeholder_1: placeholder_3: Buffer(placeholder_2, float32, [96, 8], []), T_softmax_norm_1: T_softmax_norm_3: Buffer(T_softmax_norm_2, float32, [96, 8], [])} {
  for (i0: int32, 0, 96) "parallel" {
    allocate(T_softmax_exp: Pointer(global float32), float32, [8]), storage_scope = global;
    allocate(T_softmax_maxelem: Pointer(global float32), float32, [1]), storage_scope = global;
    allocate(T_softmax_expsum: Pointer(global float32), float32, [1]), storage_scope = global {
      for (i1: int32, 0, 8) {
        let cse_var_1: int32 = (i0*8)
         {
          T_softmax_maxelem_1: Buffer(T_softmax_maxelem, float32, [1], [], align=4)[0] = -3.40282e+38f32
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[cse_var_1])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 1)])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 2)])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 3)])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 4)])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 5)])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 6)])
          T_softmax_maxelem_1[0] = max(T_softmax_maxelem_1[0], placeholder[(cse_var_1 + 7)])
          T_softmax_exp_1: Buffer(T_softmax_exp, float32, [8], [], align=32)[i1] = @tir.exp((placeholder[(cse_var_1 + i1)] - T_softmax_maxelem_1[0]), dtype=float32)
        }
      }
      T_softmax_expsum_1: Buffer(T_softmax_expsum, float32, [1], [], align=4)[0] = 0f32
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[0])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[1])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[2])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[3])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[4])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[5])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[6])
      T_softmax_expsum_1[0] = (T_softmax_expsum_1[0] + T_softmax_exp_1[7])
      T_softmax_norm[ramp((i0*8), 1, 8)] = (T_softmax_exp_1[ramp(0, 1, 8)] / broadcast(T_softmax_expsum_1[0], 8))
    }
  }
}


==== Task 1: vm_mod_fused_nn_dense_add_2 (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 768], [3072, 768], [1, 3072], [8, 3072]]) =====
placeholder = PLACEHOLDER [8, 768]
placeholder = PLACEHOLDER [3072, 768]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 3072]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])


Trace for this task is: 
T_matmul_NT_i, T_matmul_NT_j, T_matmul_NT_k = tuple(T_matmul_NT.op.axis) + tuple(T_matmul_NT.op.reduce_axis)
T_add_ax0, T_add_ax1 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
T_matmul_NT_i_o_i, T_matmul_NT_i_i = s[T_matmul_NT].split(T_matmul_NT_i, factor=2)
T_matmul_NT_i_o_o_i, T_matmul_NT_i_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_i, factor=4)
T_matmul_NT_i_o_o_o, T_matmul_NT_i_o_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_o_i, factor=1)
T_matmul_NT_j_o_i, T_matmul_NT_j_i = s[T_matmul_NT].split(T_matmul_NT_j, factor=16)
T_matmul_NT_j_o_o_i, T_matmul_NT_j_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_i, factor=3)
T_matmul_NT_j_o_o_o, T_matmul_NT_j_o_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_o_i, factor=2)
T_matmul_NT_k_o, T_matmul_NT_k_i = s[T_matmul_NT].split(T_matmul_NT_k, factor=2)
s[T_matmul_NT].reorder(T_matmul_NT_i_o_o_o, T_matmul_NT_j_o_o_o, T_matmul_NT_i_o_o_i, T_matmul_NT_j_o_o_i, T_matmul_NT_k_o, T_matmul_NT_i_o_i, T_matmul_NT_j_o_i, T_matmul_NT_k_i, T_matmul_NT_i_i, T_matmul_NT_j_i)
T_add_ax0_o_i, T_add_ax0_i = s[T_add].split(T_add_ax0, factor=8)
T_add_ax0_o_o, T_add_ax0_o_i = s[T_add].split(T_add_ax0_o_i, factor=1)
T_add_ax1_o_i, T_add_ax1_i = s[T_add].split(T_add_ax1, factor=48)
T_add_ax1_o_o, T_add_ax1_o_i = s[T_add].split(T_add_ax1_o_i, factor=2)
s[T_add].reorder(T_add_ax0_o_o, T_add_ax1_o_o, T_add_ax0_o_i, T_add_ax1_o_i, T_add_ax0_i, T_add_ax1_i)
s[T_matmul_NT].compute_at(s[T_add], T_add_ax1_o_i)
T_add_ax0_o_o_ax1_o_o_fused_ax0_o_i_fused = s[T_add].fuse(T_add_ax0_o_o, T_add_ax1_o_o, T_add_ax0_o_i)
s[T_add].parallel(T_add_ax0_o_o_ax1_o_o_fused_ax0_o_i_fused)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "auto_unroll_max_step", 16)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "unroll_explicit", True)
s[T_matmul_NT].vectorize(T_matmul_NT_j_i)


The best replacement found is:
@main = primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_6: Pointer(float32), float32, [6144], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [2359296], []),
             placeholder_2: Buffer(placeholder_8: Pointer(float32), float32, [3072], []),
             T_add: Buffer(T_add_2: Pointer(float32), float32, [24576], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add}
  preflattened_buffer_map = {placeholder_3: placeholder_9: Buffer(placeholder_6, float32, [8, 768], []), placeholder_4: placeholder_10: Buffer(placeholder_7, float32, [32, 2, 1, 1, 384, 3, 2, 16], []), placeholder_5: placeholder_11: Buffer(placeholder_8, float32, [1, 3072], []), T_add_1: T_add_3: Buffer(T_add_2, float32, [8, 3072], [])} {
  for (ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused: int32, 0, 32) "parallel" {
    allocate(T_matmul_NT: Pointer(global float32), float32, [384]), storage_scope = global;
    for (ax1.outer.inner: int32, 0, 2) {
      for (i.outer.inner.init: int32, 0, 4) {
        let cse_var_1: int32 = (i.outer.inner.init*96)
         {
          T_matmul_NT_1: Buffer(T_matmul_NT, float32, [384], [])[ramp(cse_var_1, 1, 16)] = broadcast(0f32, 16)
          T_matmul_NT_1[ramp((cse_var_1 + 48), 1, 16)] = broadcast(0f32, 16)
          T_matmul_NT_1[ramp((cse_var_1 + 16), 1, 16)] = broadcast(0f32, 16)
          T_matmul_NT_1[ramp((cse_var_1 + 64), 1, 16)] = broadcast(0f32, 16)
          T_matmul_NT_1[ramp((cse_var_1 + 32), 1, 16)] = broadcast(0f32, 16)
          T_matmul_NT_1[ramp((cse_var_1 + 80), 1, 16)] = broadcast(0f32, 16)
        }
      }
      for (k.outer: int32, 0, 384) {
        for (i.outer.inner: int32, 0, 4) {
          let cse_var_17: int32 = (i.outer.inner*96)
          let cse_var_16: int32 = (cse_var_17 + 80)
          let cse_var_15: int32 = (cse_var_17 + 64)
          let cse_var_14: int32 = (cse_var_17 + 48)
          let cse_var_13: int32 = (cse_var_17 + 32)
          let cse_var_12: int32 = (cse_var_17 + 16)
          let cse_var_11: int32 = ((i.outer.inner*1536) + (k.outer*2))
          let cse_var_10: int32 = (cse_var_11 + 769)
          let cse_var_9: int32 = (cse_var_11 + 768)
          let cse_var_8: int32 = (cse_var_11 + 1)
          let cse_var_7: int32 = (((ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused*73728) + (ax1.outer.inner*36864)) + (k.outer*96))
          let cse_var_6: int32 = (cse_var_7 + 80)
          let cse_var_5: int32 = (cse_var_7 + 64)
          let cse_var_4: int32 = (cse_var_7 + 48)
          let cse_var_3: int32 = (cse_var_7 + 32)
          let cse_var_2: int32 = (cse_var_7 + 16)
           {
            T_matmul_NT_1[ramp(cse_var_17, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_17, 1, 16)] + (broadcast(placeholder[cse_var_11], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_14, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_14, 1, 16)] + (broadcast(placeholder[cse_var_9], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_17, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_17, 1, 16)] + (broadcast(placeholder[cse_var_8], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_14, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_14, 1, 16)] + (broadcast(placeholder[cse_var_10], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_12, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_12, 1, 16)] + (broadcast(placeholder[cse_var_11], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_15, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_15, 1, 16)] + (broadcast(placeholder[cse_var_9], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_12, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_12, 1, 16)] + (broadcast(placeholder[cse_var_8], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_15, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_15, 1, 16)] + (broadcast(placeholder[cse_var_10], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_13, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_13, 1, 16)] + (broadcast(placeholder[cse_var_11], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_16, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_16, 1, 16)] + (broadcast(placeholder[cse_var_9], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_13, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_13, 1, 16)] + (broadcast(placeholder[cse_var_8], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
            T_matmul_NT_1[ramp(cse_var_16, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_16, 1, 16)] + (broadcast(placeholder[cse_var_10], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          }
        }
      }
      for (ax0.inner: int32, 0, 8) {
        for (ax1.inner: int32, 0, 48) {
          let cse_var_19: int32 = (ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused*96)
          let cse_var_18: int32 = (ax1.outer.inner*48)
          T_add[((((ax0.inner*3072) + cse_var_19) + cse_var_18) + ax1.inner)] = (T_matmul_NT_1[((ax0.inner*48) + ax1.inner)] + placeholder_2[((cse_var_19 + cse_var_18) + ax1.inner)])
        }
      }
    }
  }
}


==== Task 2: vm_mod_fused_power_mean (weight 25 key: ["db3f35d71c2b463d338eb14acd963ed2", [1, 8, 768], [1, 8, 1]]) =====
placeholder = PLACEHOLDER [1, 8, 768]
compile_engine_const() = 2f
T_power(ax0, ax1, ax2) = tir.pow(placeholder[ax0, ax1, ax2], compile_engine_const[])
T_power_red(ax0, ax1, ax2) += T_power[ax0, ax1, k2]
T_divide(ax0, ax1, ax2) = (T_power_red[ax0, ax1, ax2]/768f)


Trace for this task is: 
 = tuple(compile_engine_const.op.axis) + tuple(compile_engine_const.op.reduce_axis)
T_power_ax0, T_power_ax1, T_power_ax2 = tuple(T_power.op.axis) + tuple(T_power.op.reduce_axis)
T_power_red_ax0, T_power_red_ax1, T_power_red_ax2, T_power_red_k2 = tuple(T_power_red.op.axis) + tuple(T_power_red.op.reduce_axis)
T_divide_ax0, T_divide_ax1, T_divide_ax2 = tuple(T_divide.op.axis) + tuple(T_divide.op.reduce_axis)
T_power_red_k2_o, T_power_red_k2_i = s[T_power_red].split(T_power_red_k2, factor=32)
T_power_red_rf = s.rfactor(T_power_red, k2_i, 3)
T_power_red_rf_ax0, T_power_red_rf_ax1, T_power_red_rf_ax2, T_power_red_rf_k2_i, T_power_red_rf_k2_o = tuple(T_power_red_rf.op.axis) + tuple(T_power_red_rf.op.reduce_axis)
T_power_red_repl_ax0, T_power_red_repl_ax1, T_power_red_repl_ax2, T_power_red_repl_k2_i_v = tuple(s[T_power_red_repl].op.axis) + tuple(s[T_power_red_repl].op.reduce_axis)
s[T_power_red_rf].reorder(T_power_red_rf_ax0, T_power_red_rf_ax1, T_power_red_rf_ax2, T_power_red_rf_k2_o, T_power_red_rf_k2_i)
s[T_power].compute_inline()
s[compile_engine_const].compute_inline()
s[T_divide].compute_root()
s[T_power_red_repl].compute_at(s[T_divide], T_divide_ax2)
s[T_power_red_rf].compute_at(s[T_power_red_repl], T_power_red_repl_ax1)
T_divide_ax0 = s[T_divide].fuse(T_divide_ax0)
s[T_divide].parallel(T_divide_ax0)
s[T_power_red_rf].pragma(T_power_red_rf_ax0, "auto_unroll_max_step", 0)
s[T_power_red_rf].pragma(T_power_red_rf_ax0, "unroll_explicit", True)
s[T_power_red_repl].pragma(T_power_red_repl_ax0, "auto_unroll_max_step", 0)
s[T_power_red_repl].pragma(T_power_red_repl_ax0, "unroll_explicit", True)
s[T_power_red_rf].vectorize(T_power_red_rf_k2_i)


The best replacement found is:
@main = primfn(placeholder_1: handle, T_divide_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_2: Pointer(float32), float32, [6144], []),
             T_divide: Buffer(T_divide_2: Pointer(float32), float32, [8], [])}
  buffer_map = {placeholder_1: placeholder, T_divide_1: T_divide}
  preflattened_buffer_map = {placeholder_1: placeholder_3: Buffer(placeholder_2, float32, [1, 8, 768], []), T_divide_1: T_divide_3: Buffer(T_divide_2, float32, [1, 8, 1], [])} {
  allocate(T_power_red.rf: Pointer(global float32), float32, [32]), storage_scope = global;
  allocate(T_power_red: Pointer(global float32), float32, [1]), storage_scope = global;
  for (ax1: int32, 0, 8) {
    T_power_red.rf_1: Buffer(T_power_red.rf, float32, [32], [])[ramp(0, 1, 32)] = broadcast(0f32, 32)
    for (k2.outer: int32, 0, 24) {
      T_power_red.rf_1[ramp(0, 1, 32)] = (T_power_red.rf_1[ramp(0, 1, 32)] + @tir.pow(placeholder[ramp(((ax1*768) + (k2.outer*32)), 1, 32)], broadcast(2f32, 32), dtype=float32x32))
    }
    T_power_red_1: Buffer(T_power_red, float32, [1], [], align=4)[0] = 0f32
    for (k2.inner.v: int32, 0, 32) {
      T_power_red_1[0] = (T_power_red_1[0] + T_power_red.rf_1[k2.inner.v])
    }
    T_divide[ax1] = (T_power_red_1[0]*0.00130208f32)
  }
}


==== Task 3: vm_mod_fused_mean (weight 25 key: ["2dde9ffcbf97381c0f0307643e09dac5", [1, 8, 768], [1, 8, 1]]) =====
placeholder = PLACEHOLDER [1, 8, 768]
placeholder_red(ax0, ax1, ax2) += placeholder[ax0, ax1, k2]
T_divide(ax0, ax1, ax2) = (placeholder_red[ax0, ax1, ax2]/768f)


Trace for this task is: 
placeholder_red_ax0, placeholder_red_ax1, placeholder_red_ax2, placeholder_red_k2 = tuple(placeholder_red.op.axis) + tuple(placeholder_red.op.reduce_axis)
T_divide_ax0, T_divide_ax1, T_divide_ax2 = tuple(T_divide.op.axis) + tuple(T_divide.op.reduce_axis)
placeholder_red_k2_o, placeholder_red_k2_i = s[placeholder_red].split(placeholder_red_k2, factor=32)
placeholder_red_rf = s.rfactor(placeholder_red, k2_i, 3)
placeholder_red_rf_ax0, placeholder_red_rf_ax1, placeholder_red_rf_ax2, placeholder_red_rf_k2_i, placeholder_red_rf_k2_o = tuple(placeholder_red_rf.op.axis) + tuple(placeholder_red_rf.op.reduce_axis)
placeholder_red_repl_ax0, placeholder_red_repl_ax1, placeholder_red_repl_ax2, placeholder_red_repl_k2_i_v = tuple(s[placeholder_red_repl].op.axis) + tuple(s[placeholder_red_repl].op.reduce_axis)
s[placeholder_red_rf].reorder(placeholder_red_rf_ax0, placeholder_red_rf_ax1, placeholder_red_rf_ax2, placeholder_red_rf_k2_o, placeholder_red_rf_k2_i)
s[T_divide].compute_root()
s[placeholder_red_repl].compute_at(s[T_divide], T_divide_ax1)
s[placeholder_red_rf].compute_at(s[placeholder_red_repl], placeholder_red_repl_ax0)
T_divide_ax0 = s[T_divide].fuse(T_divide_ax0)
s[T_divide].parallel(T_divide_ax0)
s[placeholder_red_rf].pragma(placeholder_red_rf_ax0, "auto_unroll_max_step", 512)
s[placeholder_red_rf].pragma(placeholder_red_rf_ax0, "unroll_explicit", True)
s[placeholder_red_repl].pragma(placeholder_red_repl_ax0, "auto_unroll_max_step", 0)
s[placeholder_red_repl].pragma(placeholder_red_repl_ax0, "unroll_explicit", True)
s[placeholder_red_rf].vectorize(placeholder_red_rf_k2_i)
s[T_divide].vectorize(T_divide_ax2)


The best replacement found is:
@main = primfn(placeholder_1: handle, T_divide_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_2: Pointer(float32), float32, [6144], []),
             T_divide: Buffer(T_divide_2: Pointer(float32), float32, [8], [])}
  buffer_map = {placeholder_1: placeholder, T_divide_1: T_divide}
  preflattened_buffer_map = {placeholder_1: placeholder_3: Buffer(placeholder_2, float32, [1, 8, 768], []), T_divide_1: T_divide_3: Buffer(T_divide_2, float32, [1, 8, 1], [])} {
  allocate(placeholder_red.rf: Pointer(global float32), float32, [32]), storage_scope = global;
  allocate(placeholder_red: Pointer(global float32), float32, [1]), storage_scope = global;
  for (ax1: int32, 0, 8) {
    let cse_var_1: int32 = (ax1*768)
     {
      placeholder_red.rf_1: Buffer(placeholder_red.rf, float32, [32], [])[ramp(0, 1, 32)] = broadcast(0f32, 32)
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp(cse_var_1, 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 32), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 64), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 96), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 128), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 160), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 192), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 224), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 256), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 288), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 320), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 352), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 384), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 416), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 448), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 480), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 512), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 544), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 576), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 608), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 640), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 672), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 704), 1, 32)])
      placeholder_red.rf_1[ramp(0, 1, 32)] = (placeholder_red.rf_1[ramp(0, 1, 32)] + placeholder[ramp((cse_var_1 + 736), 1, 32)])
      placeholder_red_1: Buffer(placeholder_red, float32, [1], [], align=4)[0] = 0f32
      for (k2.inner.v: int32, 0, 32) {
        placeholder_red_1[0] = (placeholder_red_1[0] + placeholder_red.rf_1[k2.inner.v])
      }
      T_divide[ax1] = (placeholder_red_1[0]*0.00130208f32)
    }
  }
}


==== Task 4: vm_mod_fused_nn_dense_add_1 (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 768], [768, 768], [1, 768], [8, 768]]) =====
placeholder = PLACEHOLDER [8, 768]
placeholder = PLACEHOLDER [768, 768]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 768]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])


Trace for this task is: 
T_matmul_NT_i, T_matmul_NT_j, T_matmul_NT_k = tuple(T_matmul_NT.op.axis) + tuple(T_matmul_NT.op.reduce_axis)
T_add_ax0, T_add_ax1 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
T_matmul_NT_i_o_i, T_matmul_NT_i_i = s[T_matmul_NT].split(T_matmul_NT_i, factor=4)
T_matmul_NT_i_o_o_i, T_matmul_NT_i_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_i, factor=1)
T_matmul_NT_i_o_o_o, T_matmul_NT_i_o_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_o_i, factor=1)
T_matmul_NT_j_o_i, T_matmul_NT_j_i = s[T_matmul_NT].split(T_matmul_NT_j, factor=16)
T_matmul_NT_j_o_o_i, T_matmul_NT_j_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_i, factor=6)
T_matmul_NT_j_o_o_o, T_matmul_NT_j_o_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_o_i, factor=1)
T_matmul_NT_k_o, T_matmul_NT_k_i = s[T_matmul_NT].split(T_matmul_NT_k, factor=2)
s[T_matmul_NT].reorder(T_matmul_NT_i_o_o_o, T_matmul_NT_j_o_o_o, T_matmul_NT_i_o_o_i, T_matmul_NT_j_o_o_i, T_matmul_NT_k_o, T_matmul_NT_i_o_i, T_matmul_NT_j_o_i, T_matmul_NT_k_i, T_matmul_NT_i_i, T_matmul_NT_j_i)
T_add_ax0_o_i, T_add_ax0_i = s[T_add].split(T_add_ax0, factor=4)
T_add_ax0_o_o, T_add_ax0_o_i = s[T_add].split(T_add_ax0_o_i, factor=1)
T_add_ax1_o_i, T_add_ax1_i = s[T_add].split(T_add_ax1, factor=96)
T_add_ax1_o_o, T_add_ax1_o_i = s[T_add].split(T_add_ax1_o_i, factor=1)
s[T_add].reorder(T_add_ax0_o_o, T_add_ax1_o_o, T_add_ax0_o_i, T_add_ax1_o_i, T_add_ax0_i, T_add_ax1_i)
s[T_matmul_NT].compute_at(s[T_add], T_add_ax1_o_i)
T_add_ax0_o_o_ax1_o_o_fused_ax0_o_i_fused = s[T_add].fuse(T_add_ax0_o_o, T_add_ax1_o_o, T_add_ax0_o_i)
s[T_add].parallel(T_add_ax0_o_o_ax1_o_o_fused_ax0_o_i_fused)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "auto_unroll_max_step", 512)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "unroll_explicit", True)
s[T_matmul_NT].vectorize(T_matmul_NT_j_i)


The best replacement found is:
@main = primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_6: Pointer(float32), float32, [6144], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [589824], []),
             placeholder_2: Buffer(placeholder_8: Pointer(float32), float32, [768], []),
             T_add: Buffer(T_add_2: Pointer(float32), float32, [6144], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add}
  preflattened_buffer_map = {placeholder_3: placeholder_9: Buffer(placeholder_6, float32, [8, 768], []), placeholder_4: placeholder_10: Buffer(placeholder_7, float32, [8, 1, 1, 1, 384, 6, 2, 16], []), placeholder_5: placeholder_11: Buffer(placeholder_8, float32, [1, 768], []), T_add_1: T_add_3: Buffer(T_add_2, float32, [8, 768], [])} {
  for (ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused: int32, 0, 16) "parallel" {
    allocate(T_matmul_NT: Pointer(global float32), float32, [384]), storage_scope = global {
      T_matmul_NT_1: Buffer(T_matmul_NT, float32, [384], [])[ramp(0, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(96, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(192, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(288, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(16, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(112, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(208, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(304, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(32, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(128, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(224, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(320, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(48, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(144, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(240, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(336, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(64, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(160, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(256, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(352, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(80, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(176, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(272, 1, 16)] = broadcast(0f32, 16)
      T_matmul_NT_1[ramp(368, 1, 16)] = broadcast(0f32, 16)
      for (k.outer: int32, 0, 384) {
        let cse_var_20: int32 = ((floormod(ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused, 8)*73728) + (k.outer*192))
        let cse_var_19: int32 = ((floordiv(ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused, 8)*3072) + (k.outer*2))
        let cse_var_18: int32 = (cse_var_19 + 1)
        let cse_var_17: int32 = (cse_var_19 + 1537)
        let cse_var_16: int32 = (cse_var_19 + 2304)
        let cse_var_15: int32 = (cse_var_19 + 2305)
        let cse_var_14: int32 = (cse_var_19 + 768)
        let cse_var_13: int32 = (cse_var_19 + 769)
        let cse_var_12: int32 = (cse_var_20 + 112)
        let cse_var_11: int32 = (cse_var_20 + 128)
        let cse_var_10: int32 = (cse_var_20 + 144)
        let cse_var_9: int32 = (cse_var_20 + 16)
        let cse_var_8: int32 = (cse_var_20 + 160)
        let cse_var_7: int32 = (cse_var_20 + 176)
        let cse_var_6: int32 = (cse_var_20 + 32)
        let cse_var_5: int32 = (cse_var_20 + 48)
        let cse_var_4: int32 = (cse_var_20 + 64)
        let cse_var_3: int32 = (cse_var_20 + 80)
        let cse_var_2: int32 = (cse_var_20 + 96)
        let cse_var_1: int32 = (cse_var_19 + 1536)
         {
          T_matmul_NT_1[ramp(0, 1, 16)] = (T_matmul_NT_1[ramp(0, 1, 16)] + (broadcast(placeholder[cse_var_19], 16)*placeholder_1[ramp(cse_var_20, 1, 16)]))
          T_matmul_NT_1[ramp(96, 1, 16)] = (T_matmul_NT_1[ramp(96, 1, 16)] + (broadcast(placeholder[cse_var_14], 16)*placeholder_1[ramp(cse_var_20, 1, 16)]))
          T_matmul_NT_1[ramp(192, 1, 16)] = (T_matmul_NT_1[ramp(192, 1, 16)] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_20, 1, 16)]))
          T_matmul_NT_1[ramp(288, 1, 16)] = (T_matmul_NT_1[ramp(288, 1, 16)] + (broadcast(placeholder[cse_var_16], 16)*placeholder_1[ramp(cse_var_20, 1, 16)]))
          T_matmul_NT_1[ramp(0, 1, 16)] = (T_matmul_NT_1[ramp(0, 1, 16)] + (broadcast(placeholder[cse_var_18], 16)*placeholder_1[ramp(cse_var_9, 1, 16)]))
          T_matmul_NT_1[ramp(96, 1, 16)] = (T_matmul_NT_1[ramp(96, 1, 16)] + (broadcast(placeholder[cse_var_13], 16)*placeholder_1[ramp(cse_var_9, 1, 16)]))
          T_matmul_NT_1[ramp(192, 1, 16)] = (T_matmul_NT_1[ramp(192, 1, 16)] + (broadcast(placeholder[cse_var_17], 16)*placeholder_1[ramp(cse_var_9, 1, 16)]))
          T_matmul_NT_1[ramp(288, 1, 16)] = (T_matmul_NT_1[ramp(288, 1, 16)] + (broadcast(placeholder[cse_var_15], 16)*placeholder_1[ramp(cse_var_9, 1, 16)]))
          T_matmul_NT_1[ramp(16, 1, 16)] = (T_matmul_NT_1[ramp(16, 1, 16)] + (broadcast(placeholder[cse_var_19], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_matmul_NT_1[ramp(112, 1, 16)] = (T_matmul_NT_1[ramp(112, 1, 16)] + (broadcast(placeholder[cse_var_14], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_matmul_NT_1[ramp(208, 1, 16)] = (T_matmul_NT_1[ramp(208, 1, 16)] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_matmul_NT_1[ramp(304, 1, 16)] = (T_matmul_NT_1[ramp(304, 1, 16)] + (broadcast(placeholder[cse_var_16], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_matmul_NT_1[ramp(16, 1, 16)] = (T_matmul_NT_1[ramp(16, 1, 16)] + (broadcast(placeholder[cse_var_18], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_matmul_NT_1[ramp(112, 1, 16)] = (T_matmul_NT_1[ramp(112, 1, 16)] + (broadcast(placeholder[cse_var_13], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_matmul_NT_1[ramp(208, 1, 16)] = (T_matmul_NT_1[ramp(208, 1, 16)] + (broadcast(placeholder[cse_var_17], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_matmul_NT_1[ramp(304, 1, 16)] = (T_matmul_NT_1[ramp(304, 1, 16)] + (broadcast(placeholder[cse_var_15], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_matmul_NT_1[ramp(32, 1, 16)] = (T_matmul_NT_1[ramp(32, 1, 16)] + (broadcast(placeholder[cse_var_19], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_matmul_NT_1[ramp(128, 1, 16)] = (T_matmul_NT_1[ramp(128, 1, 16)] + (broadcast(placeholder[cse_var_14], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_matmul_NT_1[ramp(224, 1, 16)] = (T_matmul_NT_1[ramp(224, 1, 16)] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_matmul_NT_1[ramp(320, 1, 16)] = (T_matmul_NT_1[ramp(320, 1, 16)] + (broadcast(placeholder[cse_var_16], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_matmul_NT_1[ramp(32, 1, 16)] = (T_matmul_NT_1[ramp(32, 1, 16)] + (broadcast(placeholder[cse_var_18], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_matmul_NT_1[ramp(128, 1, 16)] = (T_matmul_NT_1[ramp(128, 1, 16)] + (broadcast(placeholder[cse_var_13], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_matmul_NT_1[ramp(224, 1, 16)] = (T_matmul_NT_1[ramp(224, 1, 16)] + (broadcast(placeholder[cse_var_17], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_matmul_NT_1[ramp(320, 1, 16)] = (T_matmul_NT_1[ramp(320, 1, 16)] + (broadcast(placeholder[cse_var_15], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_matmul_NT_1[ramp(48, 1, 16)] = (T_matmul_NT_1[ramp(48, 1, 16)] + (broadcast(placeholder[cse_var_19], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_matmul_NT_1[ramp(144, 1, 16)] = (T_matmul_NT_1[ramp(144, 1, 16)] + (broadcast(placeholder[cse_var_14], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_matmul_NT_1[ramp(240, 1, 16)] = (T_matmul_NT_1[ramp(240, 1, 16)] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_matmul_NT_1[ramp(336, 1, 16)] = (T_matmul_NT_1[ramp(336, 1, 16)] + (broadcast(placeholder[cse_var_16], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_matmul_NT_1[ramp(48, 1, 16)] = (T_matmul_NT_1[ramp(48, 1, 16)] + (broadcast(placeholder[cse_var_18], 16)*placeholder_1[ramp(cse_var_12, 1, 16)]))
          T_matmul_NT_1[ramp(144, 1, 16)] = (T_matmul_NT_1[ramp(144, 1, 16)] + (broadcast(placeholder[cse_var_13], 16)*placeholder_1[ramp(cse_var_12, 1, 16)]))
          T_matmul_NT_1[ramp(240, 1, 16)] = (T_matmul_NT_1[ramp(240, 1, 16)] + (broadcast(placeholder[cse_var_17], 16)*placeholder_1[ramp(cse_var_12, 1, 16)]))
          T_matmul_NT_1[ramp(336, 1, 16)] = (T_matmul_NT_1[ramp(336, 1, 16)] + (broadcast(placeholder[cse_var_15], 16)*placeholder_1[ramp(cse_var_12, 1, 16)]))
          T_matmul_NT_1[ramp(64, 1, 16)] = (T_matmul_NT_1[ramp(64, 1, 16)] + (broadcast(placeholder[cse_var_19], 16)*placeholder_1[ramp(cse_var_11, 1, 16)]))
          T_matmul_NT_1[ramp(160, 1, 16)] = (T_matmul_NT_1[ramp(160, 1, 16)] + (broadcast(placeholder[cse_var_14], 16)*placeholder_1[ramp(cse_var_11, 1, 16)]))
          T_matmul_NT_1[ramp(256, 1, 16)] = (T_matmul_NT_1[ramp(256, 1, 16)] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_11, 1, 16)]))
          T_matmul_NT_1[ramp(352, 1, 16)] = (T_matmul_NT_1[ramp(352, 1, 16)] + (broadcast(placeholder[cse_var_16], 16)*placeholder_1[ramp(cse_var_11, 1, 16)]))
          T_matmul_NT_1[ramp(64, 1, 16)] = (T_matmul_NT_1[ramp(64, 1, 16)] + (broadcast(placeholder[cse_var_18], 16)*placeholder_1[ramp(cse_var_10, 1, 16)]))
          T_matmul_NT_1[ramp(160, 1, 16)] = (T_matmul_NT_1[ramp(160, 1, 16)] + (broadcast(placeholder[cse_var_13], 16)*placeholder_1[ramp(cse_var_10, 1, 16)]))
          T_matmul_NT_1[ramp(256, 1, 16)] = (T_matmul_NT_1[ramp(256, 1, 16)] + (broadcast(placeholder[cse_var_17], 16)*placeholder_1[ramp(cse_var_10, 1, 16)]))
          T_matmul_NT_1[ramp(352, 1, 16)] = (T_matmul_NT_1[ramp(352, 1, 16)] + (broadcast(placeholder[cse_var_15], 16)*placeholder_1[ramp(cse_var_10, 1, 16)]))
          T_matmul_NT_1[ramp(80, 1, 16)] = (T_matmul_NT_1[ramp(80, 1, 16)] + (broadcast(placeholder[cse_var_19], 16)*placeholder_1[ramp(cse_var_8, 1, 16)]))
          T_matmul_NT_1[ramp(176, 1, 16)] = (T_matmul_NT_1[ramp(176, 1, 16)] + (broadcast(placeholder[cse_var_14], 16)*placeholder_1[ramp(cse_var_8, 1, 16)]))
          T_matmul_NT_1[ramp(272, 1, 16)] = (T_matmul_NT_1[ramp(272, 1, 16)] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_8, 1, 16)]))
          T_matmul_NT_1[ramp(368, 1, 16)] = (T_matmul_NT_1[ramp(368, 1, 16)] + (broadcast(placeholder[cse_var_16], 16)*placeholder_1[ramp(cse_var_8, 1, 16)]))
          T_matmul_NT_1[ramp(80, 1, 16)] = (T_matmul_NT_1[ramp(80, 1, 16)] + (broadcast(placeholder[cse_var_18], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_matmul_NT_1[ramp(176, 1, 16)] = (T_matmul_NT_1[ramp(176, 1, 16)] + (broadcast(placeholder[cse_var_13], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_matmul_NT_1[ramp(272, 1, 16)] = (T_matmul_NT_1[ramp(272, 1, 16)] + (broadcast(placeholder[cse_var_17], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_matmul_NT_1[ramp(368, 1, 16)] = (T_matmul_NT_1[ramp(368, 1, 16)] + (broadcast(placeholder[cse_var_15], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
        }
      }
      for (ax0.inner: int32, 0, 4) {
        for (ax1.inner: int32, 0, 96) {
          let cse_var_21: int32 = (floormod(ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused, 8)*96)
          T_add[((((floordiv(ax0.outer.outer.ax1.outer.outer.fused.ax0.outer.inner.fused, 8)*3072) + (ax0.inner*768)) + cse_var_21) + ax1.inner)] = (T_matmul_NT_1[((ax0.inner*96) + ax1.inner)] + placeholder_2[(cse_var_21 + ax1.inner)])
        }
      }
    }
  }
}


==== Task 5: vm_mod_fused_nn_batch_matmul_1 (weight 12 key: ["32a5ff201401d5600f77ced9f22defaa", [12, 8, 8], [12, 64, 8], [12, 8, 64]]) =====
placeholder = PLACEHOLDER [12, 8, 8]
placeholder = PLACEHOLDER [12, 64, 8]
T_batch_matmul_NT(b, i, j) += (placeholder[b, i, k]*placeholder[b, j, k])


Trace for this task is: 
T_batch_matmul_NT_b, T_batch_matmul_NT_i, T_batch_matmul_NT_j, T_batch_matmul_NT_k = tuple(T_batch_matmul_NT.op.axis) + tuple(T_batch_matmul_NT.op.reduce_axis)
T_batch_matmul_NT_local, = s.cache_write([T_batch_matmul_NT], "local")
T_batch_matmul_NT_local_b_c, T_batch_matmul_NT_local_i_c, T_batch_matmul_NT_local_j_c, T_batch_matmul_NT_local_k = tuple(T_batch_matmul_NT_local.op.axis) + tuple(T_batch_matmul_NT_local.op.reduce_axis)
T_batch_matmul_NT_local_b_c_o_i, T_batch_matmul_NT_local_b_c_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_b_c, factor=3)
T_batch_matmul_NT_local_b_c_o_o_i, T_batch_matmul_NT_local_b_c_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_b_c_o_i, factor=1)
T_batch_matmul_NT_local_b_c_o_o_o, T_batch_matmul_NT_local_b_c_o_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_b_c_o_o_i, factor=1)
T_batch_matmul_NT_local_i_c_o_i, T_batch_matmul_NT_local_i_c_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_i_c, factor=4)
T_batch_matmul_NT_local_i_c_o_o_i, T_batch_matmul_NT_local_i_c_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_i_c_o_i, factor=1)
T_batch_matmul_NT_local_i_c_o_o_o, T_batch_matmul_NT_local_i_c_o_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_i_c_o_o_i, factor=2)
T_batch_matmul_NT_local_j_c_o_i, T_batch_matmul_NT_local_j_c_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_j_c, factor=16)
T_batch_matmul_NT_local_j_c_o_o_i, T_batch_matmul_NT_local_j_c_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_j_c_o_i, factor=1)
T_batch_matmul_NT_local_j_c_o_o_o, T_batch_matmul_NT_local_j_c_o_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_j_c_o_o_i, factor=1)
T_batch_matmul_NT_local_k_o, T_batch_matmul_NT_local_k_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_k, factor=2)
s[T_batch_matmul_NT_local].reorder(T_batch_matmul_NT_local_b_c_o_o_o, T_batch_matmul_NT_local_i_c_o_o_o, T_batch_matmul_NT_local_j_c_o_o_o, T_batch_matmul_NT_local_b_c_o_o_i, T_batch_matmul_NT_local_i_c_o_o_i, T_batch_matmul_NT_local_j_c_o_o_i, T_batch_matmul_NT_local_k_o, T_batch_matmul_NT_local_b_c_o_i, T_batch_matmul_NT_local_i_c_o_i, T_batch_matmul_NT_local_j_c_o_i, T_batch_matmul_NT_local_k_i, T_batch_matmul_NT_local_b_c_i, T_batch_matmul_NT_local_i_c_i, T_batch_matmul_NT_local_j_c_i)
T_batch_matmul_NT_b_o_i, T_batch_matmul_NT_b_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_b, factor=3)
T_batch_matmul_NT_b_o_o, T_batch_matmul_NT_b_o_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_b_o_i, factor=1)
T_batch_matmul_NT_i_o_i, T_batch_matmul_NT_i_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_i, factor=4)
T_batch_matmul_NT_i_o_o, T_batch_matmul_NT_i_o_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_i_o_i, factor=2)
T_batch_matmul_NT_j_o_i, T_batch_matmul_NT_j_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_j, factor=16)
T_batch_matmul_NT_j_o_o, T_batch_matmul_NT_j_o_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_j_o_i, factor=1)
s[T_batch_matmul_NT].reorder(T_batch_matmul_NT_b_o_o, T_batch_matmul_NT_i_o_o, T_batch_matmul_NT_j_o_o, T_batch_matmul_NT_b_o_i, T_batch_matmul_NT_i_o_i, T_batch_matmul_NT_j_o_i, T_batch_matmul_NT_b_i, T_batch_matmul_NT_i_i, T_batch_matmul_NT_j_i)
s[T_batch_matmul_NT_local].compute_at(s[T_batch_matmul_NT], T_batch_matmul_NT_j_o_i)
T_batch_matmul_NT_b_o_o_i_o_o_fused_j_o_o_fused_b_o_i_fused_i_o_i_fused_j_o_i_fused = s[T_batch_matmul_NT].fuse(T_batch_matmul_NT_b_o_o, T_batch_matmul_NT_i_o_o, T_batch_matmul_NT_j_o_o, T_batch_matmul_NT_b_o_i, T_batch_matmul_NT_i_o_i, T_batch_matmul_NT_j_o_i)
s[T_batch_matmul_NT].parallel(T_batch_matmul_NT_b_o_o_i_o_o_fused_j_o_o_fused_b_o_i_fused_i_o_i_fused_j_o_i_fused)
s[T_batch_matmul_NT_local].pragma(T_batch_matmul_NT_local_b_c_o_o_o, "auto_unroll_max_step", 64)
s[T_batch_matmul_NT_local].pragma(T_batch_matmul_NT_local_b_c_o_o_o, "unroll_explicit", True)
s[T_batch_matmul_NT_local].vectorize(T_batch_matmul_NT_local_j_c_i)
s[T_batch_matmul_NT].vectorize(T_batch_matmul_NT_j_i)


The best replacement found is:
@main = primfn(placeholder_2: handle, placeholder_3: handle, T_batch_matmul_NT_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(float32), float32, [768], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [6144], []),
             T_batch_matmul_NT: Buffer(T_batch_matmul_NT_2: Pointer(float32), float32, [6144], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_batch_matmul_NT_1: T_batch_matmul_NT}
  preflattened_buffer_map = {placeholder_2: placeholder_6: Buffer(placeholder_4, float32, [12, 8, 8], []), placeholder_3: placeholder_7: Buffer(placeholder_5, float32, [4, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 16], []), T_batch_matmul_NT_1: T_batch_matmul_NT_3: Buffer(T_batch_matmul_NT_2, float32, [12, 8, 64], [])} {
  for (b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused: int32, 0, 32) "parallel" {
    allocate(T_batch_matmul_NT.local: Pointer(local float32x16), float32x16, [12]), storage_scope = local {
      T_batch_matmul_NT.local_1: Buffer(T_batch_matmul_NT.local, float32x16, [12], [], scope="local")[0] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[1] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[2] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[3] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[4] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[5] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[6] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[7] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[8] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[9] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[10] = broadcast(0f32, 16)
      T_batch_matmul_NT.local_1[11] = broadcast(0f32, 16)
      for (k.outer: int32, 0, 4) {
        let cse_var_7: int32 = ((floordiv(b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused, 2)*384) + (k.outer*96))
        let cse_var_6: int32 = (cse_var_7 + 80)
        let cse_var_5: int32 = (cse_var_7 + 64)
        let cse_var_4: int32 = (cse_var_7 + 48)
        let cse_var_3: int32 = (cse_var_7 + 32)
        let cse_var_2: int32 = (cse_var_7 + 16)
        let cse_var_1: int32 = (((floordiv(b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused, 8)*192) + (floormod(b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused, 2)*32)) + (k.outer*2))
         {
          T_batch_matmul_NT.local_1[0] = (T_batch_matmul_NT.local_1[0] + (broadcast(placeholder[cse_var_1], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_batch_matmul_NT.local_1[1] = (T_batch_matmul_NT.local_1[1] + (broadcast(placeholder[(cse_var_1 + 8)], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_batch_matmul_NT.local_1[2] = (T_batch_matmul_NT.local_1[2] + (broadcast(placeholder[(cse_var_1 + 16)], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_batch_matmul_NT.local_1[3] = (T_batch_matmul_NT.local_1[3] + (broadcast(placeholder[(cse_var_1 + 24)], 16)*placeholder_1[ramp(cse_var_7, 1, 16)]))
          T_batch_matmul_NT.local_1[4] = (T_batch_matmul_NT.local_1[4] + (broadcast(placeholder[(cse_var_1 + 64)], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_batch_matmul_NT.local_1[5] = (T_batch_matmul_NT.local_1[5] + (broadcast(placeholder[(cse_var_1 + 72)], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_batch_matmul_NT.local_1[6] = (T_batch_matmul_NT.local_1[6] + (broadcast(placeholder[(cse_var_1 + 80)], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_batch_matmul_NT.local_1[7] = (T_batch_matmul_NT.local_1[7] + (broadcast(placeholder[(cse_var_1 + 88)], 16)*placeholder_1[ramp(cse_var_2, 1, 16)]))
          T_batch_matmul_NT.local_1[8] = (T_batch_matmul_NT.local_1[8] + (broadcast(placeholder[(cse_var_1 + 128)], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_batch_matmul_NT.local_1[9] = (T_batch_matmul_NT.local_1[9] + (broadcast(placeholder[(cse_var_1 + 136)], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_batch_matmul_NT.local_1[10] = (T_batch_matmul_NT.local_1[10] + (broadcast(placeholder[(cse_var_1 + 144)], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_batch_matmul_NT.local_1[11] = (T_batch_matmul_NT.local_1[11] + (broadcast(placeholder[(cse_var_1 + 152)], 16)*placeholder_1[ramp(cse_var_3, 1, 16)]))
          T_batch_matmul_NT.local_1[0] = (T_batch_matmul_NT.local_1[0] + (broadcast(placeholder[(cse_var_1 + 1)], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_batch_matmul_NT.local_1[1] = (T_batch_matmul_NT.local_1[1] + (broadcast(placeholder[(cse_var_1 + 9)], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_batch_matmul_NT.local_1[2] = (T_batch_matmul_NT.local_1[2] + (broadcast(placeholder[(cse_var_1 + 17)], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_batch_matmul_NT.local_1[3] = (T_batch_matmul_NT.local_1[3] + (broadcast(placeholder[(cse_var_1 + 25)], 16)*placeholder_1[ramp(cse_var_4, 1, 16)]))
          T_batch_matmul_NT.local_1[4] = (T_batch_matmul_NT.local_1[4] + (broadcast(placeholder[(cse_var_1 + 65)], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_batch_matmul_NT.local_1[5] = (T_batch_matmul_NT.local_1[5] + (broadcast(placeholder[(cse_var_1 + 73)], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_batch_matmul_NT.local_1[6] = (T_batch_matmul_NT.local_1[6] + (broadcast(placeholder[(cse_var_1 + 81)], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_batch_matmul_NT.local_1[7] = (T_batch_matmul_NT.local_1[7] + (broadcast(placeholder[(cse_var_1 + 89)], 16)*placeholder_1[ramp(cse_var_5, 1, 16)]))
          T_batch_matmul_NT.local_1[8] = (T_batch_matmul_NT.local_1[8] + (broadcast(placeholder[(cse_var_1 + 129)], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_batch_matmul_NT.local_1[9] = (T_batch_matmul_NT.local_1[9] + (broadcast(placeholder[(cse_var_1 + 137)], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_batch_matmul_NT.local_1[10] = (T_batch_matmul_NT.local_1[10] + (broadcast(placeholder[(cse_var_1 + 145)], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
          T_batch_matmul_NT.local_1[11] = (T_batch_matmul_NT.local_1[11] + (broadcast(placeholder[(cse_var_1 + 153)], 16)*placeholder_1[ramp(cse_var_6, 1, 16)]))
        }
      }
      for (b.inner: int32, 0, 3) {
        for (i.inner: int32, 0, 4) {
          T_batch_matmul_NT[ramp((((((floordiv(b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused, 8)*1536) + (b.inner*512)) + (floormod(b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused, 2)*256)) + (i.inner*64)) + (floordiv(floormod(b.outer.outer.i.outer.outer.fused.j.outer.outer.fused.b.outer.inner.fused.i.outer.inner.fused.j.outer.inner.fused, 8), 2)*16)), 1, 16)] = T_batch_matmul_NT.local_1[((b.inner*4) + i.inner)]
        }
      }
    }
  }
}


==== Task 6: vm_mod_fused_nn_dense_add (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 768], [2304, 768], [1, 2304], [8, 2304]]) =====
placeholder = PLACEHOLDER [8, 768]
placeholder = PLACEHOLDER [2304, 768]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 2304]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])


Trace for this task is: 
T_matmul_NT_i, T_matmul_NT_j, T_matmul_NT_k = tuple(T_matmul_NT.op.axis) + tuple(T_matmul_NT.op.reduce_axis)
T_add_ax0, T_add_ax1 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
T_matmul_NT_i_o_i, T_matmul_NT_i_i = s[T_matmul_NT].split(T_matmul_NT_i, factor=4)
T_matmul_NT_i_o_o_i, T_matmul_NT_i_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_i, factor=2)
T_matmul_NT_i_o_o_o, T_matmul_NT_i_o_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_o_i, factor=1)
T_matmul_NT_j_o_i, T_matmul_NT_j_i = s[T_matmul_NT].split(T_matmul_NT_j, factor=32)
T_matmul_NT_j_o_o_i, T_matmul_NT_j_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_i, factor=1)
T_matmul_NT_j_o_o_o, T_matmul_NT_j_o_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_o_i, factor=4)
T_matmul_NT_k_o, T_matmul_NT_k_i = s[T_matmul_NT].split(T_matmul_NT_k, factor=3)
s[T_matmul_NT].reorder(T_matmul_NT_i_o_o_o, T_matmul_NT_j_o_o_o, T_matmul_NT_i_o_o_i, T_matmul_NT_j_o_o_i, T_matmul_NT_k_o, T_matmul_NT_i_o_i, T_matmul_NT_j_o_i, T_matmul_NT_k_i, T_matmul_NT_i_i, T_matmul_NT_j_i)
T_add_ax0_o, T_add_ax0_i = s[T_add].split(T_add_ax0, factor=8)
T_add_ax1_o, T_add_ax1_i = s[T_add].split(T_add_ax1, factor=128)
s[T_add].reorder(T_add_ax0_o, T_add_ax1_o, T_add_ax0_i, T_add_ax1_i)
s[T_matmul_NT].compute_at(s[T_add], T_add_ax1_o)
T_add_ax0_o_ax1_o_fused = s[T_add].fuse(T_add_ax0_o, T_add_ax1_o)
s[T_add].parallel(T_add_ax0_o_ax1_o_fused)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "auto_unroll_max_step", 64)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "unroll_explicit", True)
s[T_matmul_NT].vectorize(T_matmul_NT_j_i)


The best replacement found is:
@main = primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_6: Pointer(float32), float32, [6144], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [1769472], []),
             placeholder_2: Buffer(placeholder_8: Pointer(float32), float32, [2304], []),
             T_add: Buffer(T_add_2: Pointer(float32), float32, [18432], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add}
  preflattened_buffer_map = {placeholder_3: placeholder_9: Buffer(placeholder_6, float32, [8, 768], []), placeholder_4: placeholder_10: Buffer(placeholder_7, float32, [18, 1, 4, 256, 1, 3, 32], []), placeholder_5: placeholder_11: Buffer(placeholder_8, float32, [1, 2304], []), T_add_1: T_add_3: Buffer(T_add_2, float32, [8, 2304], [])} {
  for (ax0.outer.ax1.outer.fused: int32, 0, 18) "parallel" {
    allocate(T_matmul_NT: Pointer(global float32), float32, [1024]), storage_scope = global {
      for (j.outer.outer.inner: int32, 0, 4) {
        let cse_var_1: int32 = (j.outer.outer.inner*32)
         {
          T_matmul_NT_1: Buffer(T_matmul_NT, float32, [1024], [])[ramp(cse_var_1, 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 128), 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 256), 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 384), 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 512), 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 640), 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 768), 1, 32)] = broadcast(0f32, 32)
          T_matmul_NT_1[ramp((cse_var_1 + 896), 1, 32)] = broadcast(0f32, 32)
          for (k.outer: int32, 0, 256) {
            let cse_var_12: int32 = (k.outer*3)
            let cse_var_11: int32 = (cse_var_1 + 896)
            let cse_var_10: int32 = (cse_var_1 + 768)
            let cse_var_9: int32 = (cse_var_1 + 640)
            let cse_var_8: int32 = (cse_var_1 + 512)
            let cse_var_7: int32 = (cse_var_1 + 384)
            let cse_var_6: int32 = (cse_var_1 + 256)
            let cse_var_5: int32 = (cse_var_1 + 128)
            let cse_var_4: int32 = (((ax0.outer.ax1.outer.fused*98304) + (j.outer.outer.inner*24576)) + (k.outer*96))
            let cse_var_3: int32 = (cse_var_4 + 32)
            let cse_var_2: int32 = (cse_var_4 + 64)
             {
              T_matmul_NT_1[ramp(cse_var_1, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_1, 1, 32)] + (broadcast(placeholder[cse_var_12], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_5, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_5, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 768)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_6, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_6, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 1536)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_7, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_7, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 2304)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_1, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_1, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 1)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_5, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_5, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 769)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_6, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_6, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 1537)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_7, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_7, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 2305)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_1, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_1, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 2)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_5, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_5, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 770)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_6, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_6, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 1538)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_7, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_7, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 2306)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_8, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_8, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 3072)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_9, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_9, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 3840)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_10, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_10, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 4608)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_11, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_11, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 5376)], 32)*placeholder_1[ramp(cse_var_4, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_8, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_8, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 3073)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_9, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_9, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 3841)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_10, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_10, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 4609)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_11, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_11, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 5377)], 32)*placeholder_1[ramp(cse_var_3, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_8, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_8, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 3074)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_9, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_9, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 3842)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_10, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_10, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 4610)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
              T_matmul_NT_1[ramp(cse_var_11, 1, 32)] = (T_matmul_NT_1[ramp(cse_var_11, 1, 32)] + (broadcast(placeholder[(cse_var_12 + 5378)], 32)*placeholder_1[ramp(cse_var_2, 1, 32)]))
            }
          }
        }
      }
      for (ax0.inner: int32, 0, 8) {
        for (ax1.inner: int32, 0, 128) {
          let cse_var_13: int32 = (ax0.outer.ax1.outer.fused*128)
          T_add[(((ax0.inner*2304) + cse_var_13) + ax1.inner)] = (T_matmul_NT_1[((ax0.inner*128) + ax1.inner)] + placeholder_2[(cse_var_13 + ax1.inner)])
        }
      }
    }
  }
}


==== Task 7: vm_mod_fused_nn_batch_matmul (weight 12 key: ["32a5ff201401d5600f77ced9f22defaa", [12, 8, 64], [12, 8, 64], [12, 8, 8]]) =====
placeholder = PLACEHOLDER [12, 8, 64]
placeholder = PLACEHOLDER [12, 8, 64]
T_batch_matmul_NT(b, i, j) += (placeholder[b, i, k]*placeholder[b, j, k])


Trace for this task is: 
T_batch_matmul_NT_b, T_batch_matmul_NT_i, T_batch_matmul_NT_j, T_batch_matmul_NT_k = tuple(T_batch_matmul_NT.op.axis) + tuple(T_batch_matmul_NT.op.reduce_axis)
T_batch_matmul_NT_local, = s.cache_write([T_batch_matmul_NT], "local")
T_batch_matmul_NT_local_b_c, T_batch_matmul_NT_local_i_c, T_batch_matmul_NT_local_j_c, T_batch_matmul_NT_local_k = tuple(T_batch_matmul_NT_local.op.axis) + tuple(T_batch_matmul_NT_local.op.reduce_axis)
T_batch_matmul_NT_local_b_c_o_i, T_batch_matmul_NT_local_b_c_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_b_c, factor=3)
T_batch_matmul_NT_local_b_c_o_o_i, T_batch_matmul_NT_local_b_c_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_b_c_o_i, factor=1)
T_batch_matmul_NT_local_b_c_o_o_o, T_batch_matmul_NT_local_b_c_o_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_b_c_o_o_i, factor=4)
T_batch_matmul_NT_local_i_c_o_i, T_batch_matmul_NT_local_i_c_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_i_c, factor=8)
T_batch_matmul_NT_local_i_c_o_o_i, T_batch_matmul_NT_local_i_c_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_i_c_o_i, factor=1)
T_batch_matmul_NT_local_i_c_o_o_o, T_batch_matmul_NT_local_i_c_o_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_i_c_o_o_i, factor=1)
T_batch_matmul_NT_local_j_c_o_i, T_batch_matmul_NT_local_j_c_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_j_c, factor=8)
T_batch_matmul_NT_local_j_c_o_o_i, T_batch_matmul_NT_local_j_c_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_j_c_o_i, factor=1)
T_batch_matmul_NT_local_j_c_o_o_o, T_batch_matmul_NT_local_j_c_o_o_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_j_c_o_o_i, factor=1)
T_batch_matmul_NT_local_k_o, T_batch_matmul_NT_local_k_i = s[T_batch_matmul_NT_local].split(T_batch_matmul_NT_local_k, factor=32)
s[T_batch_matmul_NT_local].reorder(T_batch_matmul_NT_local_b_c_o_o_o, T_batch_matmul_NT_local_i_c_o_o_o, T_batch_matmul_NT_local_j_c_o_o_o, T_batch_matmul_NT_local_b_c_o_o_i, T_batch_matmul_NT_local_i_c_o_o_i, T_batch_matmul_NT_local_j_c_o_o_i, T_batch_matmul_NT_local_k_o, T_batch_matmul_NT_local_b_c_o_i, T_batch_matmul_NT_local_i_c_o_i, T_batch_matmul_NT_local_j_c_o_i, T_batch_matmul_NT_local_k_i, T_batch_matmul_NT_local_b_c_i, T_batch_matmul_NT_local_i_c_i, T_batch_matmul_NT_local_j_c_i)
T_batch_matmul_NT_b_o, T_batch_matmul_NT_b_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_b, factor=12)
T_batch_matmul_NT_i_o, T_batch_matmul_NT_i_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_i, factor=8)
T_batch_matmul_NT_j_o, T_batch_matmul_NT_j_i = s[T_batch_matmul_NT].split(T_batch_matmul_NT_j, factor=8)
s[T_batch_matmul_NT].reorder(T_batch_matmul_NT_b_o, T_batch_matmul_NT_i_o, T_batch_matmul_NT_j_o, T_batch_matmul_NT_b_i, T_batch_matmul_NT_i_i, T_batch_matmul_NT_j_i)
s[T_batch_matmul_NT_local].compute_at(s[T_batch_matmul_NT], T_batch_matmul_NT_j_o)
T_batch_matmul_NT_b_o_i_o_fused_j_o_fused = s[T_batch_matmul_NT].fuse(T_batch_matmul_NT_b_o, T_batch_matmul_NT_i_o, T_batch_matmul_NT_j_o)
s[T_batch_matmul_NT].parallel(T_batch_matmul_NT_b_o_i_o_fused_j_o_fused)
s[T_batch_matmul_NT_local].pragma(T_batch_matmul_NT_local_b_c_o_o_o, "auto_unroll_max_step", 64)
s[T_batch_matmul_NT_local].pragma(T_batch_matmul_NT_local_b_c_o_o_o, "unroll_explicit", True)
s[T_batch_matmul_NT_local].vectorize(T_batch_matmul_NT_local_j_c_i)
s[T_batch_matmul_NT].vectorize(T_batch_matmul_NT_j_i)


The best replacement found is:
@main = primfn(placeholder_2: handle, placeholder_3: handle, T_batch_matmul_NT_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_4: Pointer(float32), float32, [6144], []),
             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [6144], []),
             T_batch_matmul_NT: Buffer(T_batch_matmul_NT_2: Pointer(float32), float32, [768], [])}
  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1, T_batch_matmul_NT_1: T_batch_matmul_NT}
  preflattened_buffer_map = {placeholder_2: placeholder_6: Buffer(placeholder_4, float32, [12, 8, 64], []), placeholder_3: placeholder_7: Buffer(placeholder_5, float32, [1, 1, 1, 1, 4, 1, 2, 1, 1, 32, 3, 8], []), T_batch_matmul_NT_1: T_batch_matmul_NT_3: Buffer(T_batch_matmul_NT_2, float32, [12, 8, 8], [])} {
  allocate(T_batch_matmul_NT.local: Pointer(local float32x8), float32x8, [96]), storage_scope = local {
    for (b.c.outer.outer.inner: int32, 0, 4) {
      let cse_var_1: int32 = (b.c.outer.outer.inner*24)
       {
        T_batch_matmul_NT.local_1: Buffer(T_batch_matmul_NT.local, float32x8, [96], [], scope="local")[cse_var_1] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 1)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 2)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 3)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 4)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 5)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 6)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 7)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 8)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 9)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 10)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 11)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 12)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 13)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 14)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 15)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 16)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 17)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 18)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 19)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 20)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 21)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 22)] = broadcast(0f32, 8)
        T_batch_matmul_NT.local_1[(cse_var_1 + 23)] = broadcast(0f32, 8)
        for (k.outer: int32, 0, 2) {
          for (k.inner: int32, 0, 32) {
            let cse_var_29: int32 = (b.c.outer.outer.inner*1536)
            let cse_var_28: int32 = (cse_var_1 + 10)
            let cse_var_27: int32 = (cse_var_1 + 11)
            let cse_var_26: int32 = (cse_var_1 + 12)
            let cse_var_25: int32 = (cse_var_1 + 13)
            let cse_var_24: int32 = (cse_var_1 + 14)
            let cse_var_23: int32 = (cse_var_1 + 15)
            let cse_var_22: int32 = (cse_var_1 + 16)
            let cse_var_21: int32 = (cse_var_1 + 17)
            let cse_var_20: int32 = (cse_var_1 + 18)
            let cse_var_19: int32 = (cse_var_1 + 19)
            let cse_var_18: int32 = (cse_var_1 + 1)
            let cse_var_17: int32 = (cse_var_1 + 20)
            let cse_var_16: int32 = (cse_var_1 + 21)
            let cse_var_15: int32 = (cse_var_1 + 22)
            let cse_var_14: int32 = (cse_var_1 + 23)
            let cse_var_13: int32 = (cse_var_1 + 3)
            let cse_var_12: int32 = (cse_var_1 + 4)
            let cse_var_11: int32 = (cse_var_1 + 5)
            let cse_var_10: int32 = (cse_var_1 + 6)
            let cse_var_9: int32 = (cse_var_1 + 7)
            let cse_var_8: int32 = (cse_var_1 + 8)
            let cse_var_7: int32 = (cse_var_1 + 9)
            let cse_var_6: int32 = (cse_var_1 + 2)
            let cse_var_5: int32 = ((cse_var_29 + (k.outer*32)) + k.inner)
            let cse_var_4: int32 = ((cse_var_29 + (k.outer*768)) + (k.inner*24))
            let cse_var_3: int32 = (cse_var_4 + 16)
            let cse_var_2: int32 = (cse_var_4 + 8)
             {
              T_batch_matmul_NT.local_1[cse_var_1] = (T_batch_matmul_NT.local_1[cse_var_1] + (broadcast(placeholder[cse_var_5], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_18] = (T_batch_matmul_NT.local_1[cse_var_18] + (broadcast(placeholder[(cse_var_5 + 64)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_6] = (T_batch_matmul_NT.local_1[cse_var_6] + (broadcast(placeholder[(cse_var_5 + 128)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_13] = (T_batch_matmul_NT.local_1[cse_var_13] + (broadcast(placeholder[(cse_var_5 + 192)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_12] = (T_batch_matmul_NT.local_1[cse_var_12] + (broadcast(placeholder[(cse_var_5 + 256)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_11] = (T_batch_matmul_NT.local_1[cse_var_11] + (broadcast(placeholder[(cse_var_5 + 320)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_10] = (T_batch_matmul_NT.local_1[cse_var_10] + (broadcast(placeholder[(cse_var_5 + 384)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_9] = (T_batch_matmul_NT.local_1[cse_var_9] + (broadcast(placeholder[(cse_var_5 + 448)], 8)*placeholder_1[ramp(cse_var_4, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_8] = (T_batch_matmul_NT.local_1[cse_var_8] + (broadcast(placeholder[(cse_var_5 + 512)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_7] = (T_batch_matmul_NT.local_1[cse_var_7] + (broadcast(placeholder[(cse_var_5 + 576)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_28] = (T_batch_matmul_NT.local_1[cse_var_28] + (broadcast(placeholder[(cse_var_5 + 640)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_27] = (T_batch_matmul_NT.local_1[cse_var_27] + (broadcast(placeholder[(cse_var_5 + 704)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_26] = (T_batch_matmul_NT.local_1[cse_var_26] + (broadcast(placeholder[(cse_var_5 + 768)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_25] = (T_batch_matmul_NT.local_1[cse_var_25] + (broadcast(placeholder[(cse_var_5 + 832)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_24] = (T_batch_matmul_NT.local_1[cse_var_24] + (broadcast(placeholder[(cse_var_5 + 896)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_23] = (T_batch_matmul_NT.local_1[cse_var_23] + (broadcast(placeholder[(cse_var_5 + 960)], 8)*placeholder_1[ramp(cse_var_2, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_22] = (T_batch_matmul_NT.local_1[cse_var_22] + (broadcast(placeholder[(cse_var_5 + 1024)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_21] = (T_batch_matmul_NT.local_1[cse_var_21] + (broadcast(placeholder[(cse_var_5 + 1088)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_20] = (T_batch_matmul_NT.local_1[cse_var_20] + (broadcast(placeholder[(cse_var_5 + 1152)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_19] = (T_batch_matmul_NT.local_1[cse_var_19] + (broadcast(placeholder[(cse_var_5 + 1216)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_17] = (T_batch_matmul_NT.local_1[cse_var_17] + (broadcast(placeholder[(cse_var_5 + 1280)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_16] = (T_batch_matmul_NT.local_1[cse_var_16] + (broadcast(placeholder[(cse_var_5 + 1344)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_15] = (T_batch_matmul_NT.local_1[cse_var_15] + (broadcast(placeholder[(cse_var_5 + 1408)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
              T_batch_matmul_NT.local_1[cse_var_14] = (T_batch_matmul_NT.local_1[cse_var_14] + (broadcast(placeholder[(cse_var_5 + 1472)], 8)*placeholder_1[ramp(cse_var_3, 1, 8)]))
            }
          }
        }
      }
    }
    for (b.inner: int32, 0, 12) {
      for (i.inner: int32, 0, 8) {
        T_batch_matmul_NT[ramp(((b.inner*64) + (i.inner*8)), 1, 8)] = T_batch_matmul_NT.local_1[((b.inner*8) + i.inner)]
      }
    }
  }
}

/home/ubuntu/miniconda/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
  from pandas import MultiIndex, Int64Index
/home/ubuntu/miniconda/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
Traceback (most recent call last):
  File "/home/ubuntu/miniconda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ubuntu/miniconda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu/tvm/python/tvm/meta_schedule/testing/tune_relay_auto_scheduler.py", line 250, in <module>
    main()
  File "/home/ubuntu/tvm/python/tvm/meta_schedule/testing/tune_relay_auto_scheduler.py", line 240, in main
    run_module_via_rpc(
  File "/home/ubuntu/tvm/python/tvm/meta_schedule/testing/custom_builder_runner.py", line 170, in run_module_via_rpc
    return continuation(rt_mod, dev, args)
  File "/home/ubuntu/tvm/python/tvm/meta_schedule/testing/tune_relay_auto_scheduler.py", line 233, in f_per_layer
    graph_time = mod.run_individual(number=10, repeat=1, min_repeat_ms=5000)
  File "/home/ubuntu/tvm/python/tvm/contrib/debugger/debug_executor.py", line 283, in run_individual
    ret = self._run_individual(number, repeat, min_repeat_ms)
  File "/home/ubuntu/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
  2: tvm::runtime::RPCWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
  1: tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)> const&)
  0: tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::function<void (tvm::runtime::TVMArgs)>)
  File "/home/ubuntu/tvm/src/runtime/rpc/rpc_endpoint.cc", line 801
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (code == RPCCode::kReturn) is false: code=kShutdown

==== Task 8: vm_mod_fused_nn_dense_add_3 (weight 12 key: ["3458e75493bb4bb042e539b5ff4818fb", [8, 3072], [768, 3072], [1, 768], [8, 768]]) =====
placeholder = PLACEHOLDER [8, 3072]
placeholder = PLACEHOLDER [768, 3072]
T_matmul_NT(i, j) += (placeholder[i, k]*placeholder[j, k])
placeholder = PLACEHOLDER [1, 768]
T_add(ax0, ax1) = (T_matmul_NT[ax0, ax1] + placeholder[0, ax1])


Trace for this task is: 
T_matmul_NT_i, T_matmul_NT_j, T_matmul_NT_k = tuple(T_matmul_NT.op.axis) + tuple(T_matmul_NT.op.reduce_axis)
T_add_ax0, T_add_ax1 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
T_matmul_NT_i_o_i, T_matmul_NT_i_i = s[T_matmul_NT].split(T_matmul_NT_i, factor=2)
T_matmul_NT_i_o_o_i, T_matmul_NT_i_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_i, factor=4)
T_matmul_NT_i_o_o_o, T_matmul_NT_i_o_o_i = s[T_matmul_NT].split(T_matmul_NT_i_o_o_i, factor=1)
T_matmul_NT_j_o_i, T_matmul_NT_j_i = s[T_matmul_NT].split(T_matmul_NT_j, factor=16)
T_matmul_NT_j_o_o_i, T_matmul_NT_j_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_i, factor=3)
T_matmul_NT_j_o_o_o, T_matmul_NT_j_o_o_i = s[T_matmul_NT].split(T_matmul_NT_j_o_o_i, factor=1)
T_matmul_NT_k_o, T_matmul_NT_k_i = s[T_matmul_NT].split(T_matmul_NT_k, factor=2)
s[T_matmul_NT].reorder(T_matmul_NT_i_o_o_o, T_matmul_NT_j_o_o_o, T_matmul_NT_i_o_o_i, T_matmul_NT_j_o_o_i, T_matmul_NT_k_o, T_matmul_NT_i_o_i, T_matmul_NT_j_o_i, T_matmul_NT_k_i, T_matmul_NT_i_i, T_matmul_NT_j_i)
T_add_ax0_o_i, T_add_ax0_i = s[T_add].split(T_add_ax0, factor=8)
T_add_ax0_o_o, T_add_ax0_o_i = s[T_add].split(T_add_ax0_o_i, factor=1)
T_add_ax1_o_i, T_add_ax1_i = s[T_add].split(T_add_ax1, factor=48)
T_add_ax1_o_o, T_add_ax1_o_i = s[T_add].split(T_add_ax1_o_i, factor=1)
s[T_add].reorder(T_add_ax0_o_o, T_add_ax1_o_o, T_add_ax0_o_i, T_add_ax1_o_i, T_add_ax0_i, T_add_ax1_i)
s[T_matmul_NT].compute_at(s[T_add], T_add_ax1_o_i)
T_add_ax0_o_o_ax1_o_o_fused = s[T_add].fuse(T_add_ax0_o_o, T_add_ax1_o_o)
s[T_add].parallel(T_add_ax0_o_o_ax1_o_o_fused)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "auto_unroll_max_step", 0)
s[T_matmul_NT].pragma(T_matmul_NT_i_o_o_o, "unroll_explicit", True)
s[T_matmul_NT].vectorize(T_matmul_NT_j_i)


The best replacement found is:
@main = primfn(placeholder_3: handle, placeholder_4: handle, placeholder_5: handle, T_add_1: handle) -> ()
  attr = {"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True}
  buffers = {placeholder: Buffer(placeholder_6: Pointer(float32), float32, [24576], []),
             placeholder_1: Buffer(placeholder_7: Pointer(float32), float32, [2359296], []),
             placeholder_2: Buffer(placeholder_8: Pointer(float32), float32, [768], []),
             T_add: Buffer(T_add_2: Pointer(float32), float32, [6144], [])}
  buffer_map = {placeholder_3: placeholder, placeholder_4: placeholder_1, placeholder_5: placeholder_2, T_add_1: T_add}
  preflattened_buffer_map = {placeholder_3: placeholder_9: Buffer(placeholder_6, float32, [8, 3072], []), placeholder_4: placeholder_10: Buffer(placeholder_7, float32, [16, 1, 1, 1, 1536, 3, 2, 16], []), placeholder_5: placeholder_11: Buffer(placeholder_8, float32, [1, 768], []), T_add_1: T_add_3: Buffer(T_add_2, float32, [8, 768], [])} {
  for (ax0.outer.outer.ax1.outer.outer.fused: int32, 0, 16) "parallel" {
    allocate(T_matmul_NT: Pointer(global float32), float32, [384]), storage_scope = global {
      for (i.outer.inner.init: int32, 0, 4) {
        for (j.outer.inner.init: int32, 0, 3) {
          for (i.inner.init: int32, 0, 2) {
            T_matmul_NT_1: Buffer(T_matmul_NT, float32, [384], [])[ramp((((i.outer.inner.init*96) + (i.inner.init*48)) + (j.outer.inner.init*16)), 1, 16)] = broadcast(0f32, 16)
          }
        }
      }
      for (k.outer: int32, 0, 1536) {
        for (i.outer.inner: int32, 0, 4) {
          for (j.outer.inner: int32, 0, 3) {
            for (k.inner: int32, 0, 2) {
              for (i.inner: int32, 0, 2) {
                let cse_var_1: int32 = (((i.outer.inner*96) + (i.inner*48)) + (j.outer.inner*16))
                T_matmul_NT_1[ramp(cse_var_1, 1, 16)] = (T_matmul_NT_1[ramp(cse_var_1, 1, 16)] + (broadcast(placeholder[((((i.outer.inner*6144) + (i.inner*3072)) + (k.outer*2)) + k.inner)], 16)*placeholder_1[ramp(((((ax0.outer.outer.ax1.outer.outer.fused*147456) + (k.outer*96)) + (j.outer.inner*32)) + (k.inner*16)), 1, 16)]))
              }
            }
          }
        }
      }
      for (ax0.inner: int32, 0, 8) {
        for (ax1.inner: int32, 0, 48) {
          let cse_var_2: int32 = (ax0.outer.outer.ax1.outer.outer.fused*48)
          T_add[(((ax0.inner*768) + cse_var_2) + ax1.inner)] = (T_matmul_NT_1[((ax0.inner*48) + ax1.inner)] + placeholder_2[(cse_var_2 + ax1.inner)])
        }
      }
    }
  }
}


Running time in time_evaluator:  [5.6441368089887645, 5.650147325842697, 5.64538802247191]
Avg running time: 5.64655738576779
